{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["ntEQL9pojfmm","I_sRuuduqdYq"],"toc_visible":true,"authorship_tag":"ABX9TyNcoOq0FBLY1MWR3H3uN/Ic"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"ntEQL9pojfmm"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80ZXu_i9bYfB","outputId":"968e6ac8-a888-4af2-bb5b-d9f708b18241","executionInfo":{"status":"ok","timestamp":1667555099831,"user_tz":-480,"elapsed":27023,"user":{"displayName":"Khoo Zi-Yu","userId":"01763969606877785000"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scipy==1.6.3\n","  Downloading scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n","\u001b[K     |████████████████████████████████| 27.4 MB 1.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy==1.6.3) (1.21.6)\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.7.3\n","    Uninstalling scipy-1.7.3:\n","      Successfully uninstalled scipy-1.7.3\n","Successfully installed scipy-1.6.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit_optimize==0.8.1\n","  Downloading scikit_optimize-0.8.1-py2.py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 3.6 MB/s \n","\u001b[?25hCollecting pyaml>=16.9\n","  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit_optimize==0.8.1) (1.0.2)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit_optimize==0.8.1) (1.21.6)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_optimize==0.8.1) (1.2.0)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit_optimize==0.8.1) (1.6.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit_optimize==0.8.1) (6.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit_optimize==0.8.1) (3.1.0)\n","Installing collected packages: pyaml, scikit-optimize\n","Successfully installed pyaml-21.10.1 scikit-optimize-0.8.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit_learn==0.24.2\n","  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n","\u001b[K     |████████████████████████████████| 22.3 MB 1.7 MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2) (1.6.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2) (3.1.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2) (1.21.6)\n","Installing collected packages: scikit-learn\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.0.2\n","    Uninstalling scikit-learn-1.0.2:\n","      Successfully uninstalled scikit-learn-1.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n","Successfully installed scikit-learn-0.24.2\n","Cloning into 'SeparableNNs'...\n","remote: Enumerating objects: 302, done.\u001b[K\n","remote: Total 302 (delta 0), reused 0 (delta 0), pack-reused 302\u001b[K\n","Receiving objects: 100% (302/302), 72.46 KiB | 3.29 MiB/s, done.\n","Resolving deltas: 100% (170/170), done.\n"]}],"source":["!pip install scipy==1.6.3\n","!pip install scikit_optimize==0.8.1\n","!pip install scikit_learn==0.24.2\n","! git clone https://github.com/zykhoo/SeparableNNs.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23992,"status":"ok","timestamp":1667555123817,"user":{"displayName":"Khoo Zi-Yu","userId":"01763969606877785000"},"user_tz":-480},"id":"n1IJrZEeJGii","outputId":"7d9b95b3-68b4-4173-d3b1-3b222b9fdc47"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# NN takes in p, q, dq, dp, and learns the Hamiltonian. The derivative of the Hamiltonian is used for integration\n","\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import torch.utils.data as Data\n","import torch.nn.utils.prune as prune\n","import numpy as np\n","import os\n","import time\n","from tqdm import tqdm\n","import math\n","\n","# define model\n","def softplus(x):\n","    return torch.log(torch.exp(x)+1)\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","# class sepNet(nn.Module):\n","\n","#     def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n","#         super(sepNet , self).__init__()\n","#         self.mask1 = torch.cat((torch.squeeze(torch.cat((torch.ones((1,int(input_size/2))),torch.zeros((1,int(input_size/2)))),1),0).repeat(int(hidden_size1),1),\n","#             torch.squeeze(torch.cat((torch.zeros((1,int(input_size/2))),torch.ones((1,int(input_size/2)))),1),0).repeat(int(hidden_size1),1)),0)\n","#         self.mask2 = torch.cat((torch.squeeze(torch.cat((torch.ones((1,int(hidden_size1))),torch.zeros((1,int(hidden_size1)))),1),0).repeat(int(hidden_size2),1),\n","#                     torch.squeeze(torch.cat((torch.zeros((1,int(hidden_size1))),torch.ones((1,int(hidden_size1)))),1),0).repeat(int(hidden_size2),1)),0)\n","#         self.mask3 = torch.cat((torch.squeeze(torch.cat((torch.ones((1,int(hidden_size2))),torch.zeros((1,int(hidden_size2)))),1),0).repeat(int(output_size),1),\n","#                     torch.squeeze(torch.cat((torch.zeros((1,int(hidden_size2))),torch.ones((1,int(hidden_size2)))),1),0).repeat(int(output_size),1)),0)\n","#         self.hidden_layer_1 = nn.Linear( input_size, hidden_size1*2, bias=True)\n","#         with torch.no_grad():\n","#             self.hidden_layer_1.weight.mul_(self.mask1)\n","#         self.hidden_layer_2 = nn.Linear( hidden_size1*2, hidden_size2*2, bias=True)\n","#         with torch.no_grad():\n","#             self.hidden_layer_2.weight.mul_(self.mask2)\n","#         self.output_layer = nn.Linear( hidden_size2*2, output_size*2 , bias=True)\n","#         with torch.no_grad():\n","#             self.output_layer.weight.mul_(self.mask3)\n","#         prune.custom_from_mask(self.hidden_layer_1, name='weight', mask=self.mask1)\n","#         prune.custom_from_mask(self.hidden_layer_2, name='weight', mask=self.mask2)\n","#         prune.custom_from_mask(self.output_layer, name='weight', mask=self.mask3)\n","        \n","#     def forward(self, x):\n","#         x = softplus(self.hidden_layer_1(x)) # F.relu(self.hidden_layer_1(x)) # \n","#         x = softplus(self.hidden_layer_2(x)) # F.relu(self.hidden_layer_2(x)) # \n","#         x = self.output_layer(x)\n","#         x = torch.sum(x)\n","#         return x\n","\n","\n","# PINN\n","class Net(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(Net , self).__init__()\n","        self.hidden_layer_1 = nn.Linear( input_size, hidden_size, bias=True)\n","        self.hidden_layer_2 = nn.Linear( hidden_size, hidden_size, bias=True)\n","        self.output_layer = nn.Linear( hidden_size, output_size , bias=True)\n","        \n","    def forward(self, x):\n","        x = softplus(self.hidden_layer_1(x)) # F.relu(self.hidden_layer_1(x)) # \n","        x = softplus(self.hidden_layer_2(x)) # F.relu(self.hidden_layer_2(x)) # \n","        x = self.output_layer(x)\n","\n","        return x\n","\n","class SumNet(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(SumNet , self).__init__()\n","        self.hidden_layer_1 = nn.Linear( input_size, hidden_size, bias=True)\n","        self.hidden_layer_2 = nn.Linear( hidden_size, hidden_size, bias=True)\n","        self.output_layer = nn.Linear( hidden_size, 2 , bias=True)\n","        \n","    def forward(self, x):\n","        x = softplus(self.hidden_layer_1(x)) # F.relu(self.hidden_layer_1(x)) # \n","        x = softplus(self.hidden_layer_2(x)) # F.relu(self.hidden_layer_2(x)) # \n","        x = self.output_layer(x)\n","        x = torch.sum(x)\n","\n","        return x\n","\n","# calculate loss\n","def lossfuc(model,mat,x,y,device,x0,H0,dim,c1=1,c2=1,c3=1,c4=1,verbose=False):\n","    dim = int(wholemat.shape[1]/2)\n","    f3=(model(torch.tensor([[x0]*dim]).to(device))-torch.tensor([[H0]]).to(device))**2\n","    dH=torch.autograd.grad(y, x, grad_outputs=y.data.new(y.shape).fill_(1),create_graph=True, allow_unused=True)[0]\n","    dHdq=dH[:,:int(dim/2)]\n","    dHdp=dH[:,int(dim/2):]\n","    qprime=(mat[:,dim:int(3*dim/2)])\n","    pprime=(mat[:,int(3*dim/2):])\n","    assert dHdq.shape[1] == int(dim/2)\n","    assert dHdp.shape[1] == int(dim/2)\n","    assert qprime.shape[1] == int(dim/2)\n","    assert pprime.shape[1] == int(dim/2)\n","    f1=torch.mean((dHdp-qprime)**2,dim=0)\n","    # print(dHdq, pprime)\n","    f2=torch.mean((dHdq+pprime)**2,dim=0)\n","    f4=torch.mean((dHdq*qprime+dHdp*pprime)**2,dim=0)\n","    loss=torch.mean(c1*f1+c2*f2+c3*f3+c4*f4)\n","    if loss > 1000: print(\"errors:\", f1, f2, f3, f4)\n","    meanf1,meanf2,meanf3,meanf4=torch.mean(c1*f1),torch.mean(c2*f2),torch.mean(c3*f3),torch.mean(c4*f4)\n","    if verbose:\n","      print(x)\n","      print(meanf1,meanf2,meanf3,meanf4)\n","      print(loss,meanf1,meanf2,meanf3,meanf4)\n","    return loss,meanf1,meanf2,meanf3,meanf4\n","\n","\n","def data_preprocessing(start_train, final_train,device):       \n","    # wholemat=[]\n","    # for i in range(len(start_train[0,:])):\n","    #     wholemat.append(np.vstack((\n","    #         np.hstack((start_train[:,i], (final_train[:,i]-start_train[:,i])/h)),\n","    #         np.hstack((final_train[:,i], (final_train[:,i]-start_train[:,i])/h)))))\n","    wholemat = np.hstack((start_train.transpose(), final_train.transpose()))\n","\n","    wholemat =torch.tensor(wholemat)\n","    wholemat=wholemat.to(device)\n","\n","    wholemat,evalmat=train_test_split(wholemat, train_size=0.8, random_state=1)\n","\n","    return wholemat,evalmat\n","\n","## train\n","\n","# evaluate loss of dataset \n","def get_loss(model,device,initial_conditions,bs,x0,H0,dim,wholemat,evalmat,c1,c2,c3,c4,trainset=False,verbose=False):\n","    # this function is used to calculate average loss of a whole dataset\n","    # rootpath: path of set to be calculated loss\n","    # model: model\n","    # trainset: is training set or not\n","\n","\n","    if trainset:\n","        mat=wholemat\n","    else:\n","        mat=evalmat\n","    avg_loss=0\n","    avg_f1=0\n","    avg_f2=0\n","    avg_f3=0\n","    avg_f4=0\n","    for count in range(0,len(mat),bs):\n","      curmat=mat[count:count+bs]\n","      x=Variable((curmat[:,:dim]).float(),requires_grad=True)\n","      y=model(x)\n","      x=x.to(device)\n","      loss,f1,f2,f3,f4=lossfuc(model,curmat,x,y,device,x0,H0,dim,c1,c2,c3,c4)\n","      avg_loss+=loss.detach().cpu().item()\n","      avg_f1+=f1.detach().cpu().item()\n","      avg_f2+=f2.detach().cpu().item()\n","      avg_f3+=f3.detach().cpu().item()\n","      avg_f4+=f4.detach().cpu().item()\n","    num_batches=len(mat)//bs\n","    avg_loss/=num_batches\n","    avg_f1/=num_batches\n","    avg_f2/=num_batches\n","    avg_f3/=num_batches\n","    avg_f4/=num_batches\n","    if verbose:\n","        print(' loss=',avg_loss,' f1=',avg_f1,' f2=',avg_f2,' f3=',avg_f3,' f4=',avg_f4)\n","    return avg_loss\n","\n","\n","class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            上次验证集损失值改善后等待几个epoch\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement.\n","                            如果是True，为每个验证集损失值改善打印一条信息\n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            监测数量的最小变化，以符合改进的要求\n","                            Default: 0\n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            if abs(self.counter-self.patience)<5:\n","                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''\n","        Saves model when validation loss decrease.\n","        验证损失减少时保存模型。\n","        '''\n","        if self.verbose:\n","            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        # torch.save(model.state_dict(), 'checkpoint.pt')     # 这里会存储迄今最优模型的参数\n","        torch.save(model, 'checkpoint.pt')                 # 这里会存储迄今最优的模型\n","        self.val_loss_min = val_loss\n","\n","def train(net,bs,num_epoch,initial_conditions,device,wholemat,evalmat,x0,H0,dim,LR,patience,c1,c2,c3,c4):\n","    # function of training process\n","    # net: the model\n","    # bs: batch size \n","    # num_epoch: max of epoch to run\n","    # initial_conditions: number of trajectory in train set\n","    # patience: EarlyStopping parameter\n","    # c1~c4: hyperparameter for loss function\n","\n","\n","    avg_lossli,avg_f1li,avg_f2li,avg_f3li,avg_f4li=[],[],[],[],[]\n","    avg_vallosses=[]\n","    \n","    start = time.time()\n","    lr = LR # initial learning rate\n","    net=net.to(device)\n","\n","    early_stopping = EarlyStopping(patience=patience, verbose=False,delta=0.00001) # delta\n","    optimizer=torch.optim.Adam(net.parameters() , lr=lr )\n","    for epoch in range(num_epoch):\n","\n","        running_loss=0\n","\n","        running_f1=0\n","        running_f2=0\n","        running_f3=0\n","        running_f4=0\n","        num_batches=0\n","        \n","        # train\n","        shuffled_indices=torch.randperm(len(wholemat))\n","        net.train()\n","        for count in range(0,len(wholemat),bs):\n","            optimizer.zero_grad()\n","\n","            indices=shuffled_indices[count:count+bs]\n","            mat=wholemat[indices]\n","\n","            x=Variable(torch.tensor(mat[:,:dim]).float(),requires_grad=True)\n","            y=net(x)\n","\n","            loss,f1,f2,f3,f4=lossfuc(net,mat,x,y,device,x0,H0,dim,c1,c2,c3,c4)  \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm(net.parameters(), 1)\n","\n","            optimizer.step()\n","\n","            # compute some stats\n","            running_loss += loss.detach().item()\n","            running_f1 += f1.detach().item()\n","            running_f2 += f2.detach().item()\n","            running_f3 += f3.detach().item()\n","            running_f4 += f4.detach().item()\n","\n","            num_batches+=1\n","            torch.cuda.empty_cache()\n","\n","\n","\n","        avg_loss = running_loss/num_batches\n","        avg_f1 = running_f1/num_batches\n","        avg_f2 = running_f2/num_batches\n","        avg_f3 = running_f3/num_batches\n","        avg_f4 = running_f4/num_batches\n","        elapsed_time = time.time() - start\n","        \n","        avg_lossli.append(avg_loss)\n","        avg_f1li.append(avg_f1)\n","        avg_f2li.append(avg_f2)\n","        avg_f3li.append(avg_f3)\n","        avg_f4li.append(avg_f4)\n","        \n","        \n","        # evaluate\n","        net.eval()\n","        avg_val_loss=get_loss(net,device,len(evalmat),bs,x0,H0,dim,wholemat,evalmat,c1,c2,c3,c4)\n","        avg_vallosses.append(avg_val_loss)\n","        \n","        if epoch % 100 == 0 : \n","            print(' ')\n","            print('epoch=',epoch, ' time=', elapsed_time,\n","                  ' loss=', avg_loss ,' val_loss=',avg_val_loss,' f1=', avg_f1 ,' f2=', avg_f2 ,\n","                  ' f3=', avg_f3 ,' f4=', avg_f4 , 'num_batches=', num_batches, 'percent lr=', optimizer.param_groups[0][\"lr\"] )\n","        \n","        \n","        \n","        early_stopping(avg_val_loss,net)\n","        if early_stopping.early_stop:\n","            print('Early Stopping')\n","            break\n","            \n","    net=torch.load('checkpoint.pt')\n","    return net,epoch,avg_vallosses,avg_lossli,avg_f1li,avg_f2li,avg_f3li,avg_f4li\n","\n","class splitBalancedLinear(nn.Module):\n","\n","    def __init__(self, input_size, output_size):\n","        # output_size is the size of one of the two parallel networks\n","        super(splitBalancedLinear , self).__init__()\n","        self.input_size, self.output_size = input_size, output_size\n","        weights = torch.Tensor(2,self.input_size,self.output_size)\n","        self.weights = nn.Parameter(weights)\n","        bias = torch.Tensor(2,1,self.output_size)\n","        self.bias = nn.Parameter(bias)\n","\n","        # initialise weights and bias\n","        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) \n","        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n","        bound = 1 / math.sqrt(fan_in)\n","        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n","        \n","    def forward(self, x):\n","        # print(self.weights, self.bias)\n","        # print(\"mul\", torch.einsum('ijk,ikl->ijl', x, self.weights))\n","        # print(\"add\", torch.add(torch.einsum('ijk,ikl->ijl', x, self.weights), self.bias))\n","        return torch.add(torch.einsum('ijk,ikl->ijl', x, self.weights), self.bias)\n","        # return F.linear(x, self.weights, self.bias)\n","\n","class sepNet(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n","        super(sepNet , self).__init__()\n","        self.hidden_layer_1 = splitBalancedLinear(input_size, hidden_size1)\n","        self.hidden_layer_2 = splitBalancedLinear(hidden_size1, hidden_size2)\n","        self.output_layer = splitBalancedLinear(hidden_size2, output_size)\n","        \n","    def forward(self, x):\n","        # print(\"input\", x.shape)\n","        # print(x)\n","        x = torch.stack((x[:,:int(x.shape[-1]/2)],x[:,int(x.shape[-1]/2):]))\n","        # print(x)\n","        # print(\"initial\", x.shape)\n","        x = softplus(self.hidden_layer_1(x)) \n","        # print(x)\n","        # print(\"hl1\", x.shape)\n","        x = softplus(self.hidden_layer_2(x)) \n","        # print(x)\n","        # print(\"hl2\", x.shape)\n","        x = self.output_layer(x)\n","        # print(x)\n","        # print(\"output\", x.shape)\n","        x = torch.sum(x)\n","        return x"],"metadata":{"id":"UOPp0DoUxcnK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_secondgrad(model, z,device):\n","      inputs=Variable(z.clone().detach()).requires_grad_(True).to(device)\n","      out=model(inputs.float())\n","      dH=torch.autograd.grad(out, inputs, grad_outputs=out.data.new(out.shape).fill_(1),create_graph=True)[0]\n","      d2H = 0\n","      for i in range(dH.shape[1]):\n","        secondderiv=torch.autograd.grad(dH[:,i], inputs, grad_outputs=dH[:,i].data.new(dH[:,i].shape).fill_(1),create_graph=True)[0]\n","        if i < int(dH.shape[1]/2):\n","          d2H += torch.mean(torch.square(secondderiv[:,int(dH.shape[1]/2):]))\n","        else:\n","          d2H += torch.mean(torch.square(secondderiv[:,:int(dH.shape[1]/2)]))\n","      return d2H.detach().cpu().numpy()"],"metadata":{"id":"IOWhMVM9gehy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# $(x_0 + x_1 + x_2)\\times (y_0 + y_1 + y_2)$"],"metadata":{"id":"I_sRuuduqdYq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0-2tNl6q_p-"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from torch.autograd import Variable\n","from matplotlib import cm\n","from SeparableNNs.models import PINN\n","import torch\n","from SeparableNNs import metrics\n","import numpy as np\n","from SeparableNNs import groundtruth_2dim\n","from tqdm import tqdm\n","import torch.optim as optim\n","import sympy as sp\n","\n","a = sp.symbols('a')\n","b = sp.symbols('b')\n","c = sp.symbols('c')\n","x = sp.symbols('x')\n","y = sp.symbols('y')\n","z = sp.symbols('z')\n","\n","experiment,sys,dim = \"NN\",\"(a+b+c)(x+y+z)\",6\n","\n","H = (a+b+c)*(x+y+z)\n","f0 = sp.lambdify((a,b,c,x,y,z), H.diff(x))\n","f1 = sp.lambdify((a,b,c,x,y,z), H.diff(y))\n","f2 = sp.lambdify((a,b,c,x,y,z), H.diff(z))\n","f3 = sp.lambdify((a,b,c,x,y,z), H.diff(a))\n","f4 = sp.lambdify((a,b,c,x,y,z), H.diff(b))\n","f5 = sp.lambdify((a,b,c,x,y,z), H.diff(c))\n","\n","spacedim = [(-1.,1.)] \n","h= 0.01\n","x0, H0 = 0.,0.\n","initialcon = [64, 128, 256, 512, 1024, 2048] #, 4096, 8192\n","LR=0.01"]},{"cell_type":"code","source":["for i in range(20):\n","  seed = i\n","  np.random.seed(seed=seed)\n","  for ini in initialcon: \n","\n","    start = np.random.uniform(spacedim[0][0], spacedim[0][1], size = (6,ini))\n","    delta = start.copy()\n","    \n","    delta[0,:] = [f0(start[0,r], start[1,r], start[2,r], start[3,r], start[4,r], start[5,r],) for r in range(ini)]\n","    delta[1,:] = [f1(start[0,r], start[1,r], start[2,r], start[3,r], start[4,r], start[5,r],) for r in range(ini)]\n","    delta[2,:] = [f2(start[0,r], start[1,r], start[2,r], start[3,r], start[4,r], start[5,r],) for r in range(ini)]\n","    delta[3,:] = [-f3(start[0,r], start[1,r], start[2,r], start[3,r], start[4,r], start[5,r],) for r in range(ini)]\n","    delta[4,:] = [-f4(start[0,r], start[1,r], start[2,r], start[3,r], start[4,r], start[5,r],) for r in range(ini)]\n","    delta[5,:] = [-f5(start[0,r], start[1,r], start[2,r], start[3,r], start[4,r], start[5,r],) for r in range(ini)]\n","\n","    if torch.cuda.is_available():\n","      device=torch.device('cuda')\n","    else:\n","      device=torch.device('cpu')\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    wholemat, evalmat = PINN.data_preprocessing(start, delta,device)    \n","\n","\n","    net = Net(6,31,1)\n","    starttime = time.time() \n","    print(\"training PINN Net\")\n","    \n","    results = train(net,bs=min(int(len(wholemat)/5), 128),num_epoch=12000,initial_conditions=initialcon,device=device, wholemat=wholemat,evalmat=evalmat,x0=x0,H0=H0,dim=dim,LR=LR,patience=1000,c1=1,c2=1,c3=1,c4=1)\n","    net, epochs = results[0], results[1]\n","    PINNtraintime = time.time()-starttime\n","    torch.save(net.state_dict(), '/content/drive/MyDrive/CIKM2022/Separability Tests/HighDimTests/%s/%s_%s_%s_%s.pt' %(sys,sys,\"PINN\",seed,ini))"],"metadata":{"id":"xsnzp2RmV6NN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from torch.autograd import Variable\n","from matplotlib import cm\n","from SeparableNNs.models import PINN\n","import torch\n","from SeparableNNs import metrics\n","import numpy as np\n","from SeparableNNs import groundtruth_2dim\n","from tqdm import tqdm\n","import torch.optim as optim\n","import sympy as sp\n","\n","device=torch.device('cuda')\n","\n","def get_grad(model, z,device):\n","      inputs=Variable(z.clone().detach()).requires_grad_(True).to(device)\n","      out=model(inputs.float())\n","      dH=torch.autograd.grad(out, inputs, grad_outputs=out.data.new(out.shape).fill_(1),create_graph=True)[0]\n","      return np.asarray([dH.detach().cpu().numpy()[:,3], dH.detach().cpu().numpy()[:,4], dH.detach().cpu().numpy()[:,5], \n","                         -dH.detach().cpu().numpy()[:,0], -dH.detach().cpu().numpy()[:,1], -dH.detach().cpu().numpy()[:,2], ]) # negative dH/dq is dp/dt\n","\n","r1, r2 = np.zeros(len(initialcon)), np.zeros(len(initialcon))\n","\n","for seed in tqdm(range(20)):\n","  np.random.seed(seed)\n","  test1, test2 = [], []\n","\n","  for ini in initialcon: \n","\n","    start = np.random.uniform(spacedim[0][0], spacedim[0][1], size = (6,ini))\n","    delta = start.copy()\n","    \n","    delta[0,:] = [f0(start[0,r], start[1,r], start[2,r], start[3,r], start[4,r], start[5,r],) for r in range(ini)]\n","    delta[1,:] = [f1(start[0,r], start[1,r], start[2,r], start[3,r], start[4,r], start[5,r],) for r in range(ini)]\n","    delta[2,:] = [f2(start[0,r], start[1,r], start[2,r], start[3,r], start[4,r], start[5,r],) for r in range(ini)]\n","    delta[3,:] = [f3(start[0,r], start[1,r], start[2,r], start[3,r], start[4,r], start[5,r],) for r in range(ini)]\n","    delta[4,:] = [f4(start[0,r], start[1,r], start[2,r], start[3,r], start[4,r], start[5,r],) for r in range(ini)]\n","    delta[5,:] = [f5(start[0,r], start[1,r], start[2,r], start[3,r], start[4,r], start[5,r],) for r in range(ini)]\n","\n","    ab = np.tile(np.expand_dims(np.median(delta, axis = 1),0), (ini,1))\n","    xy = delta.transpose()\n","    ay = np.hstack((ab[:,:int(dim/2)], xy[:,int(dim/2):])) \n","    xb = np.hstack((xy[:,:int(dim/2)], ab[:,int(dim/2):])) \n","\n","    net = Net(6,31,1)\n","    net.load_state_dict(torch.load('/content/drive/MyDrive/CIKM2022/Separability Tests/HighDimTests/%s/%s_%s_%s_%s.pt' %(sys,sys,\"PINN\",seed,ini)))\n","\n","    results = get_secondgrad(net, torch.tensor(xy), 'cpu')\n","\n","    test1.append(results)\n","    test2.append(np.mean(np.sum(np.square(get_grad(net, torch.tensor(xy), 'cpu')+get_grad(net, torch.tensor(ab), 'cpu')-get_grad(net, torch.tensor(ay), 'cpu')-get_grad(net, torch.tensor(xb), 'cpu')),0)))\n","  r1 = np.vstack((r1, test1))\n","  r2 = np.vstack((r2, test2))\n","\n","r1, r2 = r1[1:,:], r2[1:,:]\n","print(\"2NDDERIVTEST & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g\" %(np.mean(r1[:,0]), np.std(r1[:,0]), np.mean(r1[:,1]), np.std(r1[:,1]),\n","                                                                                   np.mean(r1[:,2]), np.std(r1[:,2]), np.mean(r1[:,3]), np.std(r1[:,3]),\n","                                                                                   np.mean(r1[:,4]), np.std(r1[:,4]), np.mean(r1[:,5]), np.std(r1[:,5]),))\n","print(\"2NDDERIVTEST & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g\" %(np.min(r1[:,0]), np.max(r1[:,0]), np.min(r1[:,1]), np.max(r1[:,1]),\n","                                                                                   np.min(r1[:,2]), np.max(r1[:,2]), np.min(r1[:,3]), np.max(r1[:,3]),\n","                                                                                   np.min(r1[:,4]), np.max(r1[:,4]), np.min(r1[:,5]), np.max(r1[:,5]),))\n","\n","print(\"AIFTEST & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g\" %(np.mean(r2[:,0]), np.std(r2[:,0]), np.mean(r2[:,1]), np.std(r2[:,1]),\n","                                                                                   np.mean(r2[:,2]), np.std(r2[:,2]), np.mean(r2[:,3]), np.std(r2[:,3]),\n","                                                                                   np.mean(r2[:,4]), np.std(r2[:,4]), np.mean(r2[:,5]), np.std(r2[:,5]),))\n","print(\"AIFTEST & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g & %.1g\" %(np.min(r2[:,0]), np.max(r2[:,0]), np.min(r2[:,1]), np.max(r2[:,1]),\n","                                                                                   np.min(r2[:,2]), np.max(r2[:,2]), np.min(r2[:,3]), np.max(r2[:,3]),\n","                                                                                   np.min(r2[:,4]), np.max(r2[:,4]), np.min(r2[:,5]), np.max(r2[:,5]),))"],"metadata":{"id":"LC4-ZVjOq_p_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667543658201,"user_tz":-480,"elapsed":62545,"user":{"displayName":"Khoo Zi-Yu","userId":"01763969606877785000"}},"outputId":"541a4956-064f-4115-d3f3-525cb5706528"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 20/20 [01:02<00:00,  3.10s/it]"]},{"output_type":"stream","name":"stdout","text":["2NDDERIVTEST & 0.4 & 0.7 & 1 & 3 & 0.8 & 2 & 0.5 & 1 & 0.4 & 0.3 & 0.5 & 0.3\n","2NDDERIVTEST & 2e-05 & 2 & 2e-05 & 1e+01 & 2e-05 & 6 & 2e-05 & 4 & 4e-05 & 1 & 0.005 & 1\n","AIFTEST & 1 & 3 & 2 & 5 & 2 & 3 & 0.7 & 1 & 2 & 0.9 & 1 & 0.7\n","AIFTEST & 2e-06 & 1e+01 & 1e-06 & 2e+01 & 1e-06 & 1e+01 & 1e-06 & 5 & 0.0001 & 4 & 0.02 & 3\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}