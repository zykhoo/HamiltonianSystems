{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80ZXu_i9bYfB","outputId":"fc08e46a-1c85-481d-ad43-2c818ffb9eee","executionInfo":{"status":"ok","timestamp":1665980283505,"user_tz":-480,"elapsed":42518,"user":{"displayName":"Khoo Zi-Yu","userId":"01763969606877785000"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scipy==1.6.3\n","  Downloading scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n","\u001b[K     |████████████████████████████████| 27.4 MB 68.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy==1.6.3) (1.21.6)\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.7.3\n","    Uninstalling scipy-1.7.3:\n","      Successfully uninstalled scipy-1.7.3\n","Successfully installed scipy-1.6.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit_optimize==0.8.1\n","  Downloading scikit_optimize-0.8.1-py2.py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 4.4 MB/s \n","\u001b[?25hCollecting pyaml>=16.9\n","  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit_optimize==0.8.1) (1.0.2)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit_optimize==0.8.1) (1.21.6)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_optimize==0.8.1) (1.2.0)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit_optimize==0.8.1) (1.6.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit_optimize==0.8.1) (6.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit_optimize==0.8.1) (3.1.0)\n","Installing collected packages: pyaml, scikit-optimize\n","Successfully installed pyaml-21.10.1 scikit-optimize-0.8.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit_learn==0.24.2\n","  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n","\u001b[K     |████████████████████████████████| 22.3 MB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2) (3.1.0)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2) (1.6.3)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2) (1.21.6)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2) (1.2.0)\n","Installing collected packages: scikit-learn\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.0.2\n","    Uninstalling scikit-learn-1.0.2:\n","      Successfully uninstalled scikit-learn-1.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n","Successfully installed scikit-learn-0.24.2\n","Cloning into 'SeparableNNs'...\n","remote: Enumerating objects: 302, done.\u001b[K\n","remote: Total 302 (delta 0), reused 0 (delta 0), pack-reused 302\u001b[K\n","Receiving objects: 100% (302/302), 72.46 KiB | 6.04 MiB/s, done.\n","Resolving deltas: 100% (170/170), done.\n"]}],"source":["!pip install scipy==1.6.3\n","!pip install scikit_optimize==0.8.1\n","!pip install scikit_learn==0.24.2\n","! git clone https://github.com/zykhoo/SeparableNNs.git"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75865,"status":"ok","timestamp":1665980359362,"user":{"displayName":"Khoo Zi-Yu","userId":"01763969606877785000"},"user_tz":-480},"id":"n1IJrZEeJGii","outputId":"0e94d0ef-36e1-4496-bd24-02c4fd28838d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":103,"metadata":{"id":"8B6gDmvqbL6T","executionInfo":{"status":"ok","timestamp":1665996606371,"user_tz":-480,"elapsed":420,"user":{"displayName":"Khoo Zi-Yu","userId":"01763969606877785000"}}},"outputs":[],"source":["import numpy as np\n","\n","experiment,sys,dim = \"NN\",\"keplersystem\",4\n","H = lambda x: 0.5 * ( x[2]**2 + x[3]**2 ) - 1.0 / np.sqrt ( x[0]**2 + x[1]**2 )\n","f1 = lambda x: np.asarray([x[2], x[3]])\n","f2 = lambda x: np.asarray([-x[0]*(x[0]**2+x[1]**2)**(-1.5), -x[1]*(x[0]**2+x[1]**2)**(-1.5)])\n","\n","spacedim = [(-4.,4.),(-4.,4.),(-4.,4.),(-4.,4.)] # by using this spacedim, x<1 therefore x**2<1. the hamiltonian will usually be negative.\n","h= 0.0001\n","x0, H0 = 0.6,H([0.6]*4)\n","initialcon = [64, 128, 256, 512, 1024, 2048] #, 4096, 8192\n","LR=0.01\n","\n","\"\"\"# Test dataset creation\"\"\"\n","\n","from SeparableNNs import groundtruth_2dim\n","from tqdm import tqdm\n","import time \n","\n","# xxshort,yyshort = np.linspace(spacedim[0][0], spacedim[0][1], 20), np.linspace(spacedim[1][0], spacedim[1][1], 20)\n","# xshort,yshort = np.meshgrid(xxshort,yyshort)\n","\n","# H_true = H(np.c_[np.ravel(xshort),np.ravel(yshort)].transpose())\n","# sample_points = np.expand_dims(np.c_[np.ravel(xshort),np.ravel(yshort)],2)\n","\n","# def get_H_grad(model, z,device):\n","#     inputs=torch.unsqueeze(Variable(torch.tensor([z[0][0],z[1][0]]), requires_grad = True),0).to(device)\n","#     out=model(inputs.float())\n","#     dH=torch.autograd.grad(out, inputs, grad_outputs=out.data.new(out.shape).fill_(1),create_graph=True, allow_unused=True)[0].detach().cpu().numpy()\n","#     return out.detach().cpu().numpy(), dH[0][1],-dH[0][0] # negative dH/dq is dp/dt"]},{"cell_type":"code","source":["# NN takes in p, q, dq, dp, and learns the Hamiltonian. The derivative of the Hamiltonian is used for integration\n","\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import torch.utils.data as Data\n","import torch.nn.utils.prune as prune\n","import numpy as np\n","import os\n","import time\n","from tqdm import tqdm\n","import math\n","\n","# define model\n","def softplus(x):\n","    return torch.log(torch.exp(x)+1)\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","# class sepNet(nn.Module):\n","\n","#     def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n","#         super(sepNet , self).__init__()\n","#         self.mask1 = torch.cat((torch.squeeze(torch.cat((torch.ones((1,int(input_size/2))),torch.zeros((1,int(input_size/2)))),1),0).repeat(int(hidden_size1),1),\n","#             torch.squeeze(torch.cat((torch.zeros((1,int(input_size/2))),torch.ones((1,int(input_size/2)))),1),0).repeat(int(hidden_size1),1)),0)\n","#         self.mask2 = torch.cat((torch.squeeze(torch.cat((torch.ones((1,int(hidden_size1))),torch.zeros((1,int(hidden_size1)))),1),0).repeat(int(hidden_size2),1),\n","#                     torch.squeeze(torch.cat((torch.zeros((1,int(hidden_size1))),torch.ones((1,int(hidden_size1)))),1),0).repeat(int(hidden_size2),1)),0)\n","#         self.mask3 = torch.cat((torch.squeeze(torch.cat((torch.ones((1,int(hidden_size2))),torch.zeros((1,int(hidden_size2)))),1),0).repeat(int(output_size),1),\n","#                     torch.squeeze(torch.cat((torch.zeros((1,int(hidden_size2))),torch.ones((1,int(hidden_size2)))),1),0).repeat(int(output_size),1)),0)\n","#         self.hidden_layer_1 = nn.Linear( input_size, hidden_size1*2, bias=True)\n","#         with torch.no_grad():\n","#             self.hidden_layer_1.weight.mul_(self.mask1)\n","#         self.hidden_layer_2 = nn.Linear( hidden_size1*2, hidden_size2*2, bias=True)\n","#         with torch.no_grad():\n","#             self.hidden_layer_2.weight.mul_(self.mask2)\n","#         self.output_layer = nn.Linear( hidden_size2*2, output_size*2 , bias=True)\n","#         with torch.no_grad():\n","#             self.output_layer.weight.mul_(self.mask3)\n","#         prune.custom_from_mask(self.hidden_layer_1, name='weight', mask=self.mask1)\n","#         prune.custom_from_mask(self.hidden_layer_2, name='weight', mask=self.mask2)\n","#         prune.custom_from_mask(self.output_layer, name='weight', mask=self.mask3)\n","        \n","#     def forward(self, x):\n","#         x = softplus(self.hidden_layer_1(x)) # F.relu(self.hidden_layer_1(x)) # \n","#         x = softplus(self.hidden_layer_2(x)) # F.relu(self.hidden_layer_2(x)) # \n","#         x = self.output_layer(x)\n","#         x = torch.sum(x)\n","#         return x\n","\n","\n","# PINN\n","class Net(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(Net , self).__init__()\n","        self.hidden_layer_1 = nn.Linear( input_size, hidden_size, bias=True)\n","        self.hidden_layer_2 = nn.Linear( hidden_size, hidden_size, bias=True)\n","        self.output_layer = nn.Linear( hidden_size, output_size , bias=True)\n","        \n","    def forward(self, x):\n","        x = softplus(self.hidden_layer_1(x)) # F.relu(self.hidden_layer_1(x)) # \n","        x = softplus(self.hidden_layer_2(x)) # F.relu(self.hidden_layer_2(x)) # \n","        x = self.output_layer(x)\n","\n","        return x\n","\n","class SumNet(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(SumNet , self).__init__()\n","        self.hidden_layer_1 = nn.Linear( input_size, hidden_size, bias=True)\n","        self.hidden_layer_2 = nn.Linear( hidden_size, hidden_size, bias=True)\n","        self.output_layer = nn.Linear( hidden_size, 2 , bias=True)\n","        \n","    def forward(self, x):\n","        x = softplus(self.hidden_layer_1(x)) # F.relu(self.hidden_layer_1(x)) # \n","        x = softplus(self.hidden_layer_2(x)) # F.relu(self.hidden_layer_2(x)) # \n","        x = self.output_layer(x)\n","        x = torch.sum(x)\n","\n","        return x\n","\n","# calculate loss\n","def lossfuc(model,mat,x,y,device,x0,H0,dim,c1=1,c2=1,c3=1,c4=1,verbose=False):\n","    dim = int(wholemat.shape[1]/2)\n","    f3=(model(torch.tensor([[x0]*dim]).to(device))-torch.tensor([[H0]]).to(device))**2\n","    dH=torch.autograd.grad(y, x, grad_outputs=y.data.new(y.shape).fill_(1),create_graph=True, allow_unused=True)[0]\n","    dHdq=dH[:,:int(dim/2)]\n","    dHdp=dH[:,int(dim/2):]\n","    qprime=(mat[:,dim:int(3*dim/2)])\n","    pprime=(mat[:,int(3*dim/2):])\n","    assert dHdq.shape[1] == int(dim/2)\n","    assert dHdp.shape[1] == int(dim/2)\n","    assert qprime.shape[1] == int(dim/2)\n","    assert pprime.shape[1] == int(dim/2)\n","    f1=torch.mean((dHdp-qprime)**2,dim=0)\n","    # print(dHdq, pprime)\n","    f2=torch.mean((dHdq+pprime)**2,dim=0)\n","    f4=torch.mean((dHdq*qprime+dHdp*pprime)**2,dim=0)\n","    loss=torch.mean(c1*f1+c2*f2+c3*f3+c4*f4)\n","    if loss > 1000: print(\"errors:\", f1, f2, f3, f4)\n","    meanf1,meanf2,meanf3,meanf4=torch.mean(c1*f1),torch.mean(c2*f2),torch.mean(c3*f3),torch.mean(c4*f4)\n","    if verbose:\n","      print(x)\n","      print(meanf1,meanf2,meanf3,meanf4)\n","      print(loss,meanf1,meanf2,meanf3,meanf4)\n","    return loss,meanf1,meanf2,meanf3,meanf4\n","\n","\n","def data_preprocessing(start_train, final_train,device):       \n","    # wholemat=[]\n","    # for i in range(len(start_train[0,:])):\n","    #     wholemat.append(np.vstack((\n","    #         np.hstack((start_train[:,i], (final_train[:,i]-start_train[:,i])/h)),\n","    #         np.hstack((final_train[:,i], (final_train[:,i]-start_train[:,i])/h)))))\n","    wholemat = np.hstack((start_train.transpose(), final_train.transpose()))\n","\n","    wholemat =torch.tensor(wholemat)\n","    wholemat=wholemat.to(device)\n","\n","    wholemat,evalmat=train_test_split(wholemat, train_size=0.8, random_state=1)\n","\n","    return wholemat,evalmat\n","\n","## train\n","\n","# evaluate loss of dataset \n","def get_loss(model,device,initial_conditions,bs,x0,H0,dim,wholemat,evalmat,c1,c2,c3,c4,trainset=False,verbose=False):\n","    # this function is used to calculate average loss of a whole dataset\n","    # rootpath: path of set to be calculated loss\n","    # model: model\n","    # trainset: is training set or not\n","\n","\n","    if trainset:\n","        mat=wholemat\n","    else:\n","        mat=evalmat\n","    avg_loss=0\n","    avg_f1=0\n","    avg_f2=0\n","    avg_f3=0\n","    avg_f4=0\n","    for count in range(0,len(mat),bs):\n","      curmat=mat[count:count+bs]\n","      x=Variable((curmat[:,:dim]).float(),requires_grad=True)\n","      y=model(x)\n","      x=x.to(device)\n","      loss,f1,f2,f3,f4=lossfuc(model,curmat,x,y,device,x0,H0,dim,c1,c2,c3,c4)\n","      avg_loss+=loss.detach().cpu().item()\n","      avg_f1+=f1.detach().cpu().item()\n","      avg_f2+=f2.detach().cpu().item()\n","      avg_f3+=f3.detach().cpu().item()\n","      avg_f4+=f4.detach().cpu().item()\n","    num_batches=len(mat)//bs\n","    avg_loss/=num_batches\n","    avg_f1/=num_batches\n","    avg_f2/=num_batches\n","    avg_f3/=num_batches\n","    avg_f4/=num_batches\n","    if verbose:\n","        print(' loss=',avg_loss,' f1=',avg_f1,' f2=',avg_f2,' f3=',avg_f3,' f4=',avg_f4)\n","    return avg_loss\n","\n","\n","class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            上次验证集损失值改善后等待几个epoch\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement.\n","                            如果是True，为每个验证集损失值改善打印一条信息\n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            监测数量的最小变化，以符合改进的要求\n","                            Default: 0\n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            if abs(self.counter-self.patience)<5:\n","                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''\n","        Saves model when validation loss decrease.\n","        验证损失减少时保存模型。\n","        '''\n","        if self.verbose:\n","            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        # torch.save(model.state_dict(), 'checkpoint.pt')     # 这里会存储迄今最优模型的参数\n","        torch.save(model, 'checkpoint.pt')                 # 这里会存储迄今最优的模型\n","        self.val_loss_min = val_loss\n","\n","def train(net,bs,num_epoch,initial_conditions,device,wholemat,evalmat,x0,H0,dim,LR,patience,c1,c2,c3,c4):\n","    # function of training process\n","    # net: the model\n","    # bs: batch size \n","    # num_epoch: max of epoch to run\n","    # initial_conditions: number of trajectory in train set\n","    # patience: EarlyStopping parameter\n","    # c1~c4: hyperparameter for loss function\n","\n","\n","    avg_lossli,avg_f1li,avg_f2li,avg_f3li,avg_f4li=[],[],[],[],[]\n","    avg_vallosses=[]\n","    \n","    start = time.time()\n","    lr = LR # initial learning rate\n","    net=net.to(device)\n","\n","    early_stopping = EarlyStopping(patience=patience, verbose=False,delta=0.00001) # delta\n","    optimizer=torch.optim.Adam(net.parameters() , lr=lr )\n","    for epoch in range(num_epoch):\n","\n","        running_loss=0\n","\n","        running_f1=0\n","        running_f2=0\n","        running_f3=0\n","        running_f4=0\n","        num_batches=0\n","        \n","        # train\n","        shuffled_indices=torch.randperm(len(wholemat))\n","        net.train()\n","        for count in range(0,len(wholemat),bs):\n","            optimizer.zero_grad()\n","\n","            indices=shuffled_indices[count:count+bs]\n","            mat=wholemat[indices]\n","\n","            x=Variable(torch.tensor(mat[:,:dim]).float(),requires_grad=True)\n","            y=net(x)\n","\n","            loss,f1,f2,f3,f4=lossfuc(net,mat,x,y,device,x0,H0,dim,c1,c2,c3,c4)  \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm(net.parameters(), 1)\n","\n","            optimizer.step()\n","\n","            # compute some stats\n","            running_loss += loss.detach().item()\n","            running_f1 += f1.detach().item()\n","            running_f2 += f2.detach().item()\n","            running_f3 += f3.detach().item()\n","            running_f4 += f4.detach().item()\n","\n","            num_batches+=1\n","            torch.cuda.empty_cache()\n","\n","\n","\n","        avg_loss = running_loss/num_batches\n","        avg_f1 = running_f1/num_batches\n","        avg_f2 = running_f2/num_batches\n","        avg_f3 = running_f3/num_batches\n","        avg_f4 = running_f4/num_batches\n","        elapsed_time = time.time() - start\n","        \n","        avg_lossli.append(avg_loss)\n","        avg_f1li.append(avg_f1)\n","        avg_f2li.append(avg_f2)\n","        avg_f3li.append(avg_f3)\n","        avg_f4li.append(avg_f4)\n","        \n","        \n","        # evaluate\n","        net.eval()\n","        avg_val_loss=get_loss(net,device,len(evalmat),bs,x0,H0,dim,wholemat,evalmat,c1,c2,c3,c4)\n","        avg_vallosses.append(avg_val_loss)\n","        \n","        if epoch % 10 == 0 : \n","            print(' ')\n","            print('epoch=',epoch, ' time=', elapsed_time,\n","                  ' loss=', avg_loss ,' val_loss=',avg_val_loss,' f1=', avg_f1 ,' f2=', avg_f2 ,\n","                  ' f3=', avg_f3 ,' f4=', avg_f4 , 'num_batches=', num_batches, 'percent lr=', optimizer.param_groups[0][\"lr\"] )\n","        \n","        \n","        \n","        early_stopping(avg_val_loss,net)\n","        if early_stopping.early_stop:\n","            print('Early Stopping')\n","            break\n","            \n","    net=torch.load('checkpoint.pt')\n","    return net,epoch,avg_vallosses,avg_lossli,avg_f1li,avg_f2li,avg_f3li,avg_f4li\n","\n","class splitBalancedLinear(nn.Module):\n","\n","    def __init__(self, input_size, output_size):\n","        # output_size is the size of one of the two parallel networks\n","        super(splitBalancedLinear , self).__init__()\n","        self.input_size, self.output_size = input_size, output_size\n","        weights = torch.Tensor(2,self.input_size,self.output_size)\n","        self.weights = nn.Parameter(weights)\n","        bias = torch.Tensor(2,1,self.output_size)\n","        self.bias = nn.Parameter(bias)\n","\n","        # initialise weights and bias\n","        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) \n","        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n","        bound = 1 / math.sqrt(fan_in)\n","        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n","        \n","    def forward(self, x):\n","        # print(self.weights, self.bias)\n","        # print(\"mul\", torch.einsum('ijk,ikl->ijl', x, self.weights))\n","        # print(\"add\", torch.add(torch.einsum('ijk,ikl->ijl', x, self.weights), self.bias))\n","        return torch.add(torch.einsum('ijk,ikl->ijl', x, self.weights), self.bias)\n","        # return F.linear(x, self.weights, self.bias)\n","\n","class sepNet(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n","        super(sepNet , self).__init__()\n","        self.hidden_layer_1 = splitBalancedLinear(input_size, hidden_size1)\n","        self.hidden_layer_2 = splitBalancedLinear(hidden_size1, hidden_size2)\n","        self.output_layer = splitBalancedLinear(hidden_size2, output_size)\n","        \n","    def forward(self, x):\n","        # print(\"input\", x.shape)\n","        # print(x)\n","        x = torch.stack((x[:,:int(x.shape[-1]/2)],x[:,int(x.shape[-1]/2):]))\n","        # print(x)\n","        # print(\"initial\", x.shape)\n","        x = softplus(self.hidden_layer_1(x)) \n","        # print(x)\n","        # print(\"hl1\", x.shape)\n","        x = softplus(self.hidden_layer_2(x)) \n","        # print(x)\n","        # print(\"hl2\", x.shape)\n","        x = self.output_layer(x)\n","        # print(x)\n","        # print(\"output\", x.shape)\n","        x = torch.sum(x)\n","        return x"],"metadata":{"id":"UOPp0DoUxcnK","executionInfo":{"status":"ok","timestamp":1665996356992,"user_tz":-480,"elapsed":9,"user":{"displayName":"Khoo Zi-Yu","userId":"01763969606877785000"}}},"execution_count":101,"outputs":[]},{"cell_type":"code","source":["start, final = groundtruth_2dim.CreateTrainingDataTrajStormer(1,ini,spacedim,h,f1,f2,seed = seed,n_h = 1)\n","print(start.shape)\n","start = np.delete(start, np.where(np.abs(start[0,:])<0.1), axis = 1)\n","print(start.shape)\n","start = np.delete(start, np.where(np.abs(start[1,:])<0.1), axis = 1)\n","print(start.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-VyhsJfM6eF","executionInfo":{"status":"ok","timestamp":1665998000272,"user_tz":-480,"elapsed":1105,"user":{"displayName":"Khoo Zi-Yu","userId":"01763969606877785000"}},"outputId":"4a7f8fea-3124-470c-fd00-7a5ae710932a"},"execution_count":116,"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 2048)\n","(4, 2003)\n","(4, 1949)\n"]}]},{"cell_type":"code","execution_count":104,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"rnc_PX4nSc58","executionInfo":{"status":"error","timestamp":1665997702633,"user_tz":-480,"elapsed":1090388,"user":{"displayName":"Khoo Zi-Yu","userId":"01763969606877785000"}},"outputId":"2b1c79cf-9abf-4990-9a75-667fff2c2ba1"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.39050803  1.72151493  0.82210701  0.35906546 -0.61076161  1.1671529\n","  -0.49930231  3.13418401  3.70930208 -0.93246785  2.3338003   0.23115936\n","   0.54435649  3.40477311 -3.43171153 -3.3029656  -3.83825282  2.66095876\n","   2.22525401  2.96009719  3.82894674  2.39326851 -0.3081651   2.24423341\n","  -3.05380459  1.11936817 -2.8531737   3.55735134  0.17478657 -0.68270448\n","  -1.8835551   2.19386952 -0.35079734  0.54747159 -3.8496816   0.94108398\n","   0.89676578  0.93547197  3.54998463  1.45456239 -1.1239368  -0.50374437\n","   1.58104957 -3.51819623  1.33413372  1.36510296 -2.31693951 -2.96858962\n","  -1.47657319 -1.09031383  0.56157416 -0.49118789  3.9069907  -3.18364151\n","  -2.32898595 -2.70952386  1.2248666  -1.97366718 -0.26951382 -2.04459526\n","  -2.72824333 -3.11699887  1.25063672 -2.89453639]\n"," [-2.42734111 -1.05019863  2.56794584 -3.22318979  2.70355926 -3.23121274\n","   3.81167572 -0.25079039  3.81408871  0.83876416  1.91410864 -3.68649766\n","  -1.7375443  -3.03842751 -1.63087842 -3.05017825 -1.45613456 -0.68589604\n","  -3.48682003  1.53977695  0.53281163 -1.87688407  0.18598443 -3.24847591\n","   0.60757196  3.43436958 -1.45144838  1.33928304 -2.9456171   1.73061763\n","  -1.68475126 -2.5344691   0.69210348 -3.83913963  2.63152023 -3.96243619\n","   1.42253229 -1.83993621  1.88155218  3.69750836 -2.00997485  0.60925868\n","   0.73633545  0.57801525 -2.21534694  3.62199209 -0.42299697  2.77126938\n","   1.5958342  -1.62050439  2.51038256 -0.82795407  3.04882558  0.65018298\n","   3.05388289  1.54025272  1.80203424  0.01059506  3.64866908  1.15192159\n","  -0.60915961  0.85114571 -3.84645441 -1.58740147]\n"," [ 1.2813883  -1.67937914  0.94412343 -0.56985039 -2.91620749 -1.61374139\n","   0.55971929  0.72698209  0.59460199  1.22560656  1.21682616 -0.54865252\n","   3.17237277 -1.05950504 -0.5130806   3.13538684  2.44955191  1.63110867\n","  -3.1981849   3.35586091  1.7139304   3.99077605 -2.80441356  2.94500846\n","  -2.70005652  0.92447651 -3.00944014  2.78406583  2.45855167  0.55280591\n","  -0.74253362 -3.44666404  1.57943019 -0.37165854  1.7764448   2.93105861\n","   3.80417204  2.84642674 -3.90628733 -1.12017548  1.8399245  -2.62696258\n","   0.16829285 -3.56529609 -2.4000278  -3.85182564  2.34958163 -2.2086025\n","  -1.23718655  3.42465035  1.63531522 -3.74528856 -2.68244675  0.97182721\n","   0.61782871 -2.09685743  3.47371198  0.91172765  0.28506242  0.71927981\n","   1.84097624 -1.50444004 -0.8142315  -2.32125001]\n"," [-2.51045595  3.55497912  1.91640636 -0.07632953 -2.18068298 -1.96514815\n","  -3.53576672 -0.524667   -1.50563294  1.57074791 -0.97798529 -2.56317058\n","  -3.80257017 -3.46200295  1.43514219 -0.37042524  0.29263369  3.17337034\n","   3.92271158 -2.26482412  1.30462562 -1.89342099 -3.834792    2.06702923\n","  -1.43986279 -0.93228885  0.70653691  2.64838764  1.03185475  2.98120524\n","  -1.81166372  2.38437467 -2.51491245  3.62233326  1.49990621 -2.27593858\n","   3.57896472  1.84684645 -1.96846686 -2.29350418  0.14560571 -3.79469826\n","  -2.3402394  -0.60251625 -1.00664016 -0.29139661 -1.77897035  0.69427477\n","   2.91084485 -3.05974515  0.13903286 -2.94345515  1.73487745 -0.83152238\n","   0.52337049 -2.53376131 -2.84121793 -0.09554975 -1.1550981   3.52345556\n","   2.12260203  1.98930896  3.22975792 -3.33262052]] [[ 1.28138699e+00 -1.67938964e+00  9.44121335e-01 -5.69850919e-01\n","  -2.91620605e+00 -1.61374283e+00  5.59719725e-01  7.26977048e-01\n","   5.94600759e-01  1.22563019e+00  1.21682192e+00 -5.48652746e-01\n","   3.17236826e+00 -1.05950683e+00 -5.13077470e-01  3.13538866e+00\n","   2.44955469e+00  1.63110226e+00 -3.19818647e+00  3.35585693e+00\n","   1.71392708e+00  3.99077180e+00 -2.80408314e+00  2.94500664e+00\n","  -2.70005146e+00  9.24475327e-01 -3.00943579e+00  2.78406260e+00\n","   2.45855133e+00  5.52811210e-01 -7.42527787e-01 -3.44666695e+00\n","   1.57946773e+00 -3.71659008e-01  1.77644669e+00  2.93105791e+00\n","   3.80416261e+00  2.84642142e+00 -3.90629006e+00 -1.12017664e+00\n","   1.83992910e+00 -2.62691160e+00  1.68277949e-01 -3.56529221e+00\n","  -2.40003166e+00 -3.85182682e+00  2.34959049e+00 -2.20860028e+00\n","  -1.23717937e+00  3.42465766e+00  1.63531357e+00 -3.74526104e+00\n","  -2.68244835e+00  9.71831852e-01  6.17830764e-01 -2.09685295e+00\n","   3.47370606e+00  9.11740483e-01  2.85062699e-01  7.19287721e-01\n","   1.84098248e+00 -1.50443542e+00 -8.14232447e-01 -2.32124599e+00]\n"," [-2.51044779e+00  3.55498552e+00  1.91639981e+00 -7.63248065e-02\n","  -2.18068932e+00 -1.96514416e+00 -3.53577007e+00 -5.24666592e-01\n","  -1.50563421e+00  1.57072665e+00 -9.77988766e-01 -2.56316692e+00\n","  -3.80255578e+00 -3.46200135e+00  1.43514367e+00 -3.70423565e-01\n","   2.92634741e-01  3.17337200e+00  3.92271404e+00 -2.26482620e+00\n","   1.30462516e+00 -1.89341765e+00 -3.83499142e+00  2.06703187e+00\n","  -1.43986380e+00 -9.32292490e-01  7.06539121e-01  2.64838642e+00\n","   1.03186048e+00  2.98119181e+00 -1.81165850e+00  2.38437804e+00\n","  -2.51498652e+00  3.62233655e+00  1.49990491e+00 -2.27593565e+00\n","   3.57894977e+00  1.84685692e+00 -1.96846831e+00 -2.29350713e+00\n","   1.45613940e-01 -3.79475991e+00 -2.34024634e+00 -6.02516888e-01\n","  -1.00663375e+00 -2.91399728e-01 -1.77896873e+00  6.94272703e-01\n","   2.91083708e+00 -3.05973428e+00  1.39025484e-01 -2.94340875e+00\n","   1.73487620e+00 -8.31523325e-01  5.23367800e-01 -2.53376385e+00\n","  -2.84122664e+00 -9.55498237e-02 -1.15510182e+00  3.52345111e+00\n","   2.12260342e+00  1.98930770e+00  3.22976082e+00 -3.33261831e+00]\n"," [-2.62779561e-02 -2.09971157e-01 -4.19355347e-02 -1.05255929e-02\n","   2.86930415e-02 -2.87796484e-02  8.78959142e-03 -1.00828288e-01\n","  -2.46309744e-02  4.72602544e-01 -8.48710098e-02 -4.58582217e-03\n","  -9.01665364e-02 -3.58259113e-02  6.25644879e-02  3.63464130e-02\n","   5.54831004e-02 -1.28236812e-01 -3.14459803e-02 -7.96836789e-02\n","  -6.62713285e-02 -8.50535833e-02  6.61026041e+00 -3.64638125e-02\n","   1.01157524e-01 -2.37515414e-02  8.69738531e-02 -6.47667691e-02\n","  -6.80791896e-03  1.05998488e-01  1.16702759e-01 -5.82509943e-02\n","   7.51174282e-01 -9.38840333e-03  3.79652198e-02 -1.39320833e-02\n","  -1.88540758e-01 -1.06395031e-01 -5.47409179e-02 -2.31896791e-02\n","   9.20289134e-02  1.02012598e+00 -2.98029989e-01  7.76191121e-02\n","  -7.71359860e-02 -2.35376131e-02  1.77355495e-01  4.43205081e-02\n","   1.43652028e-01  1.46302426e-01 -3.29934822e-02  5.50366798e-01\n","  -3.20997005e-02  9.28000625e-02  4.11107836e-02  8.94968076e-02\n","  -1.18426130e-01  2.56716314e-01  5.50339603e-03  1.58183726e-01\n","   1.24904682e-01  9.23949809e-02 -1.89026782e-02  8.04439570e-02]\n"," [ 1.63321606e-01  1.28076059e-01 -1.30987835e-01  9.44917281e-02\n","  -1.26975391e-01  7.96831526e-02 -6.71004230e-02  8.06880420e-03\n","  -2.53260888e-02 -4.25178462e-01 -6.96049047e-02  7.31452832e-02\n","   2.87752356e-01  3.19734460e-02  2.97314678e-02  3.35664937e-02\n","   2.10493239e-02  3.30460140e-02  4.92744585e-02 -4.14442866e-02\n","  -9.22281493e-03  6.66997702e-02 -3.98351336e+00  5.27753826e-02\n","  -2.01225964e-02 -7.28688719e-02  4.42413757e-02 -2.43850574e-02\n","   1.14648835e-01 -2.68734249e-01  1.04388671e-01  6.72966181e-02\n","  -1.48208914e+00  6.58352230e-02 -2.59531585e-02  5.86536194e-02\n","  -2.99054881e-01  2.09221093e-01 -2.90136953e-02 -5.89487774e-02\n","   1.64591350e-01 -1.23309567e+00 -1.38777434e-01 -1.27509714e-02\n","   1.28099772e-01 -6.24602965e-02  3.23877338e-02 -4.13735318e-02\n","  -1.55262280e-01  2.17500081e-01 -1.47468383e-01  9.27518392e-01\n","  -2.50506182e-02 -1.89512806e-02 -5.39076931e-02 -5.08690964e-02\n","  -1.74191101e-01 -1.37751687e-03 -7.45075532e-02 -8.91356466e-02\n","   2.78846818e-02 -2.52322463e-02  5.81364702e-02  4.41193755e-02]] [ 3.56542917  7.23320244  1.91111697 -0.14306644  6.26903139  2.94190978\n","  6.14733648  0.08384347  1.12228372  1.18736022  0.8872537   3.16470303\n"," 11.71254123  6.3348723   0.89825177  4.76150816  2.79937492  6.00148799\n"," 12.56626932  7.89591164  2.06112674  9.4268779   8.50693247  6.21956964\n","  4.36058939  0.5850689   4.46557354  7.1194087   3.21570892  4.05907342\n","  1.52102821  8.48404762  3.12091465  6.37184784  2.48828996  6.63996058\n"," 13.04568585  5.27201882  9.31807812  3.00579896  1.26902177  9.38537654\n","  2.17916199  6.25670461  3.00003939  7.20238531  3.91804871  2.43373196\n","  4.54187559 10.03314335  0.9580552  10.30680215  4.90087623  0.51018546\n","  0.06743999  5.08752785  9.61065093 -0.08647521  0.43442822  6.03993227\n","  3.58958877  2.80085469  5.29991522  7.94436406]\n","training sepNet\n"," \n","epoch= 0  time= 0.02434849739074707  loss= 6.4700954643895585  val_loss= 11.253798454087867  f1= 5.4766599601973525  f2= 0.5421067050964071  f3= 0.4511331333315614  f4= 0.0001956657642382837 num_batches= 6 percent lr= 0.01\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:258: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:263: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","errors: tensor([0.0178, 0.0619], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.3464, 603.4436], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0012]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.3124, 4933.3837], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0414, 0.0383], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9103, 598.4197], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0016]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.9176, 5287.6807], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0301, 0.0724], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7349, 599.2307], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0015]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  91.0024, 4080.2176], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0206, 0.0261], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6397, 599.5000], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0020]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 106.3377, 5779.0003], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0094, 0.0302], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7116, 599.9673], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0010]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.0446, 5297.3827], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0132, 0.0204], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9033, 602.7444], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0134]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 119.4308, 5850.7202], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0279, 0.0607], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0685, 601.0808], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0264]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 106.8263, 4771.2682], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0190, 0.0317], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7495, 597.8643], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.7492, 5689.9301], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 630  time= 18.829835891723633  loss= 0.05754266900144975  val_loss= 3204.385078324278  f1= 0.013536750884257388  f2= 0.002212796822352432  f3= 0.011694524173529659  f4= 0.03009859712131027 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0163, 0.0809], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5976, 599.1288], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0027]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 101.8033, 4188.7877], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0658, 0.0332], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6017, 600.8042], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0010]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 121.1285, 5545.2805], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0182, 0.0413], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8222, 598.5809], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  99.4215, 5438.9651], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0144, 0.0455], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9329, 598.2232], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[7.1779e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.5084, 4681.5547], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0140, 0.0341], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7645, 596.2855], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.1407, 6147.0233], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0271, 0.0596], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7567, 596.9510], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  94.1239, 4317.4531], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0196, 0.0260], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0025, 598.9014], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 121.1569, 6129.5501], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0244, 0.0420], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0465, 602.3033], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0036]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  92.0807, 4417.5360], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0183, 0.0267], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.2975, 599.5722], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0015]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.6005, 6608.1423], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0078, 0.0460], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  7.7175, 596.8910], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[3.7831e-07]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.8913, 5070.4511], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 640  time= 19.071500778198242  loss= 0.04688420307433919  val_loss= 2891.527087266305  f1= 0.013944122011333873  f2= 0.0047911392921452444  f3= 0.0018863476408107485  f4= 0.026262594130049322 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0099, 0.0358], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  7.7663, 595.2879], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.1831, 5368.9119], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0140, 0.0259], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.2546, 599.2050], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0049]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 100.3681, 5187.8053], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0176, 0.0153], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5915, 599.1728], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.8772e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 121.9546, 6205.9209], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0184, 0.0698], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4969, 597.6032], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[6.5668e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  98.3613, 4313.1389], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0247, 0.0228], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6100, 594.7665], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[7.2320e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 124.3779, 5644.9869], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0084, 0.0215], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5540, 597.9124], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0031]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.5613, 5648.6982], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0097, 0.0189], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6755, 600.6591], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0007]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.3433, 5987.4031], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0098, 0.0303], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7306, 600.3856], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0007]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.0113, 5030.2393], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0140, 0.0191], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6487, 601.3159], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[1.7541e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.3115, 6153.6688], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0118, 0.0368], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7063, 598.7420], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 124.4370, 5625.0705], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 650  time= 19.353984832763672  loss= 0.02788535119738768  val_loss= 3178.530045655098  f1= 0.012561293546387318  f2= 0.0035252107586352093  f3= 0.00015815293842272424  f4= 0.011640693953942424 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0098, 0.0245], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8904, 600.3567], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.1838, 5807.2267], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0144, 0.0259], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0550, 598.3663], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.7772, 5676.6892], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0193, 0.0272], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.2493, 599.1532], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[1.1522e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.7272, 6537.3973], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0105, 0.0471], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0093, 598.2887], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0118]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.1812, 4842.6405], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0089, 0.0259], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9429, 597.1554], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.0808e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 123.0978, 6474.4740], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0140, 0.0387], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8966, 597.0672], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0006]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.3451, 4951.9131], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0159, 0.0220], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8739, 595.1775], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 122.3612, 5685.4919], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0145, 0.0308], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8605, 596.8107], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0009]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.5123, 5755.6437], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0122, 0.0392], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6759, 599.1546], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.0817, 4553.9843], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0145, 0.0246], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5645, 599.0078], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.0777, 5997.7104], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 660  time= 19.634757041931152  loss= 0.059892901720733464  val_loss= 3358.744005808225  f1= 0.012421419600600156  f2= 0.004608065775454103  f3= 0.0005244796864545814  f4= 0.04233893665822463 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0092, 0.0342], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5898, 599.0476], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0012]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.2815, 5506.0216], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0141, 0.0243], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9391, 599.2656], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0024]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.5251, 6204.8566], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0193, 0.0402], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6602, 596.0165], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.4457, 4880.5865], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0216, 0.0280], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6723, 597.4232], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[4.3820e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 125.8474, 5970.9629], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0173, 0.0664], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4710, 593.8724], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0016]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 101.4211, 4557.9258], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0114, 0.0252], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6460, 595.9156], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0009]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 124.4081, 5198.6770], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0146, 0.0203], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6503, 594.5717], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.4203e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  97.7206, 5795.9073], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0095, 0.0290], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8794, 598.9005], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.1721, 5280.8485], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0204, 0.0258], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8524, 597.7699], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.1502e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.5483, 5644.2876], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0169, 0.0400], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8976, 597.4144], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0015]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 121.0702, 5349.6628], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 670  time= 19.901570558547974  loss= 0.04385184053632502  val_loss= 3038.5816623064043  f1= 0.008495007737370533  f2= 0.004865777992766854  f3= 0.00041860224482732797  f4= 0.030072452561360315 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0253, 0.0327], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9956, 597.0091], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[7.5695e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  98.6341, 5588.0427], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0355, 0.0211], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9534, 600.4332], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.5593e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 122.6648, 6246.6392], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0159, 0.0304], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5084, 598.3873], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0079]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.0085, 5247.7411], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0202, 0.0216], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7666, 594.0931], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0016]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.7176, 6451.1901], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0221, 0.0490], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5261, 592.1149], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0039]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.2436, 5641.0088], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0293, 0.0418], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5166, 596.0648], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0165]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.6408, 5554.8464], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0221, 0.0532], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5038, 597.7426], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.5425e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  97.3743, 4957.8846], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0126, 0.0213], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5321, 597.9745], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0093]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 114.1568, 5192.9361], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0177, 0.0319], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5978, 595.9028], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  97.8792, 5120.8436], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0099, 0.0320], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8317, 598.3155], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[4.1663e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.8035, 5325.0637], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 680  time= 20.185404777526855  loss= 0.01989324475344242  val_loss= 3022.564088538089  f1= 0.008461546580273727  f2= 0.0025139543805900934  f3= 0.0004441722720470809  f4= 0.008473571520531518 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0218, 0.0369], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0235, 600.6770], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0014]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 100.3022, 4914.1430], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0123, 0.0181], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8807, 599.8493], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0023]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.0087, 6122.2943], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0091, 0.0353], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5586, 600.3544], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[3.3178e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 106.4832, 4901.7965], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0100, 0.0294], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7315, 597.5504], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0017]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.3797, 5532.8511], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0168, 0.0225], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.1914, 598.0971], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 114.2928, 5672.9778], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0162, 0.0360], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0417, 602.8155], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0049]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.4316, 5260.5261], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0330, 0.0241], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6413, 602.5637], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0048]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.5904, 6929.0890], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0111, 0.0623], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4876, 597.9073], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0006]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.1768, 4974.9736], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0112, 0.0171], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5485, 597.6732], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0030]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 126.3351, 7119.8556], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0221, 0.0228], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7870, 598.5486], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0013]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.3309, 5341.2036], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 690  time= 20.507421016693115  loss= 0.13735938402508122  val_loss= 3028.500536524923  f1= 0.011547964858739079  f2= 0.010589335396738819  f3= 0.0033912280477312166  f4= 0.11183085572187208 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0265, 0.0184], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8523, 601.8844], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0017]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.1465, 6184.0865], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0182, 0.0333], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6513, 598.9498], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[5.7404e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.7497, 5157.4205], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0091, 0.0280], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5890, 597.1393], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0046]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.2884, 5596.0521], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0072, 0.0334], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4582, 598.2360], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0018]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.9170, 4995.1812], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0162, 0.0466], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5270, 599.9067], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  96.8329, 4786.0444], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0142, 0.0178], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7651, 601.4932], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.0411, 6085.0263], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0115, 0.0618], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7707, 597.4617], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.0145, 4537.3614], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0536, 0.0391], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9340, 599.3457], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0027]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 128.4988, 6768.1383], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0146, 0.0329], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7407, 597.0628], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 106.8088, 5312.5340], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0178, 0.0278], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9363, 597.6537], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0107]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.0664, 5487.2821], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 700  time= 20.78316855430603  loss= 0.03827735337288782  val_loss= 3103.6146878752547  f1= 0.010732908031491224  f2= 0.0027016213505524377  f3= 0.007324746949365046  f4= 0.01751807704147911 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0181, 0.0547], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7597, 597.5315], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0011]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  98.7091, 5377.3497], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0193, 0.0295], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8998, 598.6738], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.0356e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 114.9082, 5653.5566], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0296, 0.0356], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6975, 598.5657], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0026]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 101.8847, 5743.4167], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0182, 0.0293], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8849, 600.9196], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0009]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.3213, 5427.6449], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0111, 0.0321], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8803, 599.4215], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[3.2822e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.2862, 5375.0742], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0141, 0.0264], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0122, 600.9683], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0012]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.5782, 5643.0926], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0116, 0.0210], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6909, 597.8125], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0030]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.3772, 5864.6703], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0208, 0.0335], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5539, 600.1070], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0023]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.6546, 6053.4159], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0141, 0.0410], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5806, 600.1388], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0043]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.1933, 4912.6255], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0148, 0.0284], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4836, 599.6239], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0026]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.1973, 6666.7150], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 710  time= 21.066026210784912  loss= 0.032363448165255174  val_loss= 3697.5843503184515  f1= 0.007307687663506581  f2= 0.002004481213253589  f3= 0.004278378420449663  f4= 0.018772900868045343 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0139, 0.0259], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5495, 600.3724], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0102]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.4710, 5420.4071], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0092, 0.0301], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9413, 599.0043], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.3059, 5655.8516], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0134, 0.0438], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0913, 597.6683], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0023]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.6046, 4672.5403], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0272, 0.0239], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8867, 599.5369], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 134.2348, 6757.8624], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0275, 0.0747], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5667, 599.2910], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[5.5879e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  94.4975, 4374.9344], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0085, 0.0274], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7496, 599.0942], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0134]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.4276, 5961.0206], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0071, 0.0299], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5658, 599.8814], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0117]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.6143, 5263.4159], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0129, 0.0512], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9129, 602.7970], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0006]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 114.6878, 4400.2975], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0164, 0.0276], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8418, 600.3892], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0027]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 114.0522, 5478.0835], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0106, 0.0487], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9272, 601.2272], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0022]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.2667, 4522.7513], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 720  time= 21.3508358001709  loss= 0.034541083317880354  val_loss= 2625.661349028922  f1= 0.0066533754030993104  f2= 0.00520542973225865  f3= 0.0013469521183039962  f4= 0.021335326064218407 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0174, 0.0243], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8560, 598.0625], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[3.0360e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 124.3889, 5836.5754], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0294, 0.0596], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9917, 598.1354], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0014]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 105.8317, 4821.6916], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0304, 0.0338], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7103, 595.3430], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0122]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.6709, 5821.1192], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0204, 0.0275], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8841, 599.3199], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.8674, 5953.9706], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0113, 0.0380], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8052, 597.5661], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0023]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.2322, 5252.5514], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0146, 0.0238], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7715, 597.9829], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0036]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.5243, 5731.6387], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0140, 0.0244], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6953, 600.4297], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0021]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.4343, 5213.4920], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0152, 0.0214], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6356, 601.1574], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0018]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.2468, 6402.2317], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0126, 0.0396], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5249, 598.3784], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0034]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 102.8939, 4832.5002], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0102, 0.0275], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4767, 599.7215], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.8147, 5331.5186], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 730  time= 21.639035940170288  loss= 0.020817652953063405  val_loss= 3023.808951921316  f1= 0.006207577149340319  f2= 0.0024511937045724893  f3= 0.0011878101718965906  f4= 0.010971071927254003 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0143, 0.0203], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6328, 599.0593], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[5.0640e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.3871, 5593.2398], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0101, 0.0285], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8350, 601.3605], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[3.4028e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 121.0669, 5828.3525], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0137, 0.0305], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9915, 600.1237], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 105.5089, 5174.7248], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0119, 0.0214], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9529, 599.6315], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 119.9147, 6230.4241], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0083, 0.0281], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6613, 600.3642], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.6318, 5417.3440], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0108, 0.0263], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0190, 604.1386], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0066]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.3891, 5696.5292], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0123, 0.0340], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9756, 601.5965], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[7.6123e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.7886, 5143.5745], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0202, 0.0272], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9555, 600.0961], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0084]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.1429, 5813.9250], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0149, 0.0367], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8727, 599.4401], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.5338, 5227.2510], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0102, 0.0234], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.1761, 601.8527], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0010]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 114.4137, 5321.1963], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 740  time= 21.904481649398804  loss= 0.026805245285376977  val_loss= 3023.3995334216775  f1= 0.012963063723422813  f2= 0.003656863844295474  f3= 0.0009538666526161077  f4= 0.00923145106504258 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0184, 0.0315], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8860, 598.8531], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[4.7494e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 106.0689, 5302.3921], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0075, 0.0158], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7417, 598.2258], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0012]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.3010, 6376.2652], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0115, 0.0281], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6614, 598.4184], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 106.9520, 5400.5947], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0354, 0.0243], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7792, 599.3272], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.6898, 5075.9033], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0304, 0.0696], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8031, 601.3575], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0021]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.3134, 4968.8586], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0130, 0.0274], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7909, 600.4613], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0032]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 101.4177, 5307.1312], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0099, 0.0307], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7111, 599.4098], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0015]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.5083, 5096.7534], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0239, 0.0235], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9264, 598.6195], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[6.3836e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.7731, 5677.8029], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0183, 0.0263], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6189, 598.3108], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.4064, 5711.6642], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0200, 0.0229], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6832, 600.0050], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0010]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.5959, 5471.6902], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 750  time= 22.183329105377197  loss= 0.03511128109962099  val_loss= 3092.535563836399  f1= 0.010491924283978401  f2= 0.003101669644512646  f3= 0.0004417454337714391  f4= 0.02107594173735851 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0135, 0.0202], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7677, 598.7497], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[5.6546e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 105.0097, 5808.7770], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0186, 0.0386], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9693, 595.9214], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.1802, 4935.1690], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0147, 0.0332], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7807, 596.9033], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0016]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 122.2753, 6140.3665], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0149, 0.0257], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8058, 599.0474], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.5807, 5650.1876], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0191, 0.0222], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9834, 598.6854], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[6.0214e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 102.4290, 6052.2171], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0086, 0.0207], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0312, 596.2281], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[8.1577e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.1396, 5584.6586], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0109, 0.0164], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8022, 597.7336], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0007]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.2620, 6301.7897], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0103, 0.0293], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4978, 600.1527], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.8508, 5436.2039], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0105, 0.0194], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6625, 596.5534], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0007]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.5157, 6397.0599], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0099, 0.0263], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7380, 594.2953], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.5305, 5458.6251], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 760  time= 22.494524240493774  loss= 0.03272318010419062  val_loss= 3085.6877425338976  f1= 0.0036675728137358712  f2= 0.0032245995941214867  f3= 0.0008153169335108659  f4= 0.025015690762822385 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0127, 0.0219], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8373, 600.1094], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0012]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.5394, 5615.2483], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0106, 0.0163], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8101, 596.6015], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0023]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.9956, 6323.1767], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0079, 0.0329], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8164, 598.2250], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.0359, 5619.4647], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0165, 0.0278], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9587, 599.1033], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.2828, 5159.3613], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0180, 0.0255], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8302, 599.6949], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 127.9824, 6975.1682], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0171, 0.0405], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6467, 597.9192], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 102.7992, 4945.0837], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0141, 0.0251], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5786, 597.3031], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.9827, 6676.7181], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0161, 0.0281], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4941, 595.2317], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.5520, 6128.1577], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0383, 0.0462], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7517, 599.0091], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  98.7409, 4719.6372], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0287, 0.0423], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5023, 598.3846], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0014]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 122.5248, 7249.0108], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 770  time= 22.781059741973877  loss= 0.10025959341739121  val_loss= 3989.3827610143326  f1= 0.017691583120013517  f2= 0.005724629483235422  f3= 0.00046469341787158756  f4= 0.0763786873962707 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0127, 0.0613], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.3451, 597.1761], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0011]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 102.0984, 4380.6215], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0302, 0.0495], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.2181, 594.4239], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0009]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  90.3296, 4952.6851], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0183, 0.0189], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.0881, 594.8530], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.6135, 6567.5496], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0142, 0.0512], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.2896, 596.0410], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0146]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  96.8034, 4731.0927], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0248, 0.0223], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6135, 600.0262], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.3259e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 123.2499, 7126.1114], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0126, 0.0406], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7725, 600.4712], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.8444, 4951.4306], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0216, 0.0209], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8604, 597.8310], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0011]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 123.8351, 6615.9331], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0166, 0.0610], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9657, 598.4877], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0007]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.0347, 4713.7174], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0139, 0.0176], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8849, 599.2462], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 119.0172, 5913.6658], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0121, 0.0396], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8055, 602.8291], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.2945, 4826.8538], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 780  time= 23.04440402984619  loss= 0.021569527402673195  val_loss= 2773.9694831162983  f1= 0.005863320937287322  f2= 0.0033030731899736866  f3= 0.0021764909095480533  f4= 0.010226642365864132 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0131, 0.0243], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4355, 599.4028], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0032]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.4704, 6085.4441], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0143, 0.0359], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5900, 596.6065], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0013]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.3841, 5202.0797], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0146, 0.0430], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8032, 596.1628], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0054]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 106.8448, 5113.7104], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0115, 0.0303], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8694, 600.2370], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.1688, 5340.6580], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0141, 0.0163], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6938, 599.3940], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0048]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.3359, 6035.3873], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0107, 0.0336], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5476, 597.0486], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0019]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.0782, 5046.6112], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0140, 0.0235], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6849, 597.5968], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0038]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.2636, 5850.3872], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0118, 0.0265], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6063, 599.4200], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[3.7750e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.6101, 5519.7072], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0138, 0.0344], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7518, 598.7117], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0023]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.8894, 4910.1380], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0201, 0.0212], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4905, 597.5023], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0021]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.3878, 5618.4045], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 790  time= 23.33296847343445  loss= 0.0588231906213279  val_loss= 3166.575166324737  f1= 0.020887562238797508  f2= 0.0017373149743836545  f3= 0.0007962670416622714  f4= 0.03540204636648447 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0161, 0.0336], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4381, 598.6191], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[9.4231e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.9337, 5342.9394], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0105, 0.0311], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6935, 596.4573], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.2529, 5469.7181], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0134, 0.0274], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9612, 597.7213], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.0039, 5043.5218], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0138, 0.0140], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9649, 600.5126], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0025]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 119.6877, 6625.6532], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0152, 0.0256], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7667, 599.9408], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0013]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 102.5216, 5336.8853], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0093, 0.0290], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6930, 597.6136], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 123.6082, 5897.0820], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0106, 0.0245], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5403, 595.9164], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.7671, 5477.8739], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0177, 0.0350], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7504, 600.1479], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 100.1811, 5159.2118], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0220, 0.0150], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5617, 597.7746], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 124.7984, 6278.3223], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0191, 0.0467], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6928, 596.0949], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0035]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  98.5084, 4568.0592], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 800  time= 23.589582204818726  loss= 0.02307135553269468  val_loss= 2635.8308148782535  f1= 0.006324392802787488  f2= 0.002975453331171624  f3= 0.0021206448197489034  f4= 0.011650864578986666 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0163, 0.0188], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7400, 594.8862], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0031]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 105.2360, 5783.0933], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0114, 0.0237], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5345, 600.0378], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0072]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 124.6287, 5819.8476], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0258, 0.0364], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5860, 598.6230], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0057]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.5931, 4828.2705], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0176, 0.0159], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7494, 597.1778], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.8142, 6118.8699], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0212, 0.0521], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6909, 595.1565], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[7.4731e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.2949, 4596.7402], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0422, 0.0230], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6288, 596.3335], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[6.4504e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.6259, 5831.2236], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0151, 0.0286], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6690, 600.3708], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[7.9728e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.6184, 5826.2705], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0147, 0.0372], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0084, 598.2941], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0152]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.1355, 5002.4397], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0112, 0.0448], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7218, 596.8908], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0029]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.2828, 5054.3285], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0270, 0.0324], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6547, 594.0875], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0010]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.8152, 6122.7884], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 810  time= 23.854434967041016  loss= 0.06391446437269012  val_loss= 3419.2981722389586  f1= 0.010916446684014364  f2= 0.0035453709478701304  f3= 0.0012230958253103559  f4= 0.04822955091549528 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0276, 0.0522], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7241, 597.9447], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0018]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  95.3905, 4527.0036], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0135, 0.0243], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8340, 598.6498], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0036]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 123.9141, 6108.5201], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0210, 0.0599], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8555, 597.5402], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0106]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 101.8552, 4335.0994], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0149, 0.0344], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0553, 597.4045], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0053]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 106.2527, 5140.1359], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0127, 0.0259], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8247, 600.2882], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[1.4031e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.7609, 5813.3617], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0205, 0.0512], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7999, 600.6918], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0055]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.6294, 4795.0022], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0091, 0.0174], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7284, 599.8248], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 114.3602, 5617.5884], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0144, 0.0323], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9469, 596.4028], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[6.9574e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.3873, 5282.3317], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0162, 0.0449], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9737, 595.4322], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.6913, 5101.3170], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0150, 0.0164], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0055, 594.6268], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[3.2905e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.1574, 6362.5379], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 820  time= 24.09461545944214  loss= 0.02774011251824078  val_loss= 3539.7091276073998  f1= 0.008529631756877194  f2= 0.0015538649367519048  f3= 0.00030189983843489985  f4= 0.017354715986176782 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0179, 0.0310], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9869, 597.2950], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0034]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  98.6208, 5262.6915], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0124, 0.0280], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7300, 597.2645], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.1949, 5100.0638], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0391, 0.0328], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5032, 596.5674], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 105.3123, 5718.5628], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0105, 0.0347], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5098, 598.4550], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0032]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.1803, 5087.3200], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0170, 0.0207], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7372, 596.2340], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.3566, 5612.6225], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0099, 0.0264], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9614, 597.1520], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.8555, 5253.5150], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0108, 0.0204], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0087, 599.1863], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[9.4913e-07]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.5845, 5519.3365], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0168, 0.0173], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7247, 596.4044], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.8163, 5517.3736], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0087, 0.0146], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5530, 596.6476], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0026]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.0269, 6042.9014], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0130, 0.0317], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7412, 596.1696], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[1.7468e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.0109, 5027.3438], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 830  time= 24.348278284072876  loss= 0.020320644466162555  val_loss= 2870.20528209961  f1= 0.0030568733749808882  f2= 0.0019486361576591873  f3= 0.0028845092197567865  f4= 0.012430625713765691 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0114, 0.0156], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0518, 598.4256], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0054]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.6201, 6369.0581], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0140, 0.0331], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0101, 599.6166], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[3.2858e-07]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 105.9393, 5329.8739], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0191, 0.0155], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9151, 596.3438], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.1022, 6311.3315], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0096, 0.0282], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8470, 596.2025], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0018]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.3942, 5621.6226], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0160, 0.0206], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9210, 597.2672], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[9.6711e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 114.0616, 5622.7550], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0107, 0.0155], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9236, 597.7280], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.0480, 6939.0897], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0104, 0.0284], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7098, 594.6113], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0044]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.6563, 5395.8175], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0140, 0.0162], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6504, 591.7347], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[4.6956e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 114.5121, 5977.1449], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0162, 0.0201], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7733, 597.1050], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0016]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 105.1578, 6039.8391], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0174, 0.0455], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9702, 599.4025], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0036]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.0877, 4579.1948], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 840  time= 24.60904359817505  loss= 0.015615682786716665  val_loss= 2648.059374601069  f1= 0.006208653913146869  f2= 0.002577174806307591  f3= 0.0008412350362103129  f4= 0.005988619031051895 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0178, 0.0203], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6640, 599.0783], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0071]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 126.2821, 5996.3633], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0135, 0.0195], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4441, 595.1364], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[9.8726e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 102.4237, 5645.6524], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0153, 0.0306], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7606, 595.5318], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0021]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.9038, 6039.2059], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0218, 0.0171], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9099, 596.6671], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0024]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.9066, 5787.0485], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0224, 0.0363], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7525, 595.4914], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  99.3366, 5436.7404], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0126, 0.0167], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6441, 595.3050], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0014]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.9964, 5861.0273], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0172, 0.0457], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8452, 598.0557], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  99.5883, 5041.7178], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0140, 0.0111], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8031, 599.5917], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[1.7198e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 114.2974, 6428.7909], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0123, 0.0298], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8029, 599.0643], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0043]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.8981, 4974.7978], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0102, 0.0194], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8072, 596.1602], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0143]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 114.1883, 6125.4305], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 850  time= 24.86422348022461  loss= 0.03463255078605371  val_loss= 3422.4202678311253  f1= 0.009899884195642595  f2= 0.002052358151184782  f3= 0.004417870733832017  f4= 0.018262437705394315 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0101, 0.0413], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0889, 597.5441], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0017]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.4565, 4932.8381], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0180, 0.0229], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9153, 595.1054], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.6203, 6576.7589], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0158, 0.0313], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8011, 591.9812], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 105.9405, 5812.1413], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0157, 0.0290], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5617, 595.7827], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  97.1201, 4844.4759], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0131, 0.0257], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5581, 597.7070], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[8.2008e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.5151, 6030.1457], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0145, 0.0259], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4170, 595.9695], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.3989, 5736.0270], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0237, 0.0357], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7590, 597.0573], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0024]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  98.8479, 5449.1255], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0153, 0.0306], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9094, 595.7704], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0016]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.5259, 5522.8003], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0169, 0.0136], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7566, 595.3525], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.8502, 5982.3620], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0120, 0.0257], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7276, 600.0322], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0014]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.7641, 5761.5577], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 860  time= 25.116846323013306  loss= 0.02169190815801392  val_loss= 3240.596225210402  f1= 0.005668669459723293  f2= 0.0029481746889960365  f3= 0.0019744095207870423  f4= 0.01110065448850755 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0115, 0.0203], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8260, 596.8525], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0039]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.9659, 6085.3287], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0099, 0.0187], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8974, 592.1133], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.3697, 5626.6720], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0078, 0.0204], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8384, 595.6802], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0016]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 126.5857, 5249.7632], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0094, 0.0199], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0739, 602.9102], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0007]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.8151, 5573.7178], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0118, 0.0254], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.1049, 601.3991], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0042]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.6575, 5735.8165], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0187, 0.0155], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0878, 594.9015], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0069]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 128.6272, 6291.7521], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0127, 0.0125], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8200, 593.6549], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0010]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.0075, 6021.9543], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0083, 0.0163], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.2519, 600.8765], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0010]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 119.1175, 6138.6577], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0123, 0.0169], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.1308, 599.5074], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[8.6275e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.9240, 5904.3447], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0069, 0.0224], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.2100, 597.7193], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0056]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 123.7612, 5779.4014], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 870  time= 25.387950658798218  loss= 0.04930403870736814  val_loss= 3255.096055802658  f1= 0.007281050420291626  f2= 0.00557782154884407  f3= 0.0034638484116935064  f4= 0.03298131832653894 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0140, 0.0197], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0538, 595.8814], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0009]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 105.5635, 5449.8309], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0086, 0.0137], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8853, 599.1087], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0030]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.2912, 6822.4949], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0104, 0.0316], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5251, 598.0284], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0092]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 106.9777, 5066.5537], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0100, 0.0156], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5172, 594.2383], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0047]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.8342, 6046.4889], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0084, 0.0344], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6054, 598.4701], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0021]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.4457, 5084.2119], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0102, 0.0161], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7056, 598.7556], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0006]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 106.1239, 6184.0509], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0105, 0.0402], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8547, 596.5709], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.5874, 5010.6112], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0055, 0.0160], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7390, 592.7017], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0011]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.7356, 6144.1972], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0051, 0.0224], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8804, 595.9644], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.6255, 5569.7491], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0149, 0.0192], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0462, 597.9836], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[3.4874e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 122.2849, 5456.3480], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 880  time= 25.66379690170288  loss= 0.04317011705679443  val_loss= 3093.127251722367  f1= 0.015229895927454662  f2= 0.002651700705857452  f3= 0.00031670097372085487  f4= 0.024971819449761467 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0524, 0.0344], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9566, 599.5347], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[6.5480e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 121.3825, 6828.6917], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0142, 0.0565], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.3686, 598.7313], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0015]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.2014, 5208.4590], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0067, 0.0227], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.3944, 598.5813], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0007]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.3932, 6103.2793], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0170, 0.0355], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6048, 598.3627], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0014]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([  99.4723, 5171.0233], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0152, 0.0205], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5371, 596.0548], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.6589e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.3232, 5906.7743], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0153, 0.0412], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.3562, 597.7925], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0035]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 102.1740, 5335.0264], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0161, 0.0172], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4333, 595.5812], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0012]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 123.5475, 6314.6716], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0177, 0.0312], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7514, 595.8193], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 101.6050, 5451.2003], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0114, 0.0227], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8803, 597.8104], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0006]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.4315, 5776.8006], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0108, 0.0167], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6455, 597.7401], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.6252, 6108.1178], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 890  time= 25.955739498138428  loss= 0.04735163905555071  val_loss= 3416.601955651463  f1= 0.005779158916904945  f2= 0.005048530266907544  f3= 0.0004498991877637576  f4= 0.036074050683974464 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0078, 0.0279], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7111, 598.2038], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.5906, 5364.1863], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0097, 0.0168], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8199, 596.1064], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0025]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.6558, 6322.2168], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0100, 0.0336], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9256, 599.0413], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0040]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.2924, 5222.1617], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0097, 0.0220], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8512, 596.7283], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[8.0101e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.9973, 5664.3015], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0134, 0.0350], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0771, 596.4855], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.2230e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.7139, 4782.8226], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0104, 0.0328], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8972, 595.6940], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[9.3494e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.2424, 5485.7098], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0120, 0.0246], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0681, 596.6602], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0009]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 126.8002, 5717.5806], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0144, 0.0440], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9132, 597.7992], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0024]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.4787, 5491.2183], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0202, 0.0135], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.1837, 602.7904], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[7.5480e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 124.4806, 6040.4545], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0108, 0.0272], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6794, 601.8575], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.3229e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.3036, 5870.0434], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 900  time= 26.227508306503296  loss= 0.047919780331511834  val_loss= 3296.475936363531  f1= 0.008725388694514046  f2= 0.0026231981238004454  f3= 0.00036162853750763645  f4= 0.0362095649756897 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0086, 0.0155], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.2442, 595.5675], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[9.2438e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.6532, 5882.6829], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0109, 0.0196], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6695, 594.3141], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.4181e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 105.5781, 5873.5565], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0211, 0.0260], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8701, 600.4925], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 126.1955, 5345.8791], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0112, 0.0229], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9113, 601.6650], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0072]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 121.8331, 6435.5705], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0092, 0.0293], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5991, 596.1486], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0089]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.6080, 5254.5290], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0116, 0.0285], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6131, 594.8871], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.4298, 6063.4345], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0115, 0.0326], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6045, 596.6990], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[8.6341e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.1950, 5015.2256], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0211, 0.0188], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4867, 597.4303], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 121.3883, 6193.9222], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0087, 0.0248], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5858, 596.4902], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0124]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 119.1236, 5926.2580], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0107, 0.0351], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6151, 595.8998], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0020]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.7355, 4883.4320], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 910  time= 26.505362033843994  loss= 0.02720257567514092  val_loss= 2798.0225171983657  f1= 0.005240070361622271  f2= 0.001820570363208515  f3= 0.006946764709485216  f4= 0.013195170240824915 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0104, 0.0232], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7791, 598.0106], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0011]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 124.4922, 6782.1777], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0065, 0.0343], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6149, 598.6194], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0006]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.7648, 5553.4977], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0155, 0.0131], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6170, 592.8194], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 128.5838, 6203.4901], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0113, 0.0383], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5158, 592.5249], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[4.1819e-06]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 104.1908, 4906.2458], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0139, 0.0132], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4901, 593.7235], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.4580e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.7215, 5953.7450], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0107, 0.0236], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.2664, 600.2464], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0081]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.6907, 5801.3547], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0125, 0.0208], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5742, 600.1977], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0013]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.4099, 5485.7340], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0239, 0.0196], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8494, 597.8607], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.5349, 6116.3520], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0137, 0.0239], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8752, 595.5001], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.4808, 5662.5338], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0099, 0.0155], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8287, 598.5312], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 128.1142, 6042.4908], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 920  time= 26.77677059173584  loss= 0.02595947030120661  val_loss= 3389.016177379617  f1= 0.005142759851546388  f2= 0.0018166768914082455  f3= 0.001067109114116888  f4= 0.01793292444413509 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0100, 0.0307], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7377, 601.2605], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0027]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.7902, 4994.2278], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0121, 0.0201], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7365, 598.9107], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0001]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 121.6738, 5783.1955], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0086, 0.0145], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7242, 596.5027], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0007]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.0937, 6246.0433], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0052, 0.0330], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8181, 595.4099], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0033]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 119.5983, 5150.4726], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0147, 0.0135], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0095, 596.2736], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0036]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 123.2558, 6202.4451], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0117, 0.0338], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9509, 596.6601], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.6722e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.7682, 5145.3561], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0187, 0.0130], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9379, 598.5480], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[3.0081e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 136.1503, 7029.0823], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0160, 0.0435], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9926, 596.9214], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0010]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.4158, 4912.6697], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0277, 0.0397], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8719, 597.2672], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0097]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 132.8471, 5737.1455], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0192, 0.0329], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7561, 595.5871], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0009]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.1042, 5304.6881], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 930  time= 27.04178786277771  loss= 0.029678149011594195  val_loss= 3009.1877886656184  f1= 0.012962026033181433  f2= 0.001654633630337159  f3= 0.0046331462495724715  f4= 0.010428343098503129 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0144, 0.0259], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9103, 595.7531], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0229]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.0234, 5847.9293], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0081, 0.0351], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6359, 598.3867], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0114]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.0957, 5051.0788], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0279, 0.0215], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7719, 597.6848], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 127.4607, 6231.9743], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0107, 0.0211], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6048, 593.3566], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.9554, 5810.3262], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0223, 0.0201], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7038, 594.3999], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[4.4305e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 121.0470, 6236.6548], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0123, 0.0334], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7018, 599.0624], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.7870, 5394.5826], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0099, 0.0211], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0272, 600.9608], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0028]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.7406, 6012.9664], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0072, 0.0309], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8491, 595.2163], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.0554, 5040.8071], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0084, 0.0163], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8417, 592.9885], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0021]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.8739, 6584.3490], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0110, 0.0212], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9842, 593.9557], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0018]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 109.4364, 5448.5158], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 940  time= 27.321796655654907  loss= 0.05527494510392683  val_loss= 3080.577955556318  f1= 0.0040085300828454325  f2= 0.0035801124009710995  f3= 0.001751943186118125  f4= 0.04593435943399218 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0081, 0.0260], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0464, 602.3155], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0011]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 122.4021, 5682.9212], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0213, 0.0181], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8997, 601.5680], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0006]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.8512, 6371.1830], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0135, 0.0601], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7463, 598.7131], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[1.6413e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 117.3540, 4512.7339], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0278, 0.0269], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7501, 596.3921], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0004]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 127.7463, 6209.9171], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0088, 0.0330], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0946, 600.7363], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0046]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.9309, 5289.0108], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0153, 0.0170], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.2144, 598.9025], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0051]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 119.1772, 5894.1333], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0103, 0.0510], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8064, 595.5741], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.2448, 4982.9015], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0095, 0.0231], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.3980, 593.7163], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[7.0114e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.6321, 5469.7319], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0107, 0.0170], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6140, 598.1320], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.5547e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.5956, 6097.0273], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0145, 0.0202], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9177, 598.9363], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[2.7127e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.3779, 5525.5619], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 950  time= 27.597970485687256  loss= 0.022212989989138194  val_loss= 3122.460015710881  f1= 0.004793436212044531  f2= 0.0012545094496920395  f3= 0.0003777852856420942  f4= 0.015787259041759528 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0143, 0.0277], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9287, 596.6438], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0015]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 125.6819, 6504.9613], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0082, 0.0178], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7323, 595.8212], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[7.3951e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.8197, 6302.7019], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0080, 0.0181], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6622, 598.4093], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0017]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.9486, 5637.7164], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0104, 0.0143], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6056, 596.0247], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[4.2992e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.0304, 5980.0065], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0088, 0.0232], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6737, 597.6363], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0010]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.0877, 5410.2252], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0116, 0.0232], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8358, 596.6432], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0007]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.3913, 6227.4232], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0106, 0.0418], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7017, 595.6899], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0026]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 114.6922, 5232.4461], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0096, 0.0321], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7314, 597.8756], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.6526, 5904.1197], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0233, 0.0187], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6847, 596.8364], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[9.9891e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 125.4275, 5799.0439], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0169, 0.0306], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7897, 595.8943], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0006]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 114.9693, 5972.4779], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 960  time= 27.895233154296875  loss= 0.026800087685814492  val_loss= 3346.10847714896  f1= 0.006816952323339756  f2= 0.0023076404771101644  f3= 0.0002640977733187386  f4= 0.017411397112045833 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0123, 0.0159], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8046, 596.0499], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0010]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.5730, 5813.9228], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0117, 0.0294], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9063, 600.3585], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[1.6153e-07]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.1760, 5894.4873], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0098, 0.0260], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9663, 598.7901], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.0969, 5278.5922], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0132, 0.0220], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0234, 596.4310], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 128.0406, 6752.9314], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0197, 0.0550], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9250, 595.4573], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0006]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 116.0531, 4790.9233], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0125, 0.0122], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8579, 597.9912], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 127.2518, 6608.4495], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0163, 0.0145], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7461, 595.5696], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 105.8542, 6067.7289], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0080, 0.0582], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8550, 596.7393], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.3095, 4009.2238], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0217, 0.0364], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6846, 596.1164], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0003]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 103.2447, 4773.1340], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0157, 0.0182], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5814, 597.1242], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[4.9927e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 123.4040, 6488.4508], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 970  time= 28.154406785964966  loss= 0.03404774768777538  val_loss= 3608.8474651564484  f1= 0.012484558784556019  f2= 0.0014998141011366734  f3= 0.0004631257456723528  f4= 0.019600249056410342 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0087, 0.0293], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4356, 595.2624], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[6.3646e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.0022, 5264.9462], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0052, 0.0151], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5403, 592.7766], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0019]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 111.3983, 6079.5340], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0057, 0.0227], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5667, 595.7236], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0027]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.8978, 5592.8122], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0077, 0.0177], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5009, 594.9577], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0010]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 118.8523, 5558.5422], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0097, 0.0179], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5193, 598.2903], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0014]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.1345, 5874.3269], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0176, 0.0361], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.4697, 594.1040], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0008]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 105.9541, 5044.7761], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0593, 0.0449], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8066, 595.7941], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0104]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 146.8330, 7264.5988], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0167, 0.0407], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0521, 595.8385], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.7895, 5090.0053], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0080, 0.0149], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8214, 596.0951], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0082]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 121.0808, 6182.6020], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0095, 0.0163], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6356, 598.2016], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0024]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 119.5607, 5942.4198], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 980  time= 28.42133927345276  loss= 0.04615443981119804  val_loss= 3334.457880620368  f1= 0.003913223473647517  f2= 0.0019409195277299057  f3= 0.021600214195871256  f4= 0.01870008261394936 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0131, 0.0200], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7776, 593.7332], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0114]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.7384, 5684.9757], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0258, 0.0356], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6500, 597.3099], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0048]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 113.1934, 5722.2010], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0115, 0.0186], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8820, 598.2514], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[3.0708e-05]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 126.7472, 6189.9725], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0092, 0.0421], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9225, 598.6000], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0015]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 112.9758, 4956.8301], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0267, 0.0168], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8464, 598.6520], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0015]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 130.9576, 6232.5229], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0100, 0.0262], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6742, 595.3350], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0041]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 125.1783, 5486.8411], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0153, 0.0157], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8899, 596.1682], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0048]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 123.7613, 5812.6682], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0093, 0.0139], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8346, 599.1192], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0010]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 130.6279, 5819.9594], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0180, 0.0200], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  9.0268, 595.5854], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 122.1222, 5570.7884], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0087, 0.0161], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9749, 595.6709], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0012]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 127.6674, 6133.1866], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 990  time= 28.693586587905884  loss= 0.04724955566035943  val_loss= 3432.777737650472  f1= 0.009965828752435459  f2= 0.0009492790546625367  f3= 0.00023989673014259197  f4= 0.03609455112311885 num_batches= 6 percent lr= 0.01\n","errors: tensor([0.0102, 0.0359], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9756, 595.3913], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0084]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 115.5646, 5482.4124], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0099, 0.0227], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.9359, 600.6571], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0012]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 120.4768, 5350.8155], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0112, 0.0360], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.7159, 597.5531], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0006]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 110.1474, 4961.2404], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0079, 0.0125], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5188, 596.1207], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0017]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 123.1899, 6490.4007], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0100, 0.0153], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5860, 596.1936], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0062]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 122.6214, 5722.4031], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","errors: tensor([0.0112, 0.0380], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8740, 597.1259], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0083]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.9362, 5106.4787], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","EarlyStopping counter: 996 out of 1000\n","errors: tensor([0.0125, 0.0167], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6180, 599.2463], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0028]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 126.2371, 6842.1856], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","EarlyStopping counter: 997 out of 1000\n","errors: tensor([0.0107, 0.0509], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.5068, 597.5041], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0005]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 107.1827, 4744.9972], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","EarlyStopping counter: 998 out of 1000\n","errors: tensor([0.0326, 0.0244], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.6386, 596.6110], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[3.0540e-08]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 133.2763, 6573.2872], dtype=torch.float64, grad_fn=<MeanBackward1>)\n","EarlyStopping counter: 999 out of 1000\n","errors: tensor([0.0102, 0.0281], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([  8.8414, 595.0302], dtype=torch.float64, grad_fn=<MeanBackward1>) tensor([[0.0002]], dtype=torch.float64, grad_fn=<PowBackward0>) tensor([ 108.9600, 5120.1750], dtype=torch.float64, grad_fn=<MeanBackward1>)\n"," \n","epoch= 1000  time= 28.995956897735596  loss= 0.04769878020684479  val_loss= 2916.635165236263  f1= 0.013672951449968182  f2= 0.002065915974559025  f3= 0.0031748770683817907  f4= 0.02878503571393579 num_batches= 6 percent lr= 0.01\n","EarlyStopping counter: 1000 out of 1000\n","Early Stopping\n","[[ 0.39050803  1.72151493  0.82210701 ...  1.98930896  3.22975792\n","  -3.33262052]\n"," [ 0.41753976  0.67580855  3.69549103 ... -0.4309159  -3.16297689\n","  -1.21219209]\n"," [ 1.9207802   1.44411585  0.97907543 ...  3.56021422  3.93512263\n","  -0.98606986]\n"," [ 3.72917957  2.33503656  1.40551318 ...  0.9639992   1.11697794\n","   3.58832241]] [[ 1.92067571  1.44410224  0.97907467 ...  3.56020242  3.93512089\n","  -0.98606613]\n"," [ 3.72906783  2.33503122  1.40550978 ...  0.96400176  1.11697966\n","   3.58832377]\n"," [-2.08824967 -0.27211309 -0.01515107 ... -0.23586191 -0.03496077\n","   0.07473017]\n"," [-2.23324977 -0.10683637 -0.06810352 ...  0.05108115  0.03423521\n","   0.02717758]] [ 7.04890611e+00  3.22822124e+00  1.20288518e+00  2.91307573e+00\n","  4.89627738e+00  4.07690114e+00  6.45673862e+00  5.74413723e+00\n"," -1.21676927e-01  1.10641907e+00  9.19478313e+00  3.83573901e+00\n","  1.16350503e+01  3.96176940e-01  3.59668382e+00  8.71463702e+00\n","  1.02039972e+01  3.84170586e+00  8.14427014e+00  6.34816543e+00\n","  4.67722860e-01  6.19336000e+00  1.92575365e+00  6.51411850e+00\n","  2.00132113e-01  5.56885455e+00  7.86032939e+00  1.22451365e+01\n","  7.47838353e+00  2.47519278e+00  2.08306066e+00 -1.93588616e-01\n","  1.07787980e+01  7.28917730e+00  1.25385305e+00  3.30205061e+00\n","  2.69919260e+00  1.90331589e+00  9.03063116e+00  5.51384601e+00\n","  4.87136931e+00  2.00495113e+00  1.61042116e+00  5.63343076e+00\n","  3.51770209e+00  4.35587477e+00  4.91927444e+00  7.21447771e+00\n","  3.06167800e+00  4.18410694e+00  8.98100201e+00  4.16905567e+00\n"," -2.27262120e-01  1.34922590e+01 -8.16320624e-02  1.15740495e+01\n","  5.92701177e+00  1.27391679e+01  7.51790371e+00  1.06509758e+01\n","  5.96359032e+00  3.79371777e+00  6.58137772e+00  5.71390095e+00\n","  5.02240785e+00  5.06054047e+00  2.97779411e+00  5.05886524e+00\n","  8.89928743e+00  7.02177452e+00  1.24197397e+00  6.58862794e+00\n","  1.21099553e+00  4.68041900e+00  1.14060243e+01  9.18939276e+00\n","  5.93466178e+00  9.70422694e+00  3.22097234e+00  5.06619131e+00\n","  1.05073816e+00 -7.84733767e-01  5.78631234e+00  9.31674643e+00\n","  7.58345941e+00  9.07653051e-01  3.41138994e+00  8.57073312e+00\n","  6.32133496e+00  7.48116860e-01  4.49545859e+00  6.25245635e+00\n","  6.15015907e+00  5.50383354e+00  2.51384244e+00  2.79039095e+00\n"," -7.97688652e-01  7.16341716e+00  3.65217062e+00  2.49912174e+00\n","  1.11101061e+01  4.01554906e+00  9.81207035e+00  3.75716577e+00\n"," -1.22160573e-02  1.47255799e+00  1.88534337e-01  6.58132873e+00\n","  5.80341514e-01  1.94492212e+00  5.57342616e+00  4.50823075e+00\n","  8.63678699e+00  5.82435852e+00  2.09691575e+00  3.12983063e+00\n","  3.38989209e+00  3.95676737e+00  8.19845123e+00  3.59204127e+00\n","  3.92353389e+00  4.02856629e+00  2.22364417e+00  6.00621077e+00\n","  4.42972375e+00  3.99948364e+00  5.37823265e-01  1.33862132e+00\n","  7.83511941e+00  6.32852157e+00  7.41829566e+00  1.84116590e-01\n","  3.22679561e+00  4.15322245e+00  3.47642722e+00  5.94360763e+00\n","  8.22449226e+00  1.04683031e+01  4.36337880e+00  8.17791930e-01\n","  5.21349460e+00  7.58587804e+00  8.99793994e+00  1.22713043e+01\n","  7.02681200e+00  9.67091354e+00  4.06562849e+00  7.30885462e+00\n","  4.51862205e-01  4.62708562e+00  6.48622446e+00  2.49989758e+00\n","  7.37422257e-01  7.09412906e-02  7.43334341e-01  5.56949686e+00\n","  1.32203040e+00  3.55333821e+00  1.52831870e+00  6.71100263e+00\n","  2.50785902e+00  8.22582415e+00  8.24571010e+00  7.11398101e+00\n","  2.34494330e+00  4.96413738e+00  4.62269768e+00  3.22335266e+00\n","  1.12447859e+00  4.54340385e+00  7.73701717e+00  4.75314483e+00\n","  8.70796960e+00  2.50256815e+00  6.95051063e+00  1.54529125e+00\n","  6.67678531e+00  5.90864550e+00  5.23417259e+00  7.69052657e+00\n","  1.33123366e+00  2.96276977e+00  2.00277845e+00  7.31289712e+00\n","  6.43794401e+00  8.44034567e+00  2.99842311e+00  4.25357416e+00\n","  1.72754734e-01  1.95583146e+00  9.25405992e+00  1.01353666e+01\n","  4.76925510e+00  5.39501919e+00  6.19880258e+00  5.82310089e+00\n","  6.71086427e+00  4.87017525e+00  3.46494768e+00  1.34892141e+00\n","  7.86451514e+00  4.40289456e+00  5.13055012e+00  5.98582197e+00\n","  6.07811195e+00 -2.51405883e-01  8.14861479e+00  4.97428975e+00\n","  1.58319895e+00  7.00537758e+00  8.07390492e+00  4.91235937e+00\n","  5.72829754e+00  7.20941379e+00  2.50878749e+00  7.46842103e+00\n","  7.30095429e+00  2.64614173e-01  4.19364425e+00  3.75736265e+00\n","  2.67841384e+00  3.97440189e+00  7.38591788e+00  1.64737806e+00\n","  3.69177419e+00 -2.30501807e-01  6.16344049e+00  8.46023684e+00\n","  6.36381162e+00  1.27477963e+01  9.77454547e+00  4.05412874e+00\n","  4.16691527e-01  1.30270002e+01  4.34713027e+00  1.88841562e+00\n","  1.33149862e+01  3.36262108e+00  7.40428594e-01  6.61563700e+00\n","  8.02868824e+00  6.28354580e+00  1.45534560e+00  9.14855058e+00\n","  1.53542433e+00  7.25397798e+00  7.59577174e+00  9.97721079e-01\n","  3.80395479e+00  1.86295240e+00  2.91889030e-02  2.71355399e+00\n","  6.52513656e+00  6.31091694e+00  8.14520501e+00  6.64220638e+00]\n","training sepNet\n"," \n","epoch= 0  time= 0.03215312957763672  loss= 5.891476943894342  val_loss= 11.37399560293467  f1= 5.091985178064993  f2= 0.3395499094640426  f3= 0.45973109578853016  f4= 0.00021076057677521013 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 10  time= 0.310530424118042  loss= 3.266702391132421  val_loss= 3.9686696245239683  f1= 2.4568945619908873  f2= 0.33591703590732896  f3= 0.014231552517480393  f4= 0.45965924071672487 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 20  time= 0.604691743850708  loss= 1.871123841284333  val_loss= 1.4632513913515637  f1= 0.4397586355757272  f2= 0.33456218708104096  f3= 0.07337170619829371  f4= 1.023431312429271 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 30  time= 0.8906393051147461  loss= 1.8301387729401923  val_loss= 1.4734160449240687  f1= 0.3882852166276554  f2= 0.3279053798338811  f3= 0.08074596574560534  f4= 1.033202210733051 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 40  time= 1.1844584941864014  loss= 1.6102926983007955  val_loss= 1.056994417561945  f1= 0.33299403485600365  f2= 0.3134335170405866  f3= 0.011415167267701901  f4= 0.9524499791365035 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 50  time= 1.486558437347412  loss= 1.4281317990003357  val_loss= 0.8410506637605226  f1= 0.3059113226942496  f2= 0.26120613428701267  f3= 0.0670994749428102  f4= 0.7939148670762636 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 60  time= 1.755629062652588  loss= 0.8960824907588866  val_loss= 0.7380439183426168  f1= 0.21890875178983527  f2= 0.15981312302096767  f3= 0.15763008250769264  f4= 0.3597305334403911 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 70  time= 2.027858257293701  loss= 0.7781531037048196  val_loss= 0.5199278853963168  f1= 0.1949934040376505  f2= 0.14948086587781748  f3= 0.04039073563003139  f4= 0.3932880981593203 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 80  time= 2.3036201000213623  loss= 0.45484672996888453  val_loss= 0.41915977559713224  f1= 0.13760037653441415  f2= 0.1001485488170178  f3= 0.03907417948738623  f4= 0.1780236251300664 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 90  time= 2.59216570854187  loss= 0.43839191707793246  val_loss= 0.45793335773865473  f1= 0.1588150719398035  f2= 0.05557325750050956  f3= 0.03198335223417947  f4= 0.1920202354034399 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 100  time= 2.8810269832611084  loss= 0.2752557402311728  val_loss= 1.0280645539666324  f1= 0.08792975142474767  f2= 0.048120496467947756  f3= 0.0411240896480062  f4= 0.09808140269047118 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 110  time= 3.1690115928649902  loss= 0.2501006199199853  val_loss= 0.3331451459929566  f1= 0.06528225057837933  f2= 0.03766962546919466  f3= 0.010937451987956941  f4= 0.13621129188445433 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 120  time= 3.4619109630584717  loss= 0.1607732763738466  val_loss= 0.7652807756547981  f1= 0.041094059584352464  f2= 0.019604442182488118  f3= 0.04682092716345223  f4= 0.053253847443553826 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 130  time= 3.7570128440856934  loss= 0.26764149378516344  val_loss= 0.5245416315876936  f1= 0.035996753787000235  f2= 0.023480987543495174  f3= 0.10488343970645747  f4= 0.10328031274821055 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 140  time= 4.03174901008606  loss= 0.33594018182027835  val_loss= 0.3013765242820574  f1= 0.052640663745745005  f2= 0.019553641690004963  f3= 0.1327763811497827  f4= 0.13096949523474571 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 150  time= 4.3231635093688965  loss= 0.10989975260548718  val_loss= 0.35570398647048407  f1= 0.028682176403132597  f2= 0.01803950783429809  f3= 0.028748427500667074  f4= 0.03442964086738942 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 160  time= 4.619274139404297  loss= 0.1671343350045225  val_loss= 0.5406626751557639  f1= 0.025578278123451134  f2= 0.014712650006451637  f3= 0.10006565469881606  f4= 0.026777752175803613 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 170  time= 4.916419506072998  loss= 0.1391123302274454  val_loss= 0.4174490136720336  f1= 0.022395450067549224  f2= 0.010789105066606493  f3= 0.08782952107054777  f4= 0.01809825402274193 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 180  time= 5.229340553283691  loss= 0.1909509814684847  val_loss= 0.31786337966222106  f1= 0.02481036435565011  f2= 0.019159047232987492  f3= 0.11397665657350181  f4= 0.03300491330634526 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 190  time= 5.519287586212158  loss= 0.422850990447501  val_loss= 0.4870255079350894  f1= 0.030106614094425336  f2= 0.010359124730345686  f3= 0.35964065354458086  f4= 0.02274459807814909 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 200  time= 5.789665222167969  loss= 0.29697834470135204  val_loss= 0.20581881795121396  f1= 0.0370291224026151  f2= 0.010952360002515179  f3= 0.18424601838046128  f4= 0.06475084391576046 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 210  time= 6.075272083282471  loss= 0.06449575959862154  val_loss= 0.2778342744286422  f1= 0.02415291346612361  f2= 0.008043955699026668  f3= 0.016347041067473755  f4= 0.015951849365997504 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 220  time= 6.33756160736084  loss= 0.12193119190618251  val_loss= 0.22145255098106123  f1= 0.024073170827634976  f2= 0.007793127939327637  f3= 0.07159589474382612  f4= 0.018468998395393788 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 230  time= 6.6106650829315186  loss= 0.09398651357606048  val_loss= 0.13197787743455316  f1= 0.02231269356098295  f2= 0.004094733124318691  f3= 0.017691972741108234  f4= 0.049887114149650615 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 240  time= 6.882150888442993  loss= 0.12103019221612403  val_loss= 0.12045541973782324  f1= 0.03164298302216623  f2= 0.007723576965051416  f3= 0.019787224302564858  f4= 0.06187640792634155 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 250  time= 7.136724948883057  loss= 0.07363971813531486  val_loss= 0.3041553582627034  f1= 0.009887106598097108  f2= 0.008823698184775281  f3= 0.015450180296216148  f4= 0.03947873305622633 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 260  time= 7.402770042419434  loss= 0.0771078953245235  val_loss= 0.050717948549630444  f1= 0.010513308634618338  f2= 0.003915585642022539  f3= 0.05041961174656224  f4= 0.012259389301320372 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 270  time= 7.696369171142578  loss= 0.22673872332530534  val_loss= 0.20311773704975414  f1= 0.009068770534669125  f2= 0.004098421339787249  f3= 0.2042312272406587  f4= 0.009340304210190243 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 280  time= 7.985524892807007  loss= 0.12094750949774918  val_loss= 0.17778746359522907  f1= 0.023991846820266716  f2= 0.007700341088167066  f3= 0.0577673146404071  f4= 0.03148800694890829 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 290  time= 8.281997919082642  loss= 0.0715624069025708  val_loss= 0.18751144117860186  f1= 0.014564131406782763  f2= 0.007125619683255632  f3= 0.010414021701853423  f4= 0.03945863411067898 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 300  time= 8.565642595291138  loss= 0.10725609274309973  val_loss= 0.2092615175945153  f1= 0.019152846423112494  f2= 0.004207805312128809  f3= 0.061823409062526063  f4= 0.022072031945332374 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 310  time= 8.857320070266724  loss= 0.05607630355083273  val_loss= 0.08862279990425737  f1= 0.0036899485277133944  f2= 0.002184667842481987  f3= 0.043623383372294995  f4= 0.006578303808342337 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 320  time= 9.126827001571655  loss= 0.037741430933263764  val_loss= 0.07784043674657472  f1= 0.013010812738947298  f2= 0.0024267029785572792  f3= 0.01579603317332626  f4= 0.006507882042432926 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 330  time= 9.394211292266846  loss= 0.0897879790869061  val_loss= 0.13836107591624894  f1= 0.018050403329091965  f2= 0.003944171734148152  f3= 0.03280318246282036  f4= 0.03499022156084564 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 340  time= 9.68598461151123  loss= 0.12233882012301804  val_loss= 0.08815110898722267  f1= 0.02578932364175018  f2= 0.007020003463916738  f3= 0.0348995621000762  f4= 0.05462993091727492 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 350  time= 9.977346181869507  loss= 0.07083825352449231  val_loss= 0.09421825737365191  f1= 0.00894039686134386  f2= 0.0025587769261199746  f3= 0.046443628498171796  f4= 0.012895451238856675 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 360  time= 10.276628971099854  loss= 0.09334987570164338  val_loss= 0.13838444383191448  f1= 0.01108055041514558  f2= 0.006869543610736371  f3= 0.024662136628075006  f4= 0.050737645047686415 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 370  time= 10.567692518234253  loss= 0.05808451505899873  val_loss= 0.07278465300112805  f1= 0.011733371926821291  f2= 0.0037614232177695926  f3= 0.02182577388167102  f4= 0.02076394603273683 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 380  time= 10.86877989768982  loss= 0.08765881672491188  val_loss= 0.34758267546293675  f1= 0.013673969605006116  f2= 0.002314639087865831  f3= 0.04176708658462767  f4= 0.029903121447412268 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 390  time= 11.165449142456055  loss= 0.03196965157617942  val_loss= 0.05240355569534193  f1= 0.007120641948836132  f2= 0.0018042874833118488  f3= 0.00913380743907026  f4= 0.013910914704961183 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 400  time= 11.463388681411743  loss= 0.06178624048680791  val_loss= 0.06588693729803305  f1= 0.012298103897418727  f2= 0.002258979440016863  f3= 0.005438459691825448  f4= 0.04179069745754688 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 410  time= 11.737924814224243  loss= 0.08051727082338894  val_loss= 0.06628421663137214  f1= 0.008859414052559378  f2= 0.003244841005975132  f3= 0.03815507455204101  f4= 0.03025794121281342 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 420  time= 12.007276773452759  loss= 0.06446353315852858  val_loss= 0.0716027101483725  f1= 0.018660233045780496  f2= 0.0018041865263675766  f3= 0.008504459070567393  f4= 0.03549465451581312 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 430  time= 12.275746583938599  loss= 0.05318636827621106  val_loss= 0.1386974523321187  f1= 0.014531466575553605  f2= 0.0027539622324442453  f3= 0.027888508183171443  f4= 0.00801243128504177 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 440  time= 12.540356636047363  loss= 0.09554718642350535  val_loss= 0.06305797929832228  f1= 0.014102454801114122  f2= 0.003271098135973446  f3= 0.048521372459703456  f4= 0.029652261026714328 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 450  time= 12.833552122116089  loss= 0.028259727233280338  val_loss= 0.0647674233636743  f1= 0.007792371996574634  f2= 0.0021166087831126583  f3= 0.010216590965891349  f4= 0.008134155487701697 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 460  time= 13.126868963241577  loss= 0.03743417457542129  val_loss= 0.09503584066765317  f1= 0.005541645681905585  f2= 0.0011086041524033992  f3= 0.023360828826117952  f4= 0.007423095914994353 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 470  time= 13.424679279327393  loss= 0.024192478725540437  val_loss= 0.07088002855683175  f1= 0.00757664559927904  f2= 0.001251226276634813  f3= 0.004644356945740484  f4= 0.010720249903886097 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 480  time= 13.712756395339966  loss= 0.022003767810811037  val_loss= 0.02794170675355938  f1= 0.005198319909292693  f2= 0.0012402825458367054  f3= 0.0048319592055565425  f4= 0.0107332061501251 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 490  time= 14.012186050415039  loss= 0.08356086179255627  val_loss= 0.09703660416304288  f1= 0.00512298217234105  f2= 0.0031503986655803794  f3= 0.02910695048235659  f4= 0.04618053047227827 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 500  time= 14.281224966049194  loss= 0.015383721663977676  val_loss= 0.07187211996807812  f1= 0.005469100340361538  f2= 0.0015383390394433274  f3= 0.0007130451754291147  f4= 0.007663237108743695 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 510  time= 14.577982425689697  loss= 0.03295397182982727  val_loss= 0.06735705797816646  f1= 0.004717749021549611  f2= 0.0012797789765685763  f3= 0.007373740810574837  f4= 0.019582703021134244 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 520  time= 14.86241340637207  loss= 0.03651337198306428  val_loss= 0.07792777292850842  f1= 0.0135254336777793  f2= 0.002079040792055053  f3= 0.0038822714392324903  f4= 0.017026626073997433 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 530  time= 15.142457962036133  loss= 0.11806166671536583  val_loss= 0.05686196548804233  f1= 0.005983883118924983  f2= 0.0014907165553604863  f3= 0.08677035397255557  f4= 0.023816713068524786 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 540  time= 15.4063560962677  loss= 0.08310064048463733  val_loss= 0.03385951884768544  f1= 0.01391662746280676  f2= 0.0026953786498774314  f3= 0.009009087769622268  f4= 0.05747954660233087 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 550  time= 15.6781907081604  loss= 0.05405499118476381  val_loss= 0.14461604214782112  f1= 0.0067562418393996545  f2= 0.0037581398032747134  f3= 0.022367384920465947  f4= 0.021173224621623493 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 560  time= 15.957381010055542  loss= 0.021857393287078985  val_loss= 0.042270676675419436  f1= 0.003709659561157116  f2= 0.001391580691326711  f3= 0.0013541801340844317  f4= 0.015401972900510721 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 570  time= 16.236181020736694  loss= 0.028551718992866622  val_loss= 0.030703607422109546  f1= 0.005265204617295869  f2= 0.0016230705409387644  f3= 0.009277948126270278  f4= 0.012385495708361708 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 580  time= 16.498032569885254  loss= 0.040784014185964314  val_loss= 0.04541497067760899  f1= 0.008018456352303787  f2= 0.0016893344855472047  f3= 0.014439626200651921  f4= 0.016636597147461393 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 590  time= 16.774616718292236  loss= 0.014371477637676317  val_loss= 0.05538041062034862  f1= 0.0031819346024331923  f2= 0.0010483005788945643  f3= 0.004560023108118329  f4= 0.005581219348230235 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 600  time= 17.044092893600464  loss= 0.019597683764557384  val_loss= 0.043270447865989506  f1= 0.005663420450915461  f2= 0.001377489315414544  f3= 0.003509635063816519  f4= 0.00904713893441086 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 610  time= 17.323964595794678  loss= 0.03168393478015824  val_loss= 0.04180705099235595  f1= 0.003890768469932091  f2= 0.002208881110388139  f3= 0.014218996721287312  f4= 0.011365288478550693 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 620  time= 17.609387397766113  loss= 0.01764920259157123  val_loss= 0.043082949292887196  f1= 0.005135859457370916  f2= 0.0009868771878188754  f3= 0.004807064900478569  f4= 0.0067194010459028725 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 630  time= 17.87393283843994  loss= 0.039340958787211604  val_loss= 0.12652721304820053  f1= 0.012754557480117648  f2= 0.002003782142135788  f3= 0.006958915191581795  f4= 0.017623703973376374 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 640  time= 18.146562814712524  loss= 0.022278168229126486  val_loss= 0.024386251743660457  f1= 0.003268637928082406  f2= 0.002007339414422787  f3= 0.0008828917268510541  f4= 0.01611929915977024 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 650  time= 18.42178201675415  loss= 0.02061114595937975  val_loss= 0.0613451263474383  f1= 0.005954027275266613  f2= 0.0013435100008212118  f3= 0.0053408481510364625  f4= 0.007972760532255463 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 660  time= 18.702762365341187  loss= 0.017560086693129192  val_loss= 0.033437534264063776  f1= 0.0035041410503896062  f2= 0.0009165261631643712  f3= 0.009394510618522579  f4= 0.0037449088610526315 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 670  time= 18.99035096168518  loss= 0.02629135896721055  val_loss= 0.03674972294198442  f1= 0.00992656854650522  f2= 0.0010936214061535463  f3= 0.0014608534750641894  f4= 0.013810315539487592 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 680  time= 19.275851011276245  loss= 0.023781042950217757  val_loss= 0.05269563480935327  f1= 0.003999972403505304  f2= 0.0013085177785234702  f3= 0.012972206061324362  f4= 0.005500346706864621 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 690  time= 19.555562496185303  loss= 0.026186921054413864  val_loss= 0.025331745869961057  f1= 0.0031810904210747885  f2= 0.0017420882337259478  f3= 0.005772605630305944  f4= 0.015491136769307183 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 700  time= 19.828237056732178  loss= 0.03176765369415113  val_loss= 0.04822207475352655  f1= 0.004939742190989923  f2= 0.002038979719111455  f3= 0.015343236078133413  f4= 0.009445695705916347 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 710  time= 20.12444519996643  loss= 0.018757543502257622  val_loss= 0.04079786674238538  f1= 0.004362588978456817  f2= 0.001493978352091551  f3= 0.0006733244775517207  f4= 0.012227651694157534 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 720  time= 20.40354084968567  loss= 0.04738874007512622  val_loss= 0.05008441677185743  f1= 0.008454784791900613  f2= 0.0024113510438635213  f3= 0.02027955672608669  f4= 0.0162430475132754 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 730  time= 20.69165349006653  loss= 0.022743660009839895  val_loss= 0.08427340619229702  f1= 0.0068693866587121325  f2= 0.0021973139791909486  f3= 0.0047282630010024125  f4= 0.0089486963709344 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 740  time= 20.962419271469116  loss= 0.009891976143960377  val_loss= 0.039646846510048614  f1= 0.0030944196196202567  f2= 0.0010999039863496128  f3= 0.0003553436043639445  f4= 0.005342308933626563 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 750  time= 21.25135040283203  loss= 0.0238996923570647  val_loss= 0.030802367250296878  f1= 0.006861628947979902  f2= 0.0010186620616617957  f3= 0.011318486020283685  f4= 0.004700915327139318 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 760  time= 21.5288507938385  loss= 0.018556237481488654  val_loss= 0.041234659288668374  f1= 0.00205203103516055  f2= 0.0010085153775604076  f3= 0.0035425386999700472  f4= 0.011953152368797649 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 770  time= 21.796785593032837  loss= 0.015308197315422413  val_loss= 0.04005817134429726  f1= 0.0049692123299587086  f2= 0.0009051316443365004  f3= 0.003189582657529486  f4= 0.006244270683597719 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 780  time= 22.08322548866272  loss= 0.033713898125738044  val_loss= 0.05722280166983578  f1= 0.014140024748299028  f2= 0.0010661016786769882  f3= 0.004740517371453151  f4= 0.013767254327308881 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 790  time= 22.362003087997437  loss= 0.06826485667232415  val_loss= 0.06477118219377317  f1= 0.005311104213793879  f2= 0.0056978174855351685  f3= 0.003987895089639292  f4= 0.053268039883355815 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 800  time= 22.631946086883545  loss= 0.038113968776343106  val_loss= 0.037496518462884695  f1= 0.008802344167500007  f2= 0.0026640961683682893  f3= 0.0012479011645097355  f4= 0.025399627275965068 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 810  time= 22.91373348236084  loss= 0.030383412014798522  val_loss= 0.035604226937661944  f1= 0.006881620223233528  f2= 0.002075929809974031  f3= 0.008267337004727685  f4= 0.013158524976863276 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 820  time= 23.262065649032593  loss= 0.012532934243182643  val_loss= 0.03748106220316407  f1= 0.002742271211868992  f2= 0.0010085255444733786  f3= 0.001000277983187593  f4= 0.00778185950365268 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 830  time= 23.583086013793945  loss= 0.021566848888801827  val_loss= 0.06261745825216217  f1= 0.005724264436708398  f2= 0.0016394266712023875  f3= 0.0007388269580245227  f4= 0.013464330822866521 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 840  time= 23.88824725151062  loss= 0.044683293819340025  val_loss= 0.03602792669666868  f1= 0.005024133022249792  f2= 0.002549890755275514  f3= 0.002932188531569095  f4= 0.03417708151024562 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 850  time= 24.199398040771484  loss= 0.04388268110243739  val_loss= 0.08649060231466266  f1= 0.02173447596960794  f2= 0.0015731900230714173  f3= 0.002986936985104023  f4= 0.01758807812465401 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 860  time= 24.494500398635864  loss= 0.02009742364800153  val_loss= 0.02451191387653219  f1= 0.0030781274691865017  f2= 0.0008322551821597805  f3= 0.006758309479591292  f4= 0.009428731517063955 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 870  time= 24.78977108001709  loss= 0.008297682957962666  val_loss= 0.033740667053021076  f1= 0.002175115181784539  f2= 0.00079295986825719  f3= 0.0003068180744874488  f4= 0.0050227898334334895 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 880  time= 25.06763458251953  loss= 0.013131037189935295  val_loss= 0.03612849482075957  f1= 0.004220304788062597  f2= 0.0008947800633625479  f3= 0.001791826289369229  f4= 0.006224126049140921 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 890  time= 25.395129442214966  loss= 0.02477773747466445  val_loss= 0.031231003893186927  f1= 0.007558417533418346  f2= 0.0009102778604647073  f3= 0.0038792903561973117  f4= 0.012429751724584085 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 900  time= 25.660533666610718  loss= 0.04150932549480225  val_loss= 0.03354359928782739  f1= 0.00679103516530902  f2= 0.001374065981134759  f3= 0.004667408587990953  f4= 0.02867681576036751 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 910  time= 25.93197202682495  loss= 0.01663819088275134  val_loss= 0.05219642201886854  f1= 0.00260312355565504  f2= 0.0014149693704952207  f3= 0.00947969677719033  f4= 0.0031404011794107516 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 920  time= 26.21593689918518  loss= 0.03419033829041913  val_loss= 0.0348901488279237  f1= 0.01300661609570604  f2= 0.0012604529157588349  f3= 0.0039210441074402054  f4= 0.01600222517151404 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 930  time= 26.48084783554077  loss= 0.046533869862374316  val_loss= 0.06663188868440496  f1= 0.0039548730471753125  f2= 0.0019739854156057665  f3= 0.005647656066975139  f4= 0.03495735533261809 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 940  time= 26.764533519744873  loss= 0.02930985158385389  val_loss= 0.0699239163343567  f1= 0.005661399510279011  f2= 0.0011048592898349444  f3= 0.01326675776797165  f4= 0.009276835015768285 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 950  time= 27.10213875770569  loss= 0.012226466169571607  val_loss= 0.03660923591968899  f1= 0.0046722267787185565  f2= 0.0011627301906073104  f3= 0.002376173694164609  f4= 0.004015335506081131 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 960  time= 27.402098178863525  loss= 0.039009163045380595  val_loss= 0.1272246450291694  f1= 0.007178101526410986  f2= 0.0028527235828247864  f3= 0.008520870929045762  f4= 0.020457467007099064 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 970  time= 27.69813585281372  loss= 0.02165342287720698  val_loss= 0.025911760106494486  f1= 0.00560515124146347  f2= 0.0013612988296451307  f3= 0.0006349454320959375  f4= 0.01405202737400244 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 980  time= 27.96576976776123  loss= 0.033188238199621536  val_loss= 0.058423497435749906  f1= 0.010464000277099659  f2= 0.0013852441089926247  f3= 0.0074598905528312845  f4= 0.013879103260697965 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 990  time= 28.272817373275757  loss= 0.031476072297889984  val_loss= 0.0570360601428522  f1= 0.003447073906698538  f2= 0.0011987091073523079  f3= 0.016324235222139744  f4= 0.010506054061699391 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1000  time= 28.569038152694702  loss= 0.030081014858353367  val_loss= 0.05302499254916225  f1= 0.009808722646291871  f2= 0.0021020297707443234  f3= 0.00046652830336023143  f4= 0.017703734137956943 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1010  time= 28.852977514266968  loss= 0.06213867895395603  val_loss= 0.04449893560508254  f1= 0.0072931736469905376  f2= 0.0018665580719996137  f3= 0.006533587655409669  f4= 0.04644535957955622 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1020  time= 29.13247561454773  loss= 0.025330287787480272  val_loss= 0.023454983729694415  f1= 0.004588786149509646  f2= 0.0012574518492347146  f3= 0.0018124436657191618  f4= 0.017671606123016746 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1030  time= 29.41100788116455  loss= 0.01495451648362671  val_loss= 0.035945795834608135  f1= 0.004385160039940708  f2= 0.0011354106593061306  f3= 0.0003244995498960462  f4= 0.009109446234483827 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1040  time= 29.677618265151978  loss= 0.009128234909432535  val_loss= 0.03609609443729491  f1= 0.004383743932928997  f2= 0.0007351126790597018  f3= 0.0010307352511752068  f4= 0.0029786430462686304 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1050  time= 29.95193862915039  loss= 0.027900032269867942  val_loss= 0.054147754602155596  f1= 0.006028633829656249  f2= 0.0010904373208658763  f3= 0.008658554665779559  f4= 0.012122406453566255 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1060  time= 30.262439250946045  loss= 0.01921693096659011  val_loss= 0.04144346015764205  f1= 0.004672578564747258  f2= 0.0010238531017544965  f3= 0.006070413152398399  f4= 0.007450086147689959 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1070  time= 30.552454471588135  loss= 0.03693522722173601  val_loss= 0.05653506845139819  f1= 0.009982257953838558  f2= 0.0020985727931922497  f3= 0.0009505576818935993  f4= 0.0239038387928116 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1080  time= 30.848877906799316  loss= 0.015181770209956433  val_loss= 0.029286324384501468  f1= 0.004355823603775764  f2= 0.0007348236121385701  f3= 0.0024843432052096734  f4= 0.007606779788832427 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1090  time= 31.14013433456421  loss= 0.04422063819222641  val_loss= 0.07149478085249872  f1= 0.007623584861816365  f2= 0.0023097084378045853  f3= 0.0026816486044702087  f4= 0.03160569628813525 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1100  time= 31.414204597473145  loss= 0.006068594193998693  val_loss= 0.023693431522387155  f1= 0.0023473246621349684  f2= 0.0007244068153922858  f3= 0.0004064309802258987  f4= 0.0025904317362455405 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1110  time= 31.68574047088623  loss= 0.018010652617363173  val_loss= 0.024041752053028626  f1= 0.004487959611854058  f2= 0.0009958552478240867  f3= 0.007030458061072841  f4= 0.005496379696612187 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1120  time= 31.951003074645996  loss= 0.007391642432458468  val_loss= 0.019938542289973098  f1= 0.0018898811452944557  f2= 0.0005069199088765499  f3= 0.000907576721505922  f4= 0.00408726465678154 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1130  time= 32.22067165374756  loss= 0.0346653292030125  val_loss= 0.11868945247630183  f1= 0.005153057173255984  f2= 0.0017093293240532713  f3= 0.022489950118752697  f4= 0.005312992586950543 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1140  time= 32.50664567947388  loss= 0.01276825033859203  val_loss= 0.03443810351424918  f1= 0.004598607754180861  f2= 0.0014610613132130133  f3= 0.0016735372315362154  f4= 0.0050350440396619385 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1150  time= 32.77073287963867  loss= 0.019116738124337795  val_loss= 0.04329710781435875  f1= 0.006853302519285537  f2= 0.0007403560367321623  f3= 0.004056192224239775  f4= 0.007466887344080322 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1160  time= 33.044068813323975  loss= 0.005810797569256226  val_loss= 0.02521590915541517  f1= 0.0014602912809507115  f2= 0.0009163732770627444  f3= 0.0009181103711216415  f4= 0.002516022640121129 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1170  time= 33.35106945037842  loss= 0.016247919229829797  val_loss= 0.03894062484231863  f1= 0.004174321118195747  f2= 0.0009817032682653826  f3= 0.005514656064895734  f4= 0.005577238778472934 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1180  time= 33.63994312286377  loss= 0.044363056781886144  val_loss= 0.06774176137907073  f1= 0.014644587833618808  f2= 0.0012360525979359367  f3= 0.011710079287436462  f4= 0.016772337062894927 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1190  time= 33.92978048324585  loss= 0.013041137994145262  val_loss= 0.03273188398322428  f1= 0.002990483476119898  f2= 0.0024207086527926777  f3= 0.0017206120896914744  f4= 0.005909333775541213 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1200  time= 34.21135449409485  loss= 0.03906776066687318  val_loss= 0.041419306324914654  f1= 0.010485010225353133  f2= 0.0017206611817264638  f3= 0.00470734612149198  f4= 0.022154743138301607 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1210  time= 34.49044919013977  loss= 0.013791674291556128  val_loss= 0.04656002396433048  f1= 0.005694390695289989  f2= 0.001120741411087258  f3= 0.001090812102101985  f4= 0.005885730083076894 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1220  time= 34.75470423698425  loss= 0.01709959511467249  val_loss= 0.029966112483490088  f1= 0.007271370847641528  f2= 0.0014005866345343499  f3= 0.0016789038716306449  f4= 0.006748733760865966 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1230  time= 35.013721227645874  loss= 0.02104878285531103  val_loss= 0.036047359030457235  f1= 0.005256937666542662  f2= 0.0009164802675687742  f3= 0.0022877566403204505  f4= 0.012587608280879141 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1240  time= 35.28934454917908  loss= 0.032409335885163155  val_loss= 0.0528485137231324  f1= 0.010245590752050344  f2= 0.0015134317666461673  f3= 0.006333315993166297  f4= 0.014316997373300347 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1250  time= 35.58487820625305  loss= 0.02428345088207796  val_loss= 0.053295423793953994  f1= 0.007407833344547864  f2= 0.0026971989070955008  f3= 0.004802025529230614  f4= 0.009376393101203983 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1260  time= 35.873276710510254  loss= 0.015367048098431963  val_loss= 0.04132529528849707  f1= 0.0026648490501761796  f2= 0.0012136024505246855  f3= 0.0022529086034755487  f4= 0.009235687994255547 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1270  time= 36.165695905685425  loss= 0.028816202929470066  val_loss= 0.02937335885427171  f1= 0.0070622809168405945  f2= 0.0015695655856964965  f3= 0.007330975867811627  f4= 0.012853380559121344 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1280  time= 36.48557186126709  loss= 0.0197140353186842  val_loss= 0.03285535065454902  f1= 0.005442657003016388  f2= 0.0012165270439789904  f3= 0.0035177718058793904  f4= 0.009537079465809431 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1290  time= 36.77814602851868  loss= 0.027265169999520736  val_loss= 0.04302211192186245  f1= 0.003960544348989225  f2= 0.0008789545101423828  f3= 0.017402087180382605  f4= 0.005023583960006523 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1300  time= 37.08822679519653  loss= 0.02588342767301655  val_loss= 0.0437295385406135  f1= 0.003374508222341121  f2= 0.0020547897019943424  f3= 0.0015622131502996378  f4= 0.018891916598381454 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1310  time= 37.398695945739746  loss= 0.01879215050395306  val_loss= 0.03674402456212938  f1= 0.004634403007086302  f2= 0.0007008645934686716  f3= 0.006471432949869678  f4= 0.006985449953528409 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1320  time= 37.72074580192566  loss= 0.018289499196542543  val_loss= 0.03603943118605132  f1= 0.00335270019872341  f2= 0.0007044138285593268  f3= 0.0027280483520886138  f4= 0.011504336817171194 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1330  time= 38.026854276657104  loss= 0.037590690932860764  val_loss= 0.04883858287509477  f1= 0.0066644726187141635  f2= 0.0009641837668985625  f3= 0.004010946268563308  f4= 0.02595108827868473 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1340  time= 38.30586338043213  loss= 0.00928740748079016  val_loss= 0.029072835336190345  f1= 0.0025899341707673555  f2= 0.0006883874134116615  f3= 0.0015005896348435535  f4= 0.004508496261767588 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1350  time= 38.594486474990845  loss= 0.009791183310589069  val_loss= 0.04635897088796438  f1= 0.00299042756806898  f2= 0.0006169408439369197  f3= 0.0023477537985694143  f4= 0.003836061100013754 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1360  time= 38.86696720123291  loss= 0.00871573557244562  val_loss= 0.024877417259272245  f1= 0.0020246785918569547  f2= 0.000895375181053151  f3= 0.0037626464527255026  f4= 0.0020330353468100124 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1370  time= 39.13976764678955  loss= 0.028835496819016967  val_loss= 0.04742878364593988  f1= 0.012306848283856371  f2= 0.0014395464333136975  f3= 0.006085843028552788  f4= 0.009003259073294112 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1380  time= 39.434422731399536  loss= 0.01352809755772537  val_loss= 0.021191162730561236  f1= 0.0031330935126236865  f2= 0.000990496328036256  f3= 0.005241202587818694  f4= 0.004163305129246733 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1390  time= 39.747323989868164  loss= 0.01013030856731222  val_loss= 0.023685682710480715  f1= 0.00504268350877191  f2= 0.0010173519004202651  f3= 0.00037018869095499637  f4= 0.003700084467165049 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1400  time= 40.0434775352478  loss= 0.02609314789383665  val_loss= 0.03395804866641025  f1= 0.0027089710885992435  f2= 0.0027337693691604683  f3= 0.0030063769499962764  f4= 0.017644030486080663 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1410  time= 40.33399486541748  loss= 0.006724854872593145  val_loss= 0.022383351874749144  f1= 0.0018146322287362169  f2= 0.0007247325213520804  f3= 0.001370081913373555  f4= 0.0028154082091312936 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1420  time= 40.60888385772705  loss= 0.011251166494036194  val_loss= 0.025051277637424325  f1= 0.0029243738754898313  f2= 0.0011338018890400129  f3= 0.002256266894834306  f4= 0.004936723834672041 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1430  time= 40.96297574043274  loss= 0.03292890854376936  val_loss= 0.020768923225162494  f1= 0.005908363007913943  f2= 0.0021874559513316402  f3= 0.0024704482007633587  f4= 0.022362641383760425 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1440  time= 41.23985815048218  loss= 0.026555198047674654  val_loss= 0.02571725029542589  f1= 0.006548651594128318  f2= 0.0015678104756188026  f3= 0.0006364061910565095  f4= 0.017802329786871022 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1450  time= 41.56020641326904  loss= 0.01254406728647367  val_loss= 0.02767994675716215  f1= 0.0025932551890993356  f2= 0.0009857745153134287  f3= 0.002505445982308216  f4= 0.006459591599752688 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1460  time= 41.86836838722229  loss= 0.04102461917007009  val_loss= 0.022572522240820264  f1= 0.01208717532460664  f2= 0.0017134123794177857  f3= 0.0014462925434643836  f4= 0.025777738922581284 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1470  time= 42.1688814163208  loss= 0.012316273554406355  val_loss= 0.034776479915117134  f1= 0.0035129292399864315  f2= 0.0004973244121306181  f3= 0.0008616326493588126  f4= 0.007444387252930492 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1480  time= 42.45657801628113  loss= 0.014185594729951513  val_loss= 0.03993875019837921  f1= 0.006880639865374236  f2= 0.0007590484927261446  f3= 0.0005330452932481717  f4= 0.006012861078602963 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1490  time= 42.762845039367676  loss= 0.01888055090515013  val_loss= 0.04413519829352322  f1= 0.006798715626723424  f2= 0.0007090082897980767  f3= 0.0031280476725524062  f4= 0.008244779316076227 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1500  time= 43.06904101371765  loss= 0.009596296083390876  val_loss= 0.035658393093851964  f1= 0.0013115786190310104  f2= 0.0012828285771211644  f3= 0.001977071909697667  f4= 0.005024816977541036 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1510  time= 43.38093876838684  loss= 0.014266732189143427  val_loss= 0.020519019040437656  f1= 0.0038286434151882805  f2= 0.001113420512480287  f3= 0.001106084368144701  f4= 0.00821858389333016 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1520  time= 43.7303261756897  loss= 0.01612451626711221  val_loss= 0.029049184791324408  f1= 0.004290828583288712  f2= 0.0011293062084911777  f3= 0.0005711793021412446  f4= 0.010133202173191075 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1530  time= 44.02055835723877  loss= 0.014696809107744032  val_loss= 0.020081329769068502  f1= 0.0025228473348483317  f2= 0.0008478544400416136  f3= 0.0023543354843055834  f4= 0.0089717718485485 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1540  time= 44.29235911369324  loss= 0.027929931701542537  val_loss= 0.07310399824090852  f1= 0.008867855292813672  f2= 0.0017157414567484791  f3= 0.00927197832513882  f4= 0.00807435662684157 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1550  time= 44.58775234222412  loss= 0.012838003543100574  val_loss= 0.030407040558801644  f1= 0.002107036015982305  f2= 0.0011049631544130776  f3= 0.0022965338022286736  f4= 0.007329470570476515 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1560  time= 44.88851547241211  loss= 0.02305641602370782  val_loss= 0.023425207667244038  f1= 0.0043429707994667634  f2= 0.0007995394509488714  f3= 0.0016167071090818792  f4= 0.016297198664210307 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1570  time= 45.19860863685608  loss= 0.015347405533732301  val_loss= 0.034233782816442494  f1= 0.004474834163669454  f2= 0.0008305445986543017  f3= 0.001410197015505904  f4= 0.008631829755902643 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1580  time= 45.49438261985779  loss= 0.018037599493474655  val_loss= 0.01796459139309052  f1= 0.005500953414558417  f2= 0.0009289134834757664  f3= 0.0007097905922133536  f4= 0.010897942003227115 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1590  time= 45.80165934562683  loss= 0.04372354330780507  val_loss= 0.08564310010203181  f1= 0.006886243206360404  f2= 0.0027497814915368513  f3= 0.0010281804475644333  f4= 0.03305933816234338 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1600  time= 46.09908843040466  loss= 0.03236025877630257  val_loss= 0.04239161332657264  f1= 0.0029763511818288453  f2= 0.0018043467976497681  f3= 0.012237954880333102  f4= 0.01534160591649085 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1610  time= 46.41085076332092  loss= 0.010347746056988327  val_loss= 0.032178516323080275  f1= 0.0029890302051503678  f2= 0.0006657253872406088  f3= 0.0017219668539063349  f4= 0.004971023610691014 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1620  time= 46.70120167732239  loss= 0.011293964736201409  val_loss= 0.021926711408497616  f1= 0.0027253090068456334  f2= 0.0007565841128347526  f3= 0.0008175645612778271  f4= 0.006994507055243193 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1630  time= 46.97412300109863  loss= 0.026862485016403342  val_loss= 0.0448101365990582  f1= 0.004456771685277795  f2= 0.0013285377556570312  f3= 0.0036536511292898054  f4= 0.01742352444617871 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1640  time= 47.238651514053345  loss= 0.021404366175702242  val_loss= 0.045069298162562735  f1= 0.006674001389269286  f2= 0.0011984247945648946  f3= 0.0022867576951520817  f4= 0.01124518229671598 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1650  time= 47.51785612106323  loss= 0.03204234901242666  val_loss= 0.09394469764125349  f1= 0.009778721219830573  f2= 0.0018687628436223115  f3= 0.014187523599866752  f4= 0.006207341349107029 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1660  time= 47.79996061325073  loss= 0.01562924234638076  val_loss= 0.03660921777288438  f1= 0.0064580505941218185  f2= 0.0008683204220264174  f3= 0.0007951336193985317  f4= 0.007507737710833989 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1670  time= 48.0684118270874  loss= 0.02194934419378691  val_loss= 0.03377493062540563  f1= 0.005396903712405476  f2= 0.0015057217126150397  f3= 0.00743241371732892  f4= 0.007614305051437473 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1680  time= 48.34447646141052  loss= 0.015045110573147098  val_loss= 0.027172382486346604  f1= 0.005250881286066053  f2= 0.0007717528298139767  f3= 0.00242039960404892  f4= 0.006602076853218148 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1690  time= 48.641762018203735  loss= 0.018200589622905324  val_loss= 0.021251758189208936  f1= 0.003916616116327511  f2= 0.0012249224338622543  f3= 0.0025750006600545865  f4= 0.01048405041266097 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1700  time= 48.95663833618164  loss= 0.011118755653911726  val_loss= 0.02641821461261226  f1= 0.005320224829051568  f2= 0.0009288839683145204  f3= 0.000400022813417957  f4= 0.0044696240431276795 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1710  time= 49.281938791275024  loss= 0.022676206242563876  val_loss= 0.04790699430437592  f1= 0.00527785164539643  f2= 0.000818731843943223  f3= 0.009281149827875626  f4= 0.007298472925348597 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1720  time= 49.58468294143677  loss= 0.011908193344325399  val_loss= 0.02683030709893738  f1= 0.0032258138102844616  f2= 0.0006630594733992428  f3= 0.0030354406696927344  f4= 0.004983879390948958 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1730  time= 49.86451554298401  loss= 0.017079714645730326  val_loss= 0.026134609548802416  f1= 0.0027018830582138313  f2= 0.0015175320575748419  f3= 0.0009132125288799656  f4= 0.011947087001061688 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1740  time= 50.13020181655884  loss= 0.025392848562916487  val_loss= 0.03058098509549753  f1= 0.003296414640983491  f2= 0.0016027909560576896  f3= 0.0033760400138430125  f4= 0.017117602952032292 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1750  time= 50.40607285499573  loss= 0.01910466697829583  val_loss= 0.03759617735442747  f1= 0.0041874742620952666  f2= 0.000812746323960516  f3= 0.008404620012496864  f4= 0.005699826379743182 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1760  time= 50.67057776451111  loss= 0.013207410086513258  val_loss= 0.021482695314321  f1= 0.00425848397500152  f2= 0.0007444362171012765  f3= 0.0016266839339698746  f4= 0.006577805960440586 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1770  time= 50.946049213409424  loss= 0.016662128645624576  val_loss= 0.019424103028995304  f1= 0.0021223218798974825  f2= 0.00108719198469919  f3= 0.003960202294007487  f4= 0.009492412487020417 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1780  time= 51.22190284729004  loss= 0.017129650494774166  val_loss= 0.041948027789905515  f1= 0.004043149576949442  f2= 0.0014203389151751034  f3= 0.0022358366315364535  f4= 0.00943032537111317 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1790  time= 51.493287563323975  loss= 0.005265183547215657  val_loss= 0.018219550813339036  f1= 0.0009669459716073573  f2= 0.000680922587972511  f3= 0.0005782654683225533  f4= 0.0030390495193132337 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1800  time= 51.79699182510376  loss= 0.021873307042395262  val_loss= 0.059053218882064065  f1= 0.003927363904548048  f2= 0.0006675467932711173  f3= 0.0055348928472653165  f4= 0.011743503497310783 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1810  time= 52.09995913505554  loss= 0.0201036559601403  val_loss= 0.029672194227852673  f1= 0.0025240859180232374  f2= 0.0019005097096616384  f3= 0.000818296123796713  f4= 0.014860764208658709 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1820  time= 52.40582084655762  loss= 0.02728713555891975  val_loss= 0.05688372390853816  f1= 0.00752930644873551  f2= 0.0020006954393082  f3= 0.006628340239312373  f4= 0.01112879343156367 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1830  time= 52.71294093132019  loss= 0.024393881542868548  val_loss= 0.03238314197318232  f1= 0.006963711925767335  f2= 0.001312647285954518  f3= 0.002402324652596107  f4= 0.01371519767855058 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1840  time= 53.01841449737549  loss= 0.021584146415601982  val_loss= 0.05906365293543321  f1= 0.006709424924580502  f2= 0.0012866897552970297  f3= 0.002879981361823635  f4= 0.010708050373900817 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1850  time= 53.320985555648804  loss= 0.017976344857197155  val_loss= 0.02527796206716009  f1= 0.007062215370041678  f2= 0.001022627599605715  f3= 0.004573273088458135  f4= 0.005318228799091627 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1860  time= 53.631523847579956  loss= 0.04853688550522062  val_loss= 0.046151038534869335  f1= 0.007678147210124678  f2= 0.0013421636748726098  f3= 0.030783692400609488  f4= 0.008732882219613847 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1870  time= 53.926486015319824  loss= 0.02668616776633222  val_loss= 0.031232460044504518  f1= 0.013408387840411729  f2= 0.0011748450037900302  f3= 0.004203827103977951  f4= 0.007899107818152512 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1880  time= 54.22014021873474  loss= 0.009363813496272582  val_loss= 0.016112666209376257  f1= 0.0016355337454327264  f2= 0.0006078012618654128  f3= 0.0009968349575620987  f4= 0.006123643531412346 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1890  time= 54.50620675086975  loss= 0.025916689524515858  val_loss= 0.060476288919516646  f1= 0.005279826592198688  f2= 0.002091407548608356  f3= 0.001640851553070418  f4= 0.0169046038306384 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1900  time= 54.82764506340027  loss= 0.01378279640676608  val_loss= 0.023134096009485815  f1= 0.00341925679589414  f2= 0.0007679058004785001  f3= 0.003298646006135409  f4= 0.006296987804258033 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1910  time= 55.14614510536194  loss= 0.010027380882761594  val_loss= 0.0365166326430676  f1= 0.0038527455756554346  f2= 0.000814528656102026  f3= 0.002968882556271681  f4= 0.002391224094732452 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1920  time= 55.44674849510193  loss= 0.013422672965679455  val_loss= 0.02191464526707275  f1= 0.00516181170674719  f2= 0.0009750947309294968  f3= 0.00033754543490672486  f4= 0.006948221093096043 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1930  time= 55.74790716171265  loss= 0.03425259928754521  val_loss= 0.03511213558494032  f1= 0.003630797454425685  f2= 0.001963135148275443  f3= 0.006893493304880501  f4= 0.021765173379963587 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1940  time= 56.062257289886475  loss= 0.0172011957731561  val_loss= 0.03606433490652494  f1= 0.005842520657289209  f2= 0.0006084421181595733  f3= 0.0025517695004259114  f4= 0.008198463497281412 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1950  time= 56.40109395980835  loss= 0.01135722764011885  val_loss= 0.01948579176877175  f1= 0.0029364419720844443  f2= 0.0005508044786103691  f3= 0.0010387513309590899  f4= 0.006831229858464945 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1960  time= 56.69766545295715  loss= 0.10267364301729344  val_loss= 0.042622268809267644  f1= 0.004906450985622712  f2= 0.001183034674443225  f3= 0.0013237500848563807  f4= 0.09526040727237113 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1970  time= 57.039454221725464  loss= 0.03245211985119641  val_loss= 0.041689397314458775  f1= 0.005609554963763623  f2= 0.0011795448238633472  f3= 0.0029850679267047  f4= 0.022677952136864746 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1980  time= 57.351266860961914  loss= 0.03181703688152846  val_loss= 0.02799667213856245  f1= 0.004169853612286434  f2= 0.0024129481851292004  f3= 0.0023119696362671254  f4= 0.022922265447845702 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1990  time= 57.685752868652344  loss= 0.016163304713817276  val_loss= 0.01419524299439641  f1= 0.00322270436301106  f2= 0.0009448192221191469  f3= 0.00384186986633517  f4= 0.008153911262351899 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2000  time= 57.99268293380737  loss= 0.036304459553991635  val_loss= 0.07995528547882401  f1= 0.008023033930971653  f2= 0.001973045873536982  f3= 0.0054348257127918255  f4= 0.02087355403669117 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2010  time= 58.29405975341797  loss= 0.01691415639349107  val_loss= 0.02434831432512003  f1= 0.003587060284490351  f2= 0.0012966792554879412  f3= 0.0019293137739048703  f4= 0.01010110307960791 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2020  time= 58.59071350097656  loss= 0.010917976860440179  val_loss= 0.024771336593708997  f1= 0.0024587341030576824  f2= 0.0008294652877484308  f3= 0.002017180882293033  f4= 0.005612596587341032 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2030  time= 58.89512634277344  loss= 0.013365323287126854  val_loss= 0.024802727702605457  f1= 0.0030111663353506036  f2= 0.0009679286810856368  f3= 0.002987551647225304  f4= 0.006398676623465309 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2040  time= 59.21868014335632  loss= 0.01728480797086379  val_loss= 0.04517691343100538  f1= 0.0036180117545551254  f2= 0.0015222542293344937  f3= 0.002458167688216296  f4= 0.009686374298757879 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2050  time= 59.54033041000366  loss= 0.028923611824782387  val_loss= 0.04401186051035204  f1= 0.008614374531302439  f2= 0.003142059124513359  f3= 0.0017596704993313398  f4= 0.015407507669635251 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2060  time= 59.85325741767883  loss= 0.02422445174942248  val_loss= 0.02816017371785244  f1= 0.00820961189795464  f2= 0.0009226828170839849  f3= 0.0014545795002582787  f4= 0.01363757753412558 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2070  time= 60.157851219177246  loss= 0.012275214608802515  val_loss= 0.031758822314379626  f1= 0.0038274607372416145  f2= 0.0005795881085529539  f3= 0.0011683146698815397  f4= 0.006699851093126408 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2080  time= 60.45168662071228  loss= 0.014923600052806732  val_loss= 0.025214203706733412  f1= 0.004757941017714776  f2= 0.0005851621576345786  f3= 0.002477537019126804  f4= 0.007102959858330572 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2090  time= 60.73197674751282  loss= 0.036416530825090734  val_loss= 0.03081528044449226  f1= 0.006763170807306131  f2= 0.0010870485693539674  f3= 0.0027630952630757076  f4= 0.02580321618535493 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2100  time= 61.01456165313721  loss= 0.01109626937912707  val_loss= 0.016101441185088668  f1= 0.004470790412798162  f2= 0.000528701753453535  f3= 0.0019392825794575343  f4= 0.004157494633417841 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2110  time= 61.302814960479736  loss= 0.010785974662724325  val_loss= 0.014325279721203345  f1= 0.0013514800173772476  f2= 0.0005117870921607502  f3= 0.005955422580793557  f4= 0.0029672849723927724 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2120  time= 61.57863211631775  loss= 0.013216296923765178  val_loss= 0.03242977336384538  f1= 0.0039304118996244175  f2= 0.0007539321491734742  f3= 0.001036735403610444  f4= 0.007495217471356841 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2130  time= 61.8593966960907  loss= 0.06541135422795583  val_loss= 0.028587139618484178  f1= 0.012087788413502082  f2= 0.00234235681088868  f3= 0.005350951550524713  f4= 0.045630257453040356 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2140  time= 62.14620280265808  loss= 0.016158137723027554  val_loss= 0.023031176151284145  f1= 0.0032534131410515647  f2= 0.0013929850449507168  f3= 0.0022000364273110215  f4= 0.009311703109714251 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2150  time= 62.423739433288574  loss= 0.016256273295565418  val_loss= 0.02412593226752324  f1= 0.0057976975769284565  f2= 0.00086949820301983  f3= 0.000883164427864935  f4= 0.008705913087752197 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2160  time= 62.715234994888306  loss= 0.007572594771904265  val_loss= 0.023381588500150357  f1= 0.0018615834429771255  f2= 0.0005866511026566193  f3= 0.001234707447050097  f4= 0.003889652779220423 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2170  time= 63.009284257888794  loss= 0.014212133999138447  val_loss= 0.036391561390371856  f1= 0.0036244268272056708  f2= 0.0008945282218711503  f3= 0.0007013142729097718  f4= 0.008991864677151852 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2180  time= 63.29532766342163  loss= 0.02418163953076699  val_loss= 0.05359733014875784  f1= 0.006153204448405436  f2= 0.0018222998981849796  f3= 0.004925563728088477  f4= 0.011280571456088096 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2190  time= 63.58395600318909  loss= 0.011933041278093588  val_loss= 0.023657462115810428  f1= 0.001322781197998383  f2= 0.0007145205647883365  f3= 0.002284847732042133  f4= 0.007610891783264733 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2200  time= 63.867459535598755  loss= 0.014368326343751925  val_loss= 0.027096797444077968  f1= 0.002563015961654379  f2= 0.0012714317488846114  f3= 0.001960087971735629  f4= 0.008573790661477305 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2210  time= 64.16396903991699  loss= 0.01521471388739385  val_loss= 0.03245227599115609  f1= 0.00477352632197231  f2= 0.000610572927796004  f3= 0.002630454348993231  f4= 0.0072001602886323035 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2220  time= 64.46916937828064  loss= 0.018022558186317632  val_loss= 0.058827908309908555  f1= 0.004119989928550374  f2= 0.0007112789797708756  f3= 0.004837191211286662  f4= 0.00835409806670972 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2230  time= 64.76347708702087  loss= 0.008287698613019355  val_loss= 0.03807096239899348  f1= 0.0029720659671429004  f2= 0.0004707106043853215  f3= 0.0006427247983337779  f4= 0.004202197243157357 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2240  time= 65.0492856502533  loss= 0.008396298355309752  val_loss= 0.019201125449930044  f1= 0.0018201888325042792  f2= 0.0006755916160053644  f3= 0.0035946671908663134  f4= 0.002305850715933795 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2250  time= 65.35953307151794  loss= 0.023526093195924164  val_loss= 0.04044584759410762  f1= 0.0032395595888046118  f2= 0.0014694657745438374  f3= 0.003896254743372092  f4= 0.014920813089203623 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2260  time= 65.648029088974  loss= 0.009696830233368356  val_loss= 0.029178065994388012  f1= 0.0035374485417828077  f2= 0.0007086316991881722  f3= 0.0025980511968503113  f4= 0.0028526987955470665 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2270  time= 65.92896103858948  loss= 0.022532457153481008  val_loss= 0.03771228878346395  f1= 0.004224185170138319  f2= 0.0011006881419557205  f3= 0.01006861513184065  f4= 0.0071389687095463165 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2280  time= 66.2308988571167  loss= 0.011148545559644544  val_loss= 0.02215006876020401  f1= 0.003072305177024661  f2= 0.000672360738887628  f3= 0.0020771683369244  f4= 0.005326711306807856 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2290  time= 66.51636362075806  loss= 0.02015574814016112  val_loss= 0.028856737545560104  f1= 0.0028051108603259797  f2= 0.0019376627187727034  f3= 0.004967197798033494  f4= 0.010445776763028942 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2300  time= 66.82608461380005  loss= 0.006882332533912919  val_loss= 0.024002614535003526  f1= 0.002800600106596663  f2= 0.0007056707801204204  f3= 0.0009583719415043046  f4= 0.0024176897056915306 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2310  time= 67.12801051139832  loss= 0.015581222916390641  val_loss= 0.04489227113514581  f1= 0.005193456820091129  f2= 0.0008731919722840233  f3= 0.001753227432203079  f4= 0.007761346691812412 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2320  time= 67.44083142280579  loss= 0.016996605474459973  val_loss= 0.025321559936609554  f1= 0.006202439819092551  f2= 0.0008827344649499137  f3= 0.0011943326616596671  f4= 0.00871709852875784 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2330  time= 67.75270938873291  loss= 0.011813733993745515  val_loss= 0.048113546182135564  f1= 0.003757872747951231  f2= 0.00041645260291020195  f3= 0.002022301893711198  f4= 0.00561710674917288 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2340  time= 68.05198216438293  loss= 0.010791345387872492  val_loss= 0.019011441745682273  f1= 0.002399837134246229  f2= 0.0008462692772810931  f3= 0.0030588181812421923  f4= 0.004486420795102978 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2350  time= 68.36355972290039  loss= 0.02194705869692078  val_loss= 0.018095239257885722  f1= 0.003067705399850703  f2= 0.0010659067193911798  f3= 0.0032541756178622373  f4= 0.014559270959816658 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2360  time= 68.65663623809814  loss= 0.009478594389439593  val_loss= 0.023349990914652563  f1= 0.0023199553758649944  f2= 0.0007821995286190577  f3= 0.0004712620436556224  f4= 0.005905177441299918 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2370  time= 68.94124698638916  loss= 0.013572674391860174  val_loss= 0.044324914275061675  f1= 0.0059362969084508594  f2= 0.0005725905742276104  f3= 0.0010352064181114295  f4= 0.006028580491070276 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2380  time= 69.26137185096741  loss= 0.008417973440869754  val_loss= 0.021399537463291294  f1= 0.001374472401784458  f2= 0.0009878663440456932  f3= 0.0003381409463668828  f4= 0.005717493748672721 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2390  time= 69.58404326438904  loss= 0.012683976612018047  val_loss= 0.040567888057198714  f1= 0.0046216002949448255  f2= 0.0007347594623160117  f3= 0.003473927499496639  f4= 0.0038536893552605683 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2400  time= 69.88145995140076  loss= 0.02035461647921286  val_loss= 0.08359524553523501  f1= 0.004596093012599819  f2= 0.001205387886215157  f3= 0.001169172220128619  f4= 0.013383963360269263 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2410  time= 70.18236064910889  loss= 0.020106980876055818  val_loss= 0.05930987952925325  f1= 0.005791970052366341  f2= 0.0011501292789705888  f3= 0.0020807181191230184  f4= 0.01108416342559587 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2420  time= 70.48691892623901  loss= 0.011108197634147698  val_loss= 0.02814485862006319  f1= 0.0028568361796758638  f2= 0.0007847352220327619  f3= 0.0006901405172965142  f4= 0.0067764857151425585 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2430  time= 70.7895393371582  loss= 0.018862276841538794  val_loss= 0.025357490359195524  f1= 0.006186278909110312  f2= 0.0010813208922038737  f3= 0.006596763927734292  f4= 0.004997913112490316 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2440  time= 71.08025002479553  loss= 0.01680948198483639  val_loss= 0.036966981134837304  f1= 0.006722083601578649  f2= 0.0005389291735492459  f3= 0.001920874508670974  f4= 0.00762759470103752 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2450  time= 71.41477251052856  loss= 0.010485085246500476  val_loss= 0.019882784203771153  f1= 0.001775708338105411  f2= 0.0006965819406530943  f3= 0.0008285509545810642  f4= 0.007184244013160907 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2460  time= 71.71317028999329  loss= 0.012973370781140895  val_loss= 0.023286736199516815  f1= 0.0018820228430009639  f2= 0.001405899926883697  f3= 0.001113921592238532  f4= 0.008571526419017704 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2470  time= 72.01200819015503  loss= 0.016377583921463604  val_loss= 0.0339102168518664  f1= 0.004696059419814003  f2= 0.0012906873352452554  f3= 0.0014379708631981657  f4= 0.00895286630320618 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2480  time= 72.30535078048706  loss= 0.01807735659247348  val_loss= 0.02688671478676554  f1= 0.0018880494627334249  f2= 0.0007236219719269341  f3= 0.00829532981788452  f4= 0.007170355339928602 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2490  time= 72.62011241912842  loss= 0.004866375837570088  val_loss= 0.028955277352674252  f1= 0.0012575582328999651  f2= 0.0005010421380323068  f3= 0.0002463200670386854  f4= 0.0028614553995991304 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2500  time= 72.90910172462463  loss= 0.026992656831570927  val_loss= 0.055106375236787075  f1= 0.0036569044943441688  f2= 0.0017579897501785484  f3= 0.010345272632916062  f4= 0.011232489954132147 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2510  time= 73.20439076423645  loss= 0.014824866072506276  val_loss= 0.022231988972850107  f1= 0.00241688273342069  f2= 0.0008496114503274504  f3= 0.0037178265891534006  f4= 0.007840545299604734 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2520  time= 73.51628017425537  loss= 0.009165468413723342  val_loss= 0.020463319635304542  f1= 0.0019664590459219913  f2= 0.0006916771074770473  f3= 0.003097397166019173  f4= 0.0034099350943051295 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2530  time= 73.82564234733582  loss= 0.02465219787102183  val_loss= 0.018345108889680453  f1= 0.00628879806619536  f2= 0.0020750200770438187  f3= 0.004573083243840418  f4= 0.011715296483942234 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2540  time= 74.10889410972595  loss= 0.0379206790261022  val_loss= 0.04375697588902079  f1= 0.009787366810530251  f2= 0.002008486186640682  f3= 0.007185137349870571  f4= 0.018939688679060696 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2550  time= 74.3758635520935  loss= 0.008168446855387647  val_loss= 0.020233133833282624  f1= 0.0024661358148172405  f2= 0.0009882360242701442  f3= 0.0006557704167025835  f4= 0.004058304599597678 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2560  time= 74.65266561508179  loss= 0.028472273082037144  val_loss= 0.05494448711922049  f1= 0.003378355113842745  f2= 0.0031932780313161514  f3= 0.002971056029675659  f4= 0.018929583907202587 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2570  time= 74.92628788948059  loss= 0.00990325319446173  val_loss= 0.013192294014581775  f1= 0.002152472573790586  f2= 0.0006125924494849311  f3= 0.0011189544679267877  f4= 0.006019233703259424 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2580  time= 75.20636510848999  loss= 0.006661913490926731  val_loss= 0.018042388798662125  f1= 0.0025288651221593884  f2= 0.0007702415532934811  f3= 0.00027648991502023204  f4= 0.0030863169004536287 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2590  time= 75.50218057632446  loss= 0.012919090223227861  val_loss= 0.021315429982591866  f1= 0.00309729526547589  f2= 0.0008407326244143049  f3= 0.00311494631286232  f4= 0.005866116020475346 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2600  time= 75.77828311920166  loss= 0.02390542012322205  val_loss= 0.028561283448173255  f1= 0.00734236578208778  f2= 0.0009329596117171948  f3= 0.0011040945801729324  f4= 0.01452600014924414 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2610  time= 76.06897974014282  loss= 0.01627763680070383  val_loss= 0.020535911669647487  f1= 0.002208479050773164  f2= 0.0011480748102219945  f3= 0.0034559377576136915  f4= 0.009465145182094977 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2620  time= 76.35657143592834  loss= 0.02236022344985818  val_loss= 0.03586765429876146  f1= 0.00530232850048071  f2= 0.0019192350633604114  f3= 0.003111128722800465  f4= 0.012027531163216592 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2630  time= 76.63421773910522  loss= 0.01787749435426268  val_loss= 0.040509354332446595  f1= 0.006218260081071295  f2= 0.0007529562013077453  f3= 0.004781836264651461  f4= 0.006124441807232178 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2640  time= 76.90934538841248  loss= 0.014293217528734185  val_loss= 0.028441877908275046  f1= 0.004654122076039031  f2= 0.0010810910715629647  f3= 0.0025954614657604943  f4= 0.0059625429153716955 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2650  time= 77.18135857582092  loss= 0.0067211805973841484  val_loss= 0.015938789167251926  f1= 0.0017151409985259897  f2= 0.000533230581096926  f3= 0.0015472069824535885  f4= 0.0029256020353076446 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2660  time= 77.48943376541138  loss= 0.01566594387271253  val_loss= 0.020509269834858478  f1= 0.004357423661279143  f2= 0.0005703009713251896  f3= 0.0009293315667405106  f4= 0.009808887673367685 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2670  time= 77.81272578239441  loss= 0.014699944408306674  val_loss= 0.022070749537434188  f1= 0.0034268049915665995  f2= 0.0008085025704519925  f3= 0.001496388447783877  f4= 0.008968248398504209 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2680  time= 78.11302542686462  loss= 0.009626069614684603  val_loss= 0.030941199788201427  f1= 0.00302135025882337  f2= 0.0011570513271778587  f3= 0.0004545174130623517  f4= 0.004993150615621023 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2690  time= 78.42629742622375  loss= 0.014320688360012332  val_loss= 0.018515154677091274  f1= 0.003867340996518037  f2= 0.0008351489034986332  f3= 0.00019508037149645866  f4= 0.009423118088499204 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2700  time= 78.7269036769867  loss= 0.02511035870895977  val_loss= 0.025641356130887156  f1= 0.003094045071150528  f2= 0.0014215385601965803  f3= 0.0022970611468222228  f4= 0.01829771393079044 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2710  time= 79.0215003490448  loss= 0.013888429550621804  val_loss= 0.02684768220108088  f1= 0.0024675864257917603  f2= 0.000705798446358829  f3= 0.0036857964028766074  f4= 0.007029248275594605 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2720  time= 79.31236934661865  loss= 0.018619871842827197  val_loss= 0.051339816654146525  f1= 0.0025975617582466273  f2= 0.00143686002683572  f3= 0.00563462617752515  f4= 0.0089508238802197 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2730  time= 79.61174035072327  loss= 0.005841656261084235  val_loss= 0.020638916402001946  f1= 0.0012702081145408183  f2= 0.0007399276698012368  f3= 0.0009612027351466592  f4= 0.0028703177415955214 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2740  time= 79.9036328792572  loss= 0.010298589860301395  val_loss= 0.014690465409108018  f1= 0.003110322387773586  f2= 0.0006090374776781694  f3= 0.00023072613725304202  f4= 0.006348503857596599 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2750  time= 80.17516613006592  loss= 0.01484197389619385  val_loss= 0.04102370909394134  f1= 0.0046106275149401426  f2= 0.0006558169452916342  f3= 0.0022209992976173257  f4= 0.007354530138344748 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2760  time= 80.44177770614624  loss= 0.007270821646063439  val_loss= 0.014367992337255074  f1= 0.0011561862299965247  f2= 0.0005524045252855088  f3= 0.0014662247400549647  f4= 0.004096006150726442 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2770  time= 80.73724603652954  loss= 0.011178096173524416  val_loss= 0.02174110037426007  f1= 0.002836557863536719  f2= 0.0007383641246910115  f3= 0.005134920804443743  f4= 0.0024682533808529413 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2780  time= 81.01308131217957  loss= 0.011461009171641357  val_loss= 0.015898038814364267  f1= 0.001521867207666832  f2= 0.0007721960543344688  f3= 0.0006073799029307122  f4= 0.008559566006709348 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2790  time= 81.28837537765503  loss= 0.013777145432705093  val_loss= 0.02043543198555587  f1= 0.0030534602389957724  f2= 0.0008656471793672891  f3= 0.002459291707187229  f4= 0.007398746307154803 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2800  time= 81.58643913269043  loss= 0.021565555208411896  val_loss= 0.025753443583224088  f1= 0.003965869552331187  f2= 0.0016874777000350668  f3= 0.00290433908224449  f4= 0.01300786887380115 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2810  time= 81.90561485290527  loss= 0.006217960353160536  val_loss= 0.011571001602023793  f1= 0.0014105118340969845  f2= 0.0005923336940412348  f3= 0.0005996140165933677  f4= 0.003615500808428949 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2820  time= 82.21079730987549  loss= 0.021484218575568644  val_loss= 0.03288114975914404  f1= 0.0025567412614939562  f2= 0.0011317240091049089  f3= 0.009728807477564947  f4= 0.008066945827404833 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2830  time= 82.53968358039856  loss= 0.005607588116552434  val_loss= 0.011983779448859103  f1= 0.0024737426164348456  f2= 0.0004636452919574489  f3= 0.00038588379945846124  f4= 0.0022843164087016777 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2840  time= 82.85790514945984  loss= 0.021620295871628285  val_loss= 0.06635404150890555  f1= 0.005833062959451862  f2= 0.001076696995462371  f3= 0.002212532714761619  f4= 0.012498003201952435 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2850  time= 83.18352961540222  loss= 0.00431770354249371  val_loss= 0.016525816744970216  f1= 0.0013844950487087962  f2= 0.0003802563086808038  f3= 0.0007123496468945771  f4= 0.0018406025382095336 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2860  time= 83.51719284057617  loss= 0.018278015390794083  val_loss= 0.018761552774871862  f1= 0.005166760074584653  f2= 0.0015306519158779371  f3= 0.00027878349016340065  f4= 0.011301819910168093 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2870  time= 83.84552264213562  loss= 0.010378835465710739  val_loss= 0.02072807341747486  f1= 0.0020258846657614255  f2= 0.0006963097732593009  f3= 0.0019272026692805993  f4= 0.005729438357409412 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2880  time= 84.1673800945282  loss= 0.017139398169637398  val_loss= 0.03388674118520736  f1= 0.008831714527608602  f2= 0.0006784210244027638  f3= 0.002278999180498482  f4= 0.0053502634371275495 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2890  time= 84.48403334617615  loss= 0.027978981924985658  val_loss= 0.037548124895561225  f1= 0.0028443832899822034  f2= 0.00120515667071517  f3= 0.0017236500013166017  f4= 0.02220579196297168 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2900  time= 84.81045794487  loss= 0.014828424412732378  val_loss= 0.02154398239041903  f1= 0.002611514853315358  f2= 0.0012719668970216397  f3= 6.587814853762004e-05  f4= 0.010879064513857763 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2910  time= 85.10791206359863  loss= 0.015231546735105019  val_loss= 0.026564176022303265  f1= 0.002585820151813695  f2= 0.0006744955402376495  f3= 0.000977322125031027  f4= 0.01099390891802265 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2920  time= 85.39397692680359  loss= 0.01353151825988004  val_loss= 0.027088264205203397  f1= 0.0039576927983761825  f2= 0.0009647498162847216  f3= 0.0004874634588885324  f4= 0.008121612186330604 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2930  time= 85.68637728691101  loss= 0.00852956053420275  val_loss= 0.02392863660029958  f1= 0.00274342475965935  f2= 0.0006592756767615254  f3= 0.0008145738694828746  f4= 0.004312286228299001 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2940  time= 85.96137261390686  loss= 0.015349755942476012  val_loss= 0.03258135018371062  f1= 0.00439058992265621  f2= 0.0009501874296995149  f3= 0.0027546710906542695  f4= 0.007254307499466016 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2950  time= 86.23006892204285  loss= 0.012214132927607271  val_loss= 0.017911701830117123  f1= 0.0018689432857859542  f2= 0.0007322729070846481  f3= 0.0011148095209591092  f4= 0.008498107213777561 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2960  time= 86.52885460853577  loss= 0.01584690040587464  val_loss= 0.028802865561589147  f1= 0.0014413117932746827  f2= 0.001412754959845822  f3= 0.003925879582868838  f4= 0.009066954069885298 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2970  time= 86.85304117202759  loss= 0.01377345627841728  val_loss= 0.03965504319864964  f1= 0.004413610238997322  f2= 0.0004942401768713519  f3= 0.003399229057631547  f4= 0.0054663768049170594 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2980  time= 87.16276812553406  loss= 0.019314437249182765  val_loss= 0.035615586515544  f1= 0.0025948187023059417  f2= 0.0010914297631183952  f3= 0.0032436959101974296  f4= 0.012384492873561 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2990  time= 87.44561290740967  loss= 0.007341802017582934  val_loss= 0.01808889010627476  f1= 0.0026269527183874855  f2= 0.0006055345906008883  f3= 0.0005040287777860571  f4= 0.0036052859308085025 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3000  time= 87.71301603317261  loss= 0.014135513644630117  val_loss= 0.014708277334701033  f1= 0.0033603247890261674  f2= 0.001063996445978548  f3= 0.0022457416191023778  f4= 0.007465450790523025 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3010  time= 88.004230260849  loss= 0.014509321467719383  val_loss= 0.020994382523976098  f1= 0.003807621269086009  f2= 0.0006566090011178522  f3= 0.0032033322140105885  f4= 0.0068417589835049345 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3020  time= 88.28970265388489  loss= 0.006371049483112015  val_loss= 0.015151750157094317  f1= 0.0020725480868764652  f2= 0.0006076646571388845  f3= 0.0009039922867017341  f4= 0.0027868444523949302 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3030  time= 88.58350729942322  loss= 0.010106658628749564  val_loss= 0.0181265551609572  f1= 0.0022395683045184187  f2= 0.0009734767382463248  f3= 0.001320092537865486  f4= 0.005573521048119334 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3040  time= 88.89578366279602  loss= 0.01017935999202611  val_loss= 0.01670551779657279  f1= 0.001287224813345285  f2= 0.0009428827688453494  f3= 0.0016809192684557443  f4= 0.006268333141379731 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3050  time= 89.22346687316895  loss= 0.016628153979138586  val_loss= 0.016238944320095514  f1= 0.0033721598720543  f2= 0.002185243747474512  f3= 0.001134588678364144  f4= 0.00993616168124563 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3060  time= 89.52642774581909  loss= 0.013181251294329425  val_loss= 0.016627982805651184  f1= 0.0024435478841051688  f2= 0.0003825580623640008  f3= 0.002485793354970276  f4= 0.00786935199288998 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3070  time= 89.80710554122925  loss= 0.0111946187496733  val_loss= 0.02329140497608623  f1= 0.004511910162287087  f2= 0.0005135629414447833  f3= 0.0018093980700810118  f4= 0.004359747575860418 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3080  time= 90.11439061164856  loss= 0.012082224058652143  val_loss= 0.020761284084629295  f1= 0.0010456294281037763  f2= 0.000568314325247848  f3= 0.005526235695120272  f4= 0.004942044610180249 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3090  time= 90.40720844268799  loss= 0.014410960569521299  val_loss= 0.034794016894877634  f1= 0.003226677416243906  f2= 0.00057900393956506  f3= 0.0011804752328822504  f4= 0.009424803980830083 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3100  time= 90.70050287246704  loss= 0.008247878029759938  val_loss= 0.03290253019794996  f1= 0.0021154687754011367  f2= 0.00048178631601755533  f3= 0.0028555410453397125  f4= 0.002795081893001532 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3110  time= 91.00776839256287  loss= 0.011685442230340714  val_loss= 0.011307947569687372  f1= 0.0037491528280340146  f2= 0.0012606957162142104  f3= 0.0008824756996022175  f4= 0.005793117986490272 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3120  time= 91.3068516254425  loss= 0.010059418798159196  val_loss= 0.02957592784511844  f1= 0.0023447655198168054  f2= 0.0004312638115184108  f3= 0.001744094966914679  f4= 0.005539294499909302 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3130  time= 91.61558890342712  loss= 0.016154406990795147  val_loss= 0.018295271377995193  f1= 0.0026346499951049023  f2= 0.001626003475587185  f3= 0.0017101907347725617  f4= 0.010183562785330495 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3140  time= 91.88613653182983  loss= 0.02405276598789972  val_loss= 0.029554588458092453  f1= 0.002028898315437054  f2= 0.0006636712005401117  f3= 0.0020900889394960165  f4= 0.01927010753242654 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3150  time= 92.15923643112183  loss= 0.014353114259886146  val_loss= 0.029666583794595237  f1= 0.005488440511029595  f2= 0.0013486161732802074  f3= 0.0008040808746684986  f4= 0.0067119767009078445 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3160  time= 92.43710708618164  loss= 0.015484416741152926  val_loss= 0.021430890389565437  f1= 0.005412777767377998  f2= 0.0012932218473703115  f3= 0.003976210898965527  f4= 0.004802206227439094 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3170  time= 92.70647764205933  loss= 0.01488734850856051  val_loss= 0.029083866365712894  f1= 0.006622353746237798  f2= 0.0008203430348550423  f3= 0.0008826543593887324  f4= 0.006561997368078936 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3180  time= 92.9845142364502  loss= 0.00887880476735957  val_loss= 0.01869482023479551  f1= 0.0019720111918074086  f2= 0.0011000508384681668  f3= 0.0006590805512206994  f4= 0.005147662185863295 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3190  time= 93.2553780078888  loss= 0.016595605581211325  val_loss= 0.025916881841615818  f1= 0.004606709724996632  f2= 0.0007403022051563646  f3= 0.0015459375054439506  f4= 0.00970265614561438 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3200  time= 93.53620386123657  loss= 0.017528839935552567  val_loss= 0.027328573833855577  f1= 0.003852316187722225  f2= 0.000527924038532369  f3= 0.00210073754820402  f4= 0.011047862161093954 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3210  time= 93.8137412071228  loss= 0.010996193930992585  val_loss= 0.034174519306918776  f1= 0.004801288606590175  f2= 0.0007217750448068747  f3= 0.00016766055573464735  f4= 0.005305469723860888 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3220  time= 94.09370565414429  loss= 0.013701546723003312  val_loss= 0.027585400093633923  f1= 0.0036289153426811486  f2= 0.0004740094175480116  f3= 0.0012775162940234371  f4= 0.008321105668750713 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3230  time= 94.368168592453  loss= 0.010813202313920356  val_loss= 0.021244575043467346  f1= 0.0017804627607827979  f2= 0.0005296789588909268  f3= 0.002050883569845553  f4= 0.006452177024401078 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3240  time= 94.63999915122986  loss= 0.028903579567112574  val_loss= 0.06448814779355079  f1= 0.011938528650578964  f2= 0.001126586731127769  f3= 0.0038304492534286133  f4= 0.012008014931977224 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3250  time= 94.90439987182617  loss= 0.01378185668484803  val_loss= 0.031336189049563416  f1= 0.0029693180185264606  f2= 0.0007892026101920546  f3= 0.003702701896911509  f4= 0.006320634159218006 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3260  time= 95.18313765525818  loss= 0.029493865408574127  val_loss= 0.03977448172715075  f1= 0.009076174844186132  f2= 0.0007215885697852065  f3= 0.007677352996505987  f4= 0.012018748998096802 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3270  time= 95.44553351402283  loss= 0.008520988583618458  val_loss= 0.019663436835984285  f1= 0.002254912898336236  f2= 0.0007073100354418801  f3= 0.0008180643864852654  f4= 0.004740701263355077 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3280  time= 95.75956654548645  loss= 0.01547610646800556  val_loss= 0.0366166838806974  f1= 0.0037989119852560507  f2= 0.0011921718254385104  f3= 0.003397422997190456  f4= 0.007087599660120541 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3290  time= 96.04068160057068  loss= 0.004872617730188729  val_loss= 0.012928239017690827  f1= 0.0009803071981138898  f2= 0.0004387125918712926  f3= 0.001019879231367474  f4= 0.0024337187088360724 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3300  time= 96.3095977306366  loss= 0.0156887225745848  val_loss= 0.032265463543494556  f1= 0.00323337720515291  f2= 0.0007771846144555313  f3= 0.0028315259548635  f4= 0.008846634800112855 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3310  time= 96.5778374671936  loss= 0.026525739321211893  val_loss= 0.05818898982637281  f1= 0.004491683278007101  f2= 0.002467356901166222  f3= 0.001549120501831643  f4= 0.018017578640206925 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3320  time= 96.88140726089478  loss= 0.008533423333270033  val_loss= 0.012030073081526687  f1= 0.0014910876276721897  f2= 0.0011392880137926966  f3= 0.0003642252421048663  f4= 0.005538822449700282 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3330  time= 97.22135901451111  loss= 0.009023441433864852  val_loss= 0.01645446912839436  f1= 0.00161889192097034  f2= 0.0006549669533328933  f3= 0.0013929261217072502  f4= 0.005356656437854369 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3340  time= 97.54433417320251  loss= 0.016224565344308692  val_loss= 0.032505182865766265  f1= 0.004363763682027689  f2= 0.00048186217202685224  f3= 0.0009010174772373832  f4= 0.010477922013016764 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3350  time= 97.83587551116943  loss= 0.02412367125133119  val_loss= 0.02293127223391093  f1= 0.0027807583818304934  f2= 0.0016522605779788174  f3= 0.0012872674377913183  f4= 0.018403384853730564 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3360  time= 98.12103486061096  loss= 0.007243215764069142  val_loss= 0.01023065865219785  f1= 0.0017340974128112207  f2= 0.0006189726215963571  f3= 0.0020491633912150886  f4= 0.002840982338446475 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3370  time= 98.40434837341309  loss= 0.009509424860920513  val_loss= 0.03236212970124236  f1= 0.0025285898880296593  f2= 0.0005766726571122231  f3= 0.0016915061034838035  f4= 0.0047126562122948275 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3380  time= 98.72174310684204  loss= 0.0050961398302115666  val_loss= 0.01117992376272535  f1= 0.001065354551257869  f2= 0.0005131104233371625  f3= 0.0005428265022888251  f4= 0.0029748483533277095 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3390  time= 99.01664113998413  loss= 0.03907120594022081  val_loss= 0.03163612129275645  f1= 0.0038309665694578957  f2= 0.0031506067534145424  f3= 0.003081626850672902  f4= 0.029008005766675466 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3400  time= 99.33277750015259  loss= 0.005955437299153009  val_loss= 0.020571742535078645  f1= 0.0013516851717340836  f2= 0.0007103614227249875  f3= 0.0016313256673065634  f4= 0.0022620650373873737 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3410  time= 99.62879920005798  loss= 0.01015178313660525  val_loss= 0.017447531293240847  f1= 0.0026648631957805033  f2= 0.0005969771142775117  f3= 0.0014210565155132401  f4= 0.005468886311033996 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3420  time= 99.93008756637573  loss= 0.011707756993733954  val_loss= 0.022145686754642152  f1= 0.001548341904942893  f2= 0.000588883933871355  f3= 0.0026237411164044054  f4= 0.0069467900385153025 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3430  time= 100.23188519477844  loss= 0.008332473526444653  val_loss= 0.019197480948202186  f1= 0.00261234245681507  f2= 0.0005181310000223839  f3= 0.0015570081954352152  f4= 0.0036449918741719854 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3440  time= 100.5207531452179  loss= 0.007843070864167011  val_loss= 0.01654520526065212  f1= 0.0016220513490655536  f2= 0.0006636711510008362  f3= 0.00021968512407684172  f4= 0.005337663240023778 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3450  time= 100.80698037147522  loss= 0.01302405468823123  val_loss= 0.034276880184613716  f1= 0.0032368567136919317  f2= 0.0007989975106333049  f3= 0.005400001869410552  f4= 0.003588198594495441 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3460  time= 101.11730408668518  loss= 0.024084930284224173  val_loss= 0.023087588285421415  f1= 0.0033566860116174845  f2= 0.0015794324705101727  f3= 0.005067934812387022  f4= 0.014080876989709496 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3470  time= 101.43275308609009  loss= 0.009217491187925845  val_loss= 0.026079615608930663  f1= 0.004517355929327991  f2= 0.0005661174228622103  f3= 0.00032921129953397075  f4= 0.0038048065362016707 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3480  time= 101.7396252155304  loss= 0.006847147295673238  val_loss= 0.019323655482675468  f1= 0.0028532004071564333  f2= 0.000506385026423592  f3= 0.00044114080198360894  f4= 0.003046421060109604 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3490  time= 102.02149796485901  loss= 0.006532895653301624  val_loss= 0.015269541189867973  f1= 0.0012182287724962727  f2= 0.0007352099255333466  f3= 0.00029188246558975163  f4= 0.004287574489682253 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3500  time= 102.30618023872375  loss= 0.009759029513906494  val_loss= 0.01851246364903632  f1= 0.001648907979495767  f2= 0.0006111832320936856  f3= 0.001299021648712902  f4= 0.006199916653604139 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3510  time= 102.57330369949341  loss= 0.14274082766246046  val_loss= 0.01924151154402339  f1= 0.0026437281831835934  f2= 0.01151230593199658  f3= 0.0010485023536199493  f4= 0.12753629119366036 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3520  time= 102.85825514793396  loss= 0.01365015456344564  val_loss= 0.02553949763406193  f1= 0.0024353670212574267  f2= 0.0005863551775061681  f3= 0.0007972433059251753  f4= 0.009831189058756871 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3530  time= 103.15366506576538  loss= 0.01532954880384664  val_loss= 0.021203893804422964  f1= 0.001624447038656418  f2= 0.0008712831944186145  f3= 0.0016869394430527162  f4= 0.011146879127718891 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3540  time= 103.4677734375  loss= 0.030459665194554623  val_loss= 0.035633879528331094  f1= 0.003423089295101203  f2= 0.0007875666352511568  f3= 0.00995633687476171  f4= 0.01629267238944055 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3550  time= 103.76679158210754  loss= 0.011616988493538996  val_loss= 0.026772012922143724  f1= 0.0027965475158436286  f2= 0.0007481002598926502  f3= 0.0025025336242190614  f4= 0.005569807093583654 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3560  time= 104.05193614959717  loss= 0.013334812300475138  val_loss= 0.017418620992398492  f1= 0.002707213205621253  f2= 0.00044530364436336803  f3= 0.0033112524408396133  f4= 0.006871043009650901 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3570  time= 104.3521716594696  loss= 0.013399024586628618  val_loss= 0.02576929853137847  f1= 0.0015798281323417564  f2= 0.0005811173883486837  f3= 0.0042547131682914635  f4= 0.006983365897646718 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3580  time= 104.65427470207214  loss= 0.008016042738261049  val_loss= 0.020554429708011628  f1= 0.0034782412437194786  f2= 0.0005726018390416796  f3= 9.762707452789852e-05  f4= 0.003867572580971992 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3590  time= 104.92547535896301  loss= 0.01995400190111322  val_loss= 0.016771460434442866  f1= 0.002694423114492556  f2= 0.0007392663693674414  f3= 0.004104408994742806  f4= 0.012415903422510421 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3600  time= 105.2007954120636  loss= 0.010636341289748554  val_loss= 0.016558600610739528  f1= 0.0027172293356795604  f2= 0.0004234789954957804  f3= 0.0005072984532508159  f4= 0.006988334505322398 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3610  time= 105.47863411903381  loss= 0.010073625270799406  val_loss= 0.016645070441881807  f1= 0.0014863324684214966  f2= 0.00044642771448137505  f3= 0.0016538706975508255  f4= 0.00648699439034571 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3620  time= 105.7486481666565  loss= 0.015689883083156195  val_loss= 0.013018299925551383  f1= 0.0022515021777944778  f2= 0.0008777846793258711  f3= 0.0004143738585531906  f4= 0.012146222367482654 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3630  time= 106.04147982597351  loss= 0.011082405710220893  val_loss= 0.021378963605732288  f1= 0.0026075437590334146  f2= 0.0004775196474198615  f3= 0.002449116991533658  f4= 0.005548225312233959 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3640  time= 106.34107708930969  loss= 0.01164077694161707  val_loss= 0.031042724131426898  f1= 0.0031496098460122722  f2= 0.0007016230143905945  f3= 0.0020656261365320635  f4= 0.00572391794468214 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3650  time= 106.6467707157135  loss= 0.008077211861441605  val_loss= 0.038288342960807534  f1= 0.002045173424924391  f2= 0.0007447746882501908  f3= 0.0011934324506011843  f4= 0.004093831297665838 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3660  time= 106.95921611785889  loss= 0.013126863991058162  val_loss= 0.03903981044543817  f1= 0.003052902039420154  f2= 0.0005743031887149476  f3= 0.004840636805792647  f4= 0.004659021957130415 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3670  time= 107.27927732467651  loss= 0.04239597878890656  val_loss= 0.018203165245131464  f1= 0.004889399767854392  f2= 0.0012839547490224607  f3= 0.0037899539550505443  f4= 0.03243267031697916 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3680  time= 107.61038041114807  loss= 0.005550148763486589  val_loss= 0.018865691060095516  f1= 0.0012884871896934045  f2= 0.0006253615764270637  f3= 0.0002850342559993914  f4= 0.003351265741366729 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3690  time= 107.92249035835266  loss= 0.011355216920147666  val_loss= 0.021936407513710763  f1= 0.002550729022502849  f2= 0.0005970534158129204  f3= 0.00461194246484248  f4= 0.003595492016989416 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3700  time= 108.21248531341553  loss= 0.007100249343527612  val_loss= 0.016672970410969844  f1= 0.0015058718230236301  f2= 0.0005774091540791107  f3= 0.0012106343546221903  f4= 0.0038063340118026814 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3710  time= 108.52404284477234  loss= 0.014109310954062738  val_loss= 0.019212834904404223  f1= 0.0024119097428970163  f2= 0.0006991935951186513  f3= 0.0004997754556977321  f4= 0.01049843216034934 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3720  time= 108.83553862571716  loss= 0.013767622426960445  val_loss= 0.028336089918695347  f1= 0.0037959585109102467  f2= 0.001272712204087384  f3= 0.0015643089352434419  f4= 0.007134642776719372 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3730  time= 109.12887454032898  loss= 0.015378327766154791  val_loss= 0.019290265542942695  f1= 0.0024494568742568826  f2= 0.001065764018177088  f3= 0.0027269266099930636  f4= 0.009136180263727757 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3740  time= 109.42014336585999  loss= 0.0066342543991477106  val_loss= 0.023013138026768482  f1= 0.0017958077469034208  f2= 0.0004212413551282284  f3= 0.0016909344435398592  f4= 0.0027262708535762026 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3750  time= 109.70329928398132  loss= 0.009100806465360765  val_loss= 0.02563821395264245  f1= 0.0014106701769042022  f2= 0.0006265529051950715  f3= 0.003910839516271878  f4= 0.0031527438669896133 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3760  time= 109.98957681655884  loss= 0.011417108322779987  val_loss= 0.01051469999150991  f1= 0.0024934562778109255  f2= 0.0008747103034959129  f3= 0.0014363505995998649  f4= 0.006612591141873285 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3770  time= 110.29402470588684  loss= 0.012705100034346358  val_loss= 0.029265818449995137  f1= 0.001452122296875095  f2= 0.0008200304734364  f3= 0.0009016828316018447  f4= 0.00953126443243302 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3780  time= 110.61414194107056  loss= 0.010677855240179286  val_loss= 0.03597498313518381  f1= 0.001490167377227936  f2= 0.00040283782821743085  f3= 0.006661844051011279  f4= 0.0021230059837226384 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3790  time= 110.90242671966553  loss= 0.00964971948415533  val_loss= 0.011984375826680355  f1= 0.0016838530219845358  f2= 0.0005081041787695519  f3= 0.0025841158425738637  f4= 0.004873646440827377 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3800  time= 111.21175980567932  loss= 0.014734972786806737  val_loss= 0.014732581856299502  f1= 0.0023663217175285915  f2= 0.0005627981631334514  f3= 0.00046063664382870427  f4= 0.011345216262315988 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3810  time= 111.52514743804932  loss= 0.011057368790789326  val_loss= 0.017607735725194362  f1= 0.003243612500115499  f2= 0.0005370998636722274  f3= 0.0004209963635784665  f4= 0.0068556600634231335 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3820  time= 111.82848405838013  loss= 0.007870276495702322  val_loss= 0.014784908273052124  f1= 0.0014504905497204085  f2= 0.0005944307192384013  f3= 0.0030020779086655414  f4= 0.0028232773180779706 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3830  time= 112.10911154747009  loss= 0.006297060506889039  val_loss= 0.017088666497076493  f1= 0.0007902759530034461  f2= 0.0005308406205846455  f3= 0.0014967370785372383  f4= 0.0034792068547637076 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3840  time= 112.39524626731873  loss= 0.018252137881829267  val_loss= 0.015143329422234824  f1= 0.0022935058103567105  f2= 0.0009356450211830124  f3= 0.004481834180149322  f4= 0.010541152870140222 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3850  time= 112.69457149505615  loss= 0.005191453715644401  val_loss= 0.013550819924520932  f1= 0.0008889085586855542  f2= 0.0003419122542341283  f3= 0.00034831907457772874  f4= 0.003612313828146991 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3860  time= 112.9681179523468  loss= 0.009618053785460741  val_loss= 0.016511683442989485  f1= 0.0017899482449209901  f2= 0.0005719845832903723  f3= 0.0035629942440979167  f4= 0.0036931267131514625 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3870  time= 113.27453422546387  loss= 0.009254330664218855  val_loss= 0.02192374849576833  f1= 0.0020272746081618775  f2= 0.0007479781303158994  f3= 0.0031411516428529514  f4= 0.003337926282888127 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3880  time= 113.59891676902771  loss= 0.009100245166618934  val_loss= 0.03486805180579358  f1= 0.0024018291450278173  f2= 0.0007625349154322063  f3= 0.00229483678460929  f4= 0.003641044321549622 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3890  time= 113.89661359786987  loss= 0.011924527837071208  val_loss= 0.020513537601158295  f1= 0.0014713951531781626  f2= 0.0007962785481771616  f3= 0.003974533194254589  f4= 0.005682320941461296 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3900  time= 114.21465396881104  loss= 0.012289882764559152  val_loss= 0.029415121067652057  f1= 0.0025851830235439667  f2= 0.0009241161830862062  f3= 0.00045317460035272  f4= 0.008327408957576258 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3910  time= 114.531179189682  loss= 0.0063774011608658246  val_loss= 0.012630495912969893  f1= 0.002233417197503107  f2= 0.00041670076877219536  f3= 0.0008891341453423178  f4= 0.0028381490492482057 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3920  time= 114.82787585258484  loss= 0.007484266281298671  val_loss= 0.0168164070506929  f1= 0.002058005129780393  f2= 0.0005595517956009356  f3= 0.0011289850068677268  f4= 0.003737724349049615 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3930  time= 115.12250304222107  loss= 0.00874285099197221  val_loss= 0.023386624071498437  f1= 0.0032129896092125457  f2= 0.0006270445931919461  f3= 0.0006426688072070709  f4= 0.004260147982360646 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3940  time= 115.44628739356995  loss= 0.007218717570774799  val_loss= 0.015373216433115419  f1= 0.0007790456876496389  f2= 0.0006576223410287066  f3= 0.0007276400863535716  f4= 0.005054409455742884 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3950  time= 115.76748514175415  loss= 0.011710673291207013  val_loss= 0.012953908578547462  f1= 0.0018542373498038618  f2= 0.00048448609080176575  f3= 0.004828678304507942  f4= 0.004543271546093444 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3960  time= 116.0704460144043  loss= 0.013728345222673558  val_loss= 0.027283973296732413  f1= 0.0024822211927250926  f2= 0.0011349371302617833  f3= 0.0009119430534282666  f4= 0.009199243846258415 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3970  time= 116.34896087646484  loss= 0.015575080906318031  val_loss= 0.018686636740510125  f1= 0.003038937785541663  f2= 0.0007993303679290272  f3= 0.00626660592276506  f4= 0.00547020683008228 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3980  time= 116.63726592063904  loss= 0.006611895265066688  val_loss= 0.015353531502655502  f1= 0.0021991008419718584  f2= 0.0005214432828490358  f3= 0.0012588175972769262  f4= 0.002632533542968867 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3990  time= 116.9099793434143  loss= 0.006621024932464134  val_loss= 0.013866238440315034  f1= 0.0015037680220050915  f2= 0.0004770976272010223  f3= 0.0027257786482377134  f4= 0.0019143806350203066 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4000  time= 117.20874285697937  loss= 0.00850997281481438  val_loss= 0.01595471342344054  f1= 0.001786542493072918  f2= 0.0007946851000024546  f3= 0.0018257807382722003  f4= 0.004102964483466807 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4010  time= 117.52606534957886  loss= 0.010082388920176072  val_loss= 0.011218427697037577  f1= 0.0035866234330507746  f2= 0.0011987798385422568  f3= 0.0003483151302328134  f4= 0.004948670518350227 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4020  time= 117.86108303070068  loss= 0.019515225787237873  val_loss= 0.022773176731812544  f1= 0.003787205698062569  f2= 0.0008968461466813817  f3= 0.0038509217864799252  f4= 0.010980252156013994 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4030  time= 118.17516875267029  loss= 0.006681379636738409  val_loss= 0.016963215299339886  f1= 0.0036706285450474746  f2= 0.00069614431112924  f3= 0.0002631047282502337  f4= 0.0020515020523114605 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4040  time= 118.49128270149231  loss= 0.00785237226910085  val_loss= 0.01603640630564051  f1= 0.0018085938390509442  f2= 0.0005400404343876752  f3= 0.00016928361838282566  f4= 0.0053344543772794045 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4050  time= 118.82532835006714  loss= 0.011828488311088741  val_loss= 0.01912861348377328  f1= 0.002331503899238433  f2= 0.00035640426830315145  f3= 0.0009711398090157917  f4= 0.008169440334531363 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4060  time= 119.13864874839783  loss= 0.013180643961972022  val_loss= 0.02380103034671146  f1= 0.0019099137492365418  f2= 0.0008937171679856914  f3= 0.000833565088481094  f4= 0.009543447956268694 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4070  time= 119.42964911460876  loss= 0.015283416791032858  val_loss= 0.03195131666680483  f1= 0.0021871528823879417  f2= 0.0014566907939305067  f3= 0.0005022693677431924  f4= 0.011137303746971217 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4080  time= 119.74665188789368  loss= 0.010838223599675774  val_loss= 0.02442834971836642  f1= 0.0024165885671801703  f2= 0.0007113963515273813  f3= 0.000534250254568311  f4= 0.0071759884263999105 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4090  time= 120.0530149936676  loss= 0.005378097476183687  val_loss= 0.01386167856099187  f1= 0.001613783759522101  f2= 0.0003332639084175637  f3= 0.002000925936601881  f4= 0.0014301238716421414 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4100  time= 120.35338640213013  loss= 0.011255456921062753  val_loss= 0.024001322100446143  f1= 0.002656029866061888  f2= 0.0007864061976418056  f3= 0.00030239315091430533  f4= 0.007510627706444756 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4110  time= 120.64169669151306  loss= 0.008568040700765464  val_loss= 0.017235115052164875  f1= 0.0011793637988772752  f2= 0.0006445807420639867  f3= 0.00310208189439053  f4= 0.0036420142654336692 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4120  time= 120.95298743247986  loss= 0.008713762846139646  val_loss= 0.01369235370753517  f1= 0.001310261293745847  f2= 0.0003747941727573775  f3= 0.00047369850666844666  f4= 0.006555008872967973 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4130  time= 121.28030896186829  loss= 0.013179948954612398  val_loss= 0.010331556243810571  f1= 0.003845663279361478  f2= 0.0007023152112351248  f3= 0.0037642803350931967  f4= 0.004867690128922597 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4140  time= 121.6027193069458  loss= 0.012205010130642388  val_loss= 0.020424182908996284  f1= 0.0034430729865957913  f2= 0.0005086743720119202  f3= 0.0016769337056078526  f4= 0.006576329066426825 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4150  time= 121.92315220832825  loss= 0.020236840921450935  val_loss= 0.010700755520335078  f1= 0.004727057783641125  f2= 0.0016830863318682947  f3= 0.0003544057649344163  f4= 0.0134722910410071 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4160  time= 122.21451759338379  loss= 0.007243709565061193  val_loss= 0.012737765914307423  f1= 0.0020338912437097194  f2= 0.0004793584690431046  f3= 0.0018218194784246939  f4= 0.002908640373883674 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4170  time= 122.48309135437012  loss= 0.009076798142697894  val_loss= 0.012021816850515323  f1= 0.0021721199486196135  f2= 0.0004999838546216635  f3= 0.0028887680425740635  f4= 0.0035159262968825523 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4180  time= 122.78034782409668  loss= 0.00895324766512916  val_loss= 0.020911938294160465  f1= 0.002386849982928242  f2= 0.000626979275014279  f3= 0.0031437157450190617  f4= 0.002795702662167577 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4190  time= 123.07602453231812  loss= 0.006690225941491189  val_loss= 0.014463291093961345  f1= 0.0025086035811703942  f2= 0.0007879018457300765  f3= 0.0012002249274268143  f4= 0.0021934955871639045 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4200  time= 123.36477398872375  loss= 0.019363066837528847  val_loss= 0.024156139838396408  f1= 0.0031225143667711667  f2= 0.0006806165950678469  f3= 0.006567036749524116  f4= 0.008992899126165717 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4210  time= 123.66393709182739  loss= 0.007160066106859234  val_loss= 0.025473188502645815  f1= 0.000812652279084463  f2= 0.0007773405764901555  f3= 0.0003791819378785481  f4= 0.005190891313406068 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4220  time= 123.95529222488403  loss= 0.01292136663415609  val_loss= 0.02062979718049824  f1= 0.0023624242299326403  f2= 0.0008305586742101892  f3= 0.005040284088402692  f4= 0.004688099641610569 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4230  time= 124.24962186813354  loss= 0.005799639623983811  val_loss= 0.009542086631338943  f1= 0.0009870044326649263  f2= 0.0004166324194356214  f3= 0.0010439107281781676  f4= 0.003352092043705096 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4240  time= 124.52222752571106  loss= 0.01329246818971748  val_loss= 0.013009688496975884  f1= 0.0021255693123747323  f2= 0.0009172775709160842  f3= 0.003037257817229032  f4= 0.007212363489197632 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4250  time= 124.79591822624207  loss= 0.017754484092677433  val_loss= 0.013967272855895841  f1= 0.0026111685217147435  f2= 0.00093365028702691  f3= 0.0011441889874125096  f4= 0.013065476296523265 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4260  time= 125.05889940261841  loss= 0.008015501060301229  val_loss= 0.01451517493958318  f1= 0.00210421516621836  f2= 0.0005097229968691275  f3= 0.0008304105372488718  f4= 0.00457115235996487 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4270  time= 125.33437466621399  loss= 0.02760192904304905  val_loss= 0.07973083625913817  f1= 0.005020728225829742  f2= 0.0007434173397796944  f3= 0.009235306541889492  f4= 0.012602476935550118 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4280  time= 125.63783931732178  loss= 0.011247396101437905  val_loss= 0.019564973722446498  f1= 0.001498531088705017  f2= 0.0004458643797465315  f3= 0.004972984231877587  f4= 0.004330016401108771 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4290  time= 125.93086695671082  loss= 0.012931133887274841  val_loss= 0.023435273596542995  f1= 0.003737780816984239  f2= 0.0004922360771693539  f3= 0.0028396838517992435  f4= 0.005861433141322006 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4300  time= 126.22965955734253  loss= 0.009753036115354346  val_loss= 0.012637139746757411  f1= 0.002932057665481442  f2= 0.0005556261092952208  f3= 0.0008734247850156104  f4= 0.005391927555562072 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4310  time= 126.50459313392639  loss= 0.009900274140604877  val_loss= 0.008809563106720361  f1= 0.0007327949458004714  f2= 0.0012875278639675222  f3= 0.0002496750480652866  f4= 0.007630276282771597 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4320  time= 126.78068470954895  loss= 0.013333063280961649  val_loss= 0.018672641292571436  f1= 0.002365370752710389  f2= 0.0006576940521004038  f3= 0.0015481794261545583  f4= 0.008761819049996301 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4330  time= 127.0672492980957  loss= 0.015192097422091919  val_loss= 0.029509025968442883  f1= 0.0014694638389447538  f2= 0.0007904269716634487  f3= 0.0030621918026397105  f4= 0.009870014808844007 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4340  time= 127.36316180229187  loss= 0.004387764456329518  val_loss= 0.01964504293962898  f1= 0.0019393722695212011  f2= 0.00047291085622837634  f3= 0.00015922364605698287  f4= 0.001816257684522958 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4350  time= 127.68465685844421  loss= 0.013920640660449799  val_loss= 0.018129234322000264  f1= 0.0013130400516037455  f2= 0.0009066621599898183  f3= 0.0010000825798656234  f4= 0.010700855868990613 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4360  time= 127.99369597434998  loss= 0.005481328003315831  val_loss= 0.013606918518264675  f1= 0.0015762628165787846  f2= 0.0003049293482605976  f3= 0.0007224125839170198  f4= 0.002877723254559429 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4370  time= 128.30219507217407  loss= 0.014919868189156518  val_loss= 0.014406380198208082  f1= 0.002131461793674751  f2= 0.0008826939040434684  f3= 0.005798049001821406  f4= 0.006107663489616891 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4380  time= 128.5822606086731  loss= 0.012265114821568652  val_loss= 0.025168591367876872  f1= 0.002029749358400497  f2= 0.0005201075513181846  f3= 0.004683659698734609  f4= 0.005031598213115365 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4390  time= 128.86212253570557  loss= 0.005883058746111921  val_loss= 0.018064876838790073  f1= 0.0011818167038954953  f2= 0.00033554249585010797  f3= 0.0028036204848177998  f4= 0.0015620790615485187 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4400  time= 129.1448531150818  loss= 0.02272172875311625  val_loss= 0.02663838968793407  f1= 0.004526969633527262  f2= 0.0008748177660607269  f3= 0.005977817428184586  f4= 0.011342123925343681 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4410  time= 129.4380009174347  loss= 0.007633734793085149  val_loss= 0.019093511865656443  f1= 0.0022467276851307534  f2= 0.0005552007752452583  f3= 0.002075115473610994  f4= 0.002756690859098144 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4420  time= 129.71478986740112  loss= 0.01035512770459975  val_loss= 0.01655548325122337  f1= 0.0016118003608817759  f2= 0.000602582948839704  f3= 0.0007377278530744714  f4= 0.007403016541803799 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4430  time= 129.9831666946411  loss= 0.009712145972090537  val_loss= 0.029404008509074552  f1= 0.0017123962221007275  f2= 0.0017301360359181443  f3= 0.00026384708778392525  f4= 0.006005766626287739 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4440  time= 130.25033736228943  loss= 0.00492121552626527  val_loss= 0.01622746770227729  f1= 0.0014633400746347366  f2= 0.0004526707774452213  f3= 0.0002206956693401572  f4= 0.0027845090048451535 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4450  time= 130.53285217285156  loss= 0.006705668179179233  val_loss= 0.01174790791022717  f1= 0.0025869645387456047  f2= 0.0005775845970408894  f3= 1.5123597752839892e-05  f4= 0.0035259954456399 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4460  time= 130.84704780578613  loss= 0.016360625926382842  val_loss= 0.01276126893322237  f1= 0.003147641347167328  f2= 0.0008976593229921078  f3= 0.0015389944971895364  f4= 0.010776330759033873 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4470  time= 131.16215682029724  loss= 0.013651804895409797  val_loss= 0.016356375367400762  f1= 0.001738053542696281  f2= 0.0009231839549360793  f3= 0.001868271761548231  f4= 0.009122295636229205 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4480  time= 131.45604252815247  loss= 0.013981074561838253  val_loss= 0.025285621597103923  f1= 0.003929574902929331  f2= 0.0012169939311798116  f3= 0.0005331978626354545  f4= 0.008301307865093657 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4490  time= 131.74023008346558  loss= 0.012467227473011669  val_loss= 0.012458261466304233  f1= 0.003219560489600624  f2= 0.0004741669509421555  f3= 0.0036179791095870216  f4= 0.005155520922881867 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4500  time= 132.02491283416748  loss= 0.009484181415516569  val_loss= 0.01757700910741627  f1= 0.0017129622078710296  f2= 0.00136407419328896  f3= 0.0005084356694613559  f4= 0.005898709344895225 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4510  time= 132.29997539520264  loss= 0.005932148205631711  val_loss= 0.01460822591346796  f1= 0.0011180263226727456  f2= 0.0004206011873114802  f3= 0.00031643514487236394  f4= 0.004077085550775122 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4520  time= 132.606600522995  loss= 0.010777730475636545  val_loss= 0.01712019455775827  f1= 0.0016999788073952316  f2= 0.0009317729123687017  f3= 0.001268753511665179  f4= 0.006877225244207433 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4530  time= 132.94011783599854  loss= 0.02347584634916024  val_loss= 0.01371931496521518  f1= 0.0022638711929669005  f2= 0.00038585805021654034  f3= 0.00032155648752891803  f4= 0.02050456061844788 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4540  time= 133.26765656471252  loss= 0.0062548063913056276  val_loss= 0.016449454318303358  f1= 0.0014061345545734072  f2= 0.000394521156930509  f3= 0.0008051239317776313  f4= 0.0036490267480240794 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4550  time= 133.58723950386047  loss= 0.06943673578221073  val_loss= 0.012896331902297643  f1= 0.0015311350986842902  f2= 0.0029843391612114213  f3= 0.00016849631196513186  f4= 0.06475276521034988 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4560  time= 133.8969624042511  loss= 0.015267048738329217  val_loss= 0.013333129508203055  f1= 0.0018477518467904456  f2= 0.0012927970769044924  f3= 0.00027859753547095377  f4= 0.011847902279163327 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4570  time= 134.21574449539185  loss= 0.011800387260231191  val_loss= 0.012329992846585836  f1= 0.0010140169906998806  f2= 0.0011095855235564347  f3= 0.0006558711159221734  f4= 0.009020913630052701 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4580  time= 134.54107189178467  loss= 0.006801708248660523  val_loss= 0.015204274988114698  f1= 0.001463116322534098  f2= 0.0004364738103115997  f3= 0.00150759243462932  f4= 0.003394525681185505 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4590  time= 134.8744990825653  loss= 0.010468871353181933  val_loss= 0.026972326774294817  f1= 0.0029803711465459883  f2= 0.0004739481920277238  f3= 0.002193754759180123  f4= 0.004820797255428097 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4600  time= 135.20719122886658  loss= 0.005511489545477376  val_loss= 0.01766809213202946  f1= 0.001173326686826935  f2= 0.0003436778494366036  f3= 0.002359079219273199  f4= 0.0016354057899406378 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4610  time= 135.51706075668335  loss= 0.010972956011246883  val_loss= 0.02159206361734168  f1= 0.0013516658576479892  f2= 0.0008428302018618762  f3= 0.0021535532205218225  f4= 0.006624906731215193 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4620  time= 135.79564762115479  loss= 0.01212807687789414  val_loss= 0.016331779661430616  f1= 0.0025859442234841836  f2= 0.00048648575078597656  f3= 0.0034005053956420055  f4= 0.005655141507981976 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4630  time= 136.08675813674927  loss= 0.01485659537446679  val_loss= 0.014136553145174036  f1= 0.003478716670360749  f2= 0.0005963656610960163  f3= 0.007205964622481759  f4= 0.0035755484205282663 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4640  time= 136.35314416885376  loss= 0.0060855762171153  val_loss= 0.008597428517809366  f1= 0.0015829951426887378  f2= 0.000530785220065098  f3= 0.001498501609482687  f4= 0.002473294244878779 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4650  time= 136.62859964370728  loss= 0.011411231197180766  val_loss= 0.016786696921474994  f1= 0.002325912587319524  f2= 0.0009969069928066222  f3= 0.0015359209869857421  f4= 0.00655249063006888 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4660  time= 136.9043242931366  loss= 0.01072261227021751  val_loss= 0.011650135045236279  f1= 0.002375088705274339  f2= 0.0010589527877200148  f3= 0.00036394913297478827  f4= 0.006924621644248366 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4670  time= 137.21843838691711  loss= 0.006377225391561075  val_loss= 0.01240273193251621  f1= 0.0021468348790689494  f2= 0.00027339332337303167  f3= 0.001880740775394201  f4= 0.002076256413724892 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4680  time= 137.4960913658142  loss= 0.03638082948934287  val_loss= 0.023650106453909213  f1= 0.010250596719372921  f2= 0.0005280357679880261  f3= 0.008836929159117157  f4= 0.01676526784286476 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4690  time= 137.77147459983826  loss= 0.02630942033509159  val_loss= 0.044440963014000315  f1= 0.0023850419071459517  f2= 0.0006276448218474062  f3= 0.015080292346424572  f4= 0.00821644125967366 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4700  time= 138.0400836467743  loss= 0.011592010023079516  val_loss= 0.02153771996286464  f1= 0.0010845439965763693  f2= 0.0006795052348983125  f3= 0.0014514162891366526  f4= 0.008376544502468181 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4710  time= 138.3179202079773  loss= 0.01261241858500158  val_loss= 0.01514708681074106  f1= 0.002114212082034454  f2= 0.0004955367486596669  f3= 0.004812648335367656  f4= 0.005190021418939805 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4720  time= 138.60576939582825  loss= 0.008321669982703183  val_loss= 0.01414497643712003  f1= 0.0014270672439897067  f2= 0.0003251976757206449  f3= 0.0006138149473103922  f4= 0.0059555901156824395 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4730  time= 138.8968152999878  loss= 0.003094021544202051  val_loss= 0.012142735178079699  f1= 0.0014340029206605742  f2= 0.000278967065260062  f3= 0.0002668910426603479  f4= 0.0011141605156210668 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4740  time= 139.22305011749268  loss= 0.007605946212803082  val_loss= 0.016947223480599426  f1= 0.0018118861934920021  f2= 0.0005572437301376293  f3= 0.0011218747766962671  f4= 0.0041149415124771835 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4750  time= 139.539653301239  loss= 0.008511403001021814  val_loss= 0.01212181229041653  f1= 0.000862112732733953  f2= 0.0004897972432460761  f3= 0.0029886418218447812  f4= 0.004170851203197004 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4760  time= 139.85267639160156  loss= 0.0028291379348262323  val_loss= 0.009261674747504181  f1= 0.0008487015841155892  f2= 0.0003527312739793485  f3= 0.00014888415249651485  f4= 0.0014788209242347801 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4770  time= 140.18714141845703  loss= 0.00859825939449029  val_loss= 0.012397995311079012  f1= 0.0012052260479680354  f2= 0.0006438755179927029  f3= 0.002492540178787943  f4= 0.0042566176497416085 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4780  time= 140.49997329711914  loss= 0.023351355217980277  val_loss= 0.03452535383054921  f1= 0.004313389932564161  f2= 0.0007762960777529973  f3= 0.004593525954787624  f4= 0.013668143252875494 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4790  time= 140.82702898979187  loss= 0.02666509910629321  val_loss= 0.013245898179744217  f1= 0.004607640670648615  f2= 0.0015495436036062375  f3= 0.0002811568224876227  f4= 0.020226758009550743 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4800  time= 141.12618231773376  loss= 0.004074019687883128  val_loss= 0.013535786762428888  f1= 0.0014330480036864057  f2= 0.0004198355641694799  f3= 0.0005421098305892532  f4= 0.0016790262894379893 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4810  time= 141.4443621635437  loss= 0.006759307466965871  val_loss= 0.0261701762606912  f1= 0.0013806243764291082  f2= 0.0004141504567395959  f3= 0.003464652038899675  f4= 0.0014998805948974927 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4820  time= 141.7519247531891  loss= 0.006787576889504664  val_loss= 0.012801094900959844  f1= 0.002307463355474348  f2= 0.00031036444093720183  f3= 0.0020726310742214555  f4= 0.0020971180188716584 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4830  time= 142.05590629577637  loss= 0.004190557849035029  val_loss= 0.012712869556931643  f1= 0.0013448875061661146  f2= 0.0003165098184073465  f3= 0.00013991441268972963  f4= 0.002389246111771839 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4840  time= 142.37346267700195  loss= 0.009349292629861427  val_loss= 0.017380727864492084  f1= 0.002019046203020888  f2= 0.0005149083328006293  f3= 0.0019621240266891695  f4= 0.004853214067350739 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4850  time= 142.67938446998596  loss= 0.012275661628623258  val_loss= 0.020556329851591254  f1= 0.0029609083181382713  f2= 0.0005554519845038866  f3= 0.0020336854918364656  f4= 0.0067256158341446335 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4860  time= 142.98437356948853  loss= 0.009209513492462436  val_loss= 0.020812013374026574  f1= 0.002768564862941651  f2= 0.0005037256544043674  f3= 0.0022394151915740234  f4= 0.0036978077835423937 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4870  time= 143.34037113189697  loss= 0.019323727271201505  val_loss= 0.0190659195246171  f1= 0.002489926670443746  f2= 0.0010435956975951477  f3= 0.007285110730450431  f4= 0.008505094172712179 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4880  time= 143.69179153442383  loss= 0.05933383436680789  val_loss= 0.022483246530112876  f1= 0.0014929718090089415  f2= 0.004264828947332394  f3= 0.0006904759768996928  f4= 0.052885557633566865 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4890  time= 143.9882218837738  loss= 0.004720076348016809  val_loss= 0.008915876152210329  f1= 0.00151557954995041  f2= 0.000538694129122244  f3= 0.0006157289868122988  f4= 0.002050073682131856 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4900  time= 144.29023003578186  loss= 0.0066619658487709335  val_loss= 0.024980123389410316  f1= 0.0013974833528369371  f2= 0.0007293632876045976  f3= 0.00026648801641353893  f4= 0.00426863119191586 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4910  time= 144.59086632728577  loss= 0.004632120201176712  val_loss= 0.012898222780177323  f1= 0.0009571198218261639  f2= 0.0005440089789534576  f3= 0.0009544183674656615  f4= 0.002176573032931429 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4920  time= 144.88105702400208  loss= 0.007023450671783386  val_loss= 0.021207350220698862  f1= 0.0011421940485211919  f2= 0.00037364312912858993  f3= 0.0018244218291368665  f4= 0.0036831916649967384 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4930  time= 145.1860945224762  loss= 0.00814965616181113  val_loss= 0.013470241494022223  f1= 0.0007611000101383447  f2= 0.00030844352123170297  f3= 0.0015140745236973457  f4= 0.005566038106743738 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4940  time= 145.49582195281982  loss= 0.007199748616865243  val_loss= 0.016328192485528286  f1= 0.0013521258602909453  f2= 0.0004907031263560993  f3= 0.00042866506014058923  f4= 0.004928254570077609 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4950  time= 145.7860085964203  loss= 0.021931767724825274  val_loss= 0.03160940102264416  f1= 0.0035654174951327846  f2= 0.0006695470766358622  f3= 0.008557963388930815  f4= 0.009138839764125812 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4960  time= 146.08109712600708  loss= 0.008117567132559218  val_loss= 0.01330371766500819  f1= 0.003424945221187746  f2= 0.00040409487053648347  f3= 0.0004937971548465003  f4= 0.003794729885988488 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4970  time= 146.3634214401245  loss= 0.0025861292876726495  val_loss= 0.012269562822306113  f1= 0.0008479739586264318  f2= 0.00027859891668130575  f3= 0.0005444961710595885  f4= 0.0009150602413053239 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4980  time= 146.66108512878418  loss= 0.0071916850841810065  val_loss= 0.013824926816697022  f1= 0.0017396478363445676  f2= 0.0003556528698248976  f3= 0.0016925189351806261  f4= 0.0034038654428309163 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 4990  time= 146.96156406402588  loss= 0.00765283119461458  val_loss= 0.02687638429086961  f1= 0.0032255032476648155  f2= 0.00032281773479896184  f3= 0.0006355081867674329  f4= 0.00346900202538337 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 5000  time= 147.29084491729736  loss= 0.01575599382135268  val_loss= 0.016779116778712178  f1= 0.0029423410957718737  f2= 0.0011947648923418739  f3= 0.0002886817141915889  f4= 0.011330206119047344 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 5010  time= 147.62111687660217  loss= 0.006502265353999379  val_loss= 0.017815662930362474  f1= 0.0017956051094598945  f2= 0.00038230385835004915  f3= 0.0016970021894677004  f4= 0.002627354196721736 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 5020  time= 147.92364382743835  loss= 0.00526832880159614  val_loss= 0.011711787941704046  f1= 0.0011195311930756238  f2= 0.000476988603414726  f3= 0.000945103768999012  f4= 0.002726705236106779 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 5030  time= 148.2325508594513  loss= 0.004773249738758521  val_loss= 0.01554739835830449  f1= 0.00144885475547873  f2= 0.0007029023833058761  f3= 0.0011236616112131382  f4= 0.0014978309887607774 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 5040  time= 148.55256366729736  loss= 0.011535035918886544  val_loss= 0.008791472100832285  f1= 0.001077463056321058  f2= 0.000533185011331712  f3= 0.002688726907259395  f4= 0.007235660943974382 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 5050  time= 148.8801555633545  loss= 0.01381825123982614  val_loss= 0.018263297958560284  f1= 0.0032223344875097807  f2= 0.0008098418692215409  f3= 0.001098659156560621  f4= 0.0086874157265342 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 5060  time= 149.21407079696655  loss= 0.009362785832489116  val_loss= 0.014853384877903043  f1= 0.0013001740454632335  f2= 0.0007490113808161645  f3= 0.0021427840702698607  f4= 0.005170816335939857 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 5070  time= 149.54082894325256  loss= 0.0039982956056458996  val_loss= 0.007327731283281218  f1= 0.0007583297473374418  f2= 0.0004457844596614824  f3= 0.00013619432134295286  f4= 0.0026579870773040227 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 5080  time= 149.86224508285522  loss= 0.007829448540885069  val_loss= 0.02163581319185781  f1= 0.0011948352170609588  f2= 0.000406527785815001  f3= 0.002943201452566071  f4= 0.0032848840854430386 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 5090  time= 150.14103746414185  loss= 0.005060717137127948  val_loss= 0.016221065679798932  f1= 0.0011287203193839372  f2= 0.0002934460625044093  f3= 0.0009166918776290908  f4= 0.0027218588776105096 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 5100  time= 150.41532397270203  loss= 0.008783516837722875  val_loss= 0.018903854156763024  f1= 0.002134591724059729  f2= 0.0004435208159848522  f3= 0.002878158052851052  f4= 0.003327246244827241 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 5110  time= 150.7007122039795  loss= 0.003871358371925778  val_loss= 0.011696656069192637  f1= 0.0011853171551165216  f2= 0.0003180662432963401  f3= 0.0005297508561916035  f4= 0.0018382241173213129 num_batches= 6 percent lr= 0.01\n","EarlyStopping counter: 996 out of 1000\n","EarlyStopping counter: 997 out of 1000\n","EarlyStopping counter: 998 out of 1000\n","EarlyStopping counter: 999 out of 1000\n","EarlyStopping counter: 1000 out of 1000\n","Early Stopping\n","training PINN Net\n"," \n","epoch= 0  time= 0.024169206619262695  loss= 5.735643406855562  val_loss= 11.031358499918646  f1= 5.275076875720533  f2= 0.3415642107776036  f3= 0.11502342538402577  f4= 0.003978894973400939 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 10  time= 0.24584698677062988  loss= 1.7507353219152646  val_loss= 1.9448592346785114  f1= 0.5866957146098024  f2= 0.3354891448018226  f3= 0.03396953515213324  f4= 0.7945809273515062 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 20  time= 0.4391019344329834  loss= 1.479523966202615  val_loss= 1.5109654478305823  f1= 0.41265550956829183  f2= 0.3220211770194912  f3= 0.024208852845867383  f4= 0.7206384267689646 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 30  time= 0.635472297668457  loss= 1.3448643412559622  val_loss= 1.3489292076827177  f1= 0.35986387271723325  f2= 0.3165069822847812  f3= 0.008567614410976336  f4= 0.6599258718429714 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 40  time= 0.8805806636810303  loss= 1.3222607455438302  val_loss= 1.6667479038639819  f1= 0.41708775216830896  f2= 0.3108809698584977  f3= 0.009928130050178207  f4= 0.5843638934668453 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 50  time= 1.0718603134155273  loss= 1.3154745781519332  val_loss= 1.9364186648072046  f1= 0.314989510165775  f2= 0.3356028360561821  f3= 0.025501173686308345  f4= 0.6393810582436676 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 60  time= 1.289421558380127  loss= 0.623930826770727  val_loss= 1.2385211681601949  f1= 0.17278798008579602  f2= 0.20275455887931973  f3= 0.0007504998069641022  f4= 0.2476377879986471 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 70  time= 1.5085322856903076  loss= 0.4232519098503429  val_loss= 1.1528098775852265  f1= 0.12306973231664758  f2= 0.14575400200212352  f3= 0.020396397654516323  f4= 0.13403177787705553 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 80  time= 1.7295563220977783  loss= 0.3482211275302402  val_loss= 0.9783389772877558  f1= 0.10127457519744987  f2= 0.12070476438642701  f3= 0.00502733944267227  f4= 0.12121444850369105 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 90  time= 1.9395852088928223  loss= 0.3771344603860784  val_loss= 1.008887529685957  f1= 0.08543632918125703  f2= 0.11291312871810903  f3= 0.01780369105129961  f4= 0.16098131143541272 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 100  time= 2.1420481204986572  loss= 0.5988268997082743  val_loss= 0.8555644665011242  f1= 0.08733778245914124  f2= 0.07129342658276344  f3= 0.3347210555046615  f4= 0.10547463516170803 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 110  time= 2.3464834690093994  loss= 0.21577742480584475  val_loss= 0.9641434735469592  f1= 0.07117228400342115  f2= 0.0761074640195782  f3= 0.004340866432980388  f4= 0.06415681034986497 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 120  time= 2.5507194995880127  loss= 0.2823955280855907  val_loss= 0.9600722732140088  f1= 0.06283711372328991  f2= 0.06947019359341437  f3= 0.052753880135533913  f4= 0.0973343406333525 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 130  time= 2.783229351043701  loss= 0.9621845131524301  val_loss= 0.7500633103620107  f1= 0.2075843365847246  f2= 0.2396748268371304  f3= 0.007792439037634918  f4= 0.5071329106929401 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 140  time= 2.9774668216705322  loss= 0.1860192420645653  val_loss= 0.5659774682567815  f1= 0.04745879224088926  f2= 0.05037592818847001  f3= 0.027106425597990286  f4= 0.0610780960372157 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 150  time= 3.189162015914917  loss= 0.22937220043297726  val_loss= 0.9570042781843436  f1= 0.03947367358515661  f2= 0.06549114251475972  f3= 0.008676795937478983  f4= 0.11573058839558198 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 160  time= 3.3820364475250244  loss= 0.2043525965005316  val_loss= 0.3428756617824801  f1= 0.04433717466766871  f2= 0.03798878653979324  f3= 0.04990248215335566  f4= 0.07212415313971403 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 170  time= 3.6192679405212402  loss= 0.15476771173229342  val_loss= 0.4544622569752457  f1= 0.035587561309398205  f2= 0.05068611596110556  f3= 0.003576569242900743  f4= 0.06491746521888891 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 180  time= 3.8216326236724854  loss= 0.45650230605232345  val_loss= 0.7053813081199529  f1= 0.08328145321222127  f2= 0.14400988934425057  f3= 0.0873478610313496  f4= 0.14186310246450196 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 190  time= 4.01900053024292  loss= 0.18036039123091427  val_loss= 0.21508177441256351  f1= 0.04048397625429899  f2= 0.03957171746677525  f3= 0.005110705462396203  f4= 0.09519399204744382 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 200  time= 4.230473756790161  loss= 0.18482117909371834  val_loss= 0.39959954390248537  f1= 0.023915004504357282  f2= 0.03573561836284381  f3= 0.08284722619542197  f4= 0.04232333003109532 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 210  time= 4.4050514698028564  loss= 0.13552184970463402  val_loss= 0.23297804410326112  f1= 0.023918303513350805  f2= 0.03322491398930406  f3= 0.029678241424545424  f4= 0.04870039077743373 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 220  time= 4.61635160446167  loss= 0.16278592954112117  val_loss= 0.32416106000838274  f1= 0.027764085125070493  f2= 0.03722202840485754  f3= 0.013518348063989974  f4= 0.08428146794720316 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 230  time= 4.802302122116089  loss= 0.1715274189291399  val_loss= 0.25061928981257153  f1= 0.02023150920638701  f2= 0.04152916164938917  f3= 0.05132359968294727  f4= 0.0584431483904164 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 240  time= 5.0109312534332275  loss= 0.13162367558894564  val_loss= 0.2578556687382112  f1= 0.020473027652529684  f2= 0.02491418731711621  f3= 0.01781568202973664  f4= 0.06842077858956312 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 250  time= 5.23185920715332  loss= 0.12638422220411716  val_loss= 0.2592260376812432  f1= 0.019093545460452922  f2= 0.02306801431817761  f3= 0.023877192088869417  f4= 0.06034547033661719 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 260  time= 5.444331407546997  loss= 0.19073160366222486  val_loss= 0.23891759601801665  f1= 0.042434983979238367  f2= 0.03459069536296982  f3= 0.0084703667893187  f4= 0.10523555753069797 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 270  time= 5.642913579940796  loss= 0.10593721570202731  val_loss= 0.32085533060510496  f1= 0.01937141548646515  f2= 0.026475178447211742  f3= 0.004765497676945129  f4= 0.055325124091405296 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 280  time= 5.869990825653076  loss= 0.13494843876733645  val_loss= 0.18061202014476418  f1= 0.021010658377011115  f2= 0.0218962271886923  f3= 0.015397420409473698  f4= 0.07664413279215934 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 290  time= 6.084251165390015  loss= 0.11677355957576623  val_loss= 0.1192830762594066  f1= 0.018081680047535142  f2= 0.0194816129580636  f3= 0.010054977802531288  f4= 0.06915528876763617 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 300  time= 6.304792642593384  loss= 0.11240581187631377  val_loss= 0.13198429227890546  f1= 0.018072397231696583  f2= 0.0169239487049085  f3= 0.003609497351069732  f4= 0.07379996858863895 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 310  time= 6.4920079708099365  loss= 0.16007721534775263  val_loss= 0.24691233897202033  f1= 0.018024076025697017  f2= 0.022676472934020777  f3= 0.03945001128055236  f4= 0.07992665510748248 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 320  time= 6.704833507537842  loss= 0.06978559138783198  val_loss= 0.09478553509088036  f1= 0.018152208254509968  f2= 0.017757636462618714  f3= 0.0027776264237046964  f4= 0.0310981202469986 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 330  time= 6.938420534133911  loss= 0.05926024609365287  val_loss= 0.06983335073071073  f1= 0.012381603967824117  f2= 0.009165666490317376  f3= 0.007373360308518287  f4= 0.030339615326993096 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 340  time= 7.155311107635498  loss= 0.06266815855173043  val_loss= 0.08196560471718484  f1= 0.011741637660521505  f2= 0.013140566706607735  f3= 0.005531117878677871  f4= 0.03225483630592331 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 350  time= 7.355082273483276  loss= 0.07290502787680692  val_loss= 0.05690370722565728  f1= 0.012481040290772495  f2= 0.010239419929735557  f3= 0.011232648666601041  f4= 0.03895191898969781 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 360  time= 7.557842969894409  loss= 0.08382795398712523  val_loss= 0.08863218911504192  f1= 0.01638381530977855  f2= 0.011211966062498198  f3= 0.008862125070055828  f4= 0.04737004754479266 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 370  time= 7.748564720153809  loss= 0.05849916271948736  val_loss= 0.13851542516816298  f1= 0.00967151661922676  f2= 0.013347461326783758  f3= 0.014709909482566461  f4= 0.02077027529091037 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 380  time= 7.945122003555298  loss= 0.07691129967914549  val_loss= 0.09552991078610376  f1= 0.008727796146795272  f2= 0.012017280155525433  f3= 0.016287677711245205  f4= 0.03987854566557957 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 390  time= 8.15268349647522  loss= 0.07434359712781051  val_loss= 0.11481632069934691  f1= 0.0098364734072096  f2= 0.01599411587156145  f3= 0.00622954951917207  f4= 0.042283458329867386 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 400  time= 8.354458808898926  loss= 0.06767095993153667  val_loss= 0.09992831258362961  f1= 0.011079032060518433  f2= 0.009983632201481152  f3= 0.003981186963254138  f4= 0.04262710870628295 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 410  time= 8.554420232772827  loss= 0.10852830920681855  val_loss= 0.22800877825725643  f1= 0.01344553686313575  f2= 0.014688697604408394  f3= 0.03334057778461725  f4= 0.047053496954657166 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 420  time= 8.784992456436157  loss= 0.10575840515532635  val_loss= 0.17969251785208662  f1= 0.017922142184410888  f2= 0.01222333965594753  f3= 0.04875898747890139  f4= 0.02685393583606654 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 430  time= 8.9845449924469  loss= 0.0738596227562858  val_loss= 0.1694124523184684  f1= 0.008288601774050953  f2= 0.00782748478464124  f3= 0.016104575510978283  f4= 0.04163896068661533 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 440  time= 9.192315101623535  loss= 0.038317072952541  val_loss= 0.07319492918911749  f1= 0.012381716385198487  f2= 0.007875013088623472  f3= 0.0018898067795485444  f4= 0.016170536699170488 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 450  time= 9.410987615585327  loss= 0.08212065876956033  val_loss= 0.10056408219616575  f1= 0.017063371962456626  f2= 0.010212574491453047  f3= 0.0056489471748444485  f4= 0.0491957651408062 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 460  time= 9.618840217590332  loss= 0.1007304662279447  val_loss= 0.10918456204003267  f1= 0.021674908657877943  f2= 0.016796215802064605  f3= 0.005675409523551836  f4= 0.05658393224445032 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 470  time= 9.821036338806152  loss= 0.14175692079453267  val_loss= 0.17085213709601402  f1= 0.013679009672968786  f2= 0.013200650199823097  f3= 0.004417619085842236  f4= 0.11045964183589857 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 480  time= 10.032402038574219  loss= 0.04649980759098482  val_loss= 0.09036872280518306  f1= 0.01139483822606059  f2= 0.006199685012679811  f3= 0.013441775376515641  f4= 0.015463508975728782 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 490  time= 10.229682922363281  loss= 0.38363446354928166  val_loss= 0.1422526065147087  f1= 0.044193253849399594  f2= 0.016750079899496614  f3= 0.023580477522584655  f4= 0.2991106522778008 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 500  time= 10.462064743041992  loss= 0.3032104117106814  val_loss= 0.17264122088535525  f1= 0.025117773081658912  f2= 0.024417685870303924  f3= 0.008902127996650872  f4= 0.24477282476206771 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 510  time= 10.660295009613037  loss= 0.14047714208231651  val_loss= 0.21128124242565954  f1= 0.007180665923834351  f2= 0.010587095550464746  f3= 0.0760410342750427  f4= 0.04666834633297471 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 520  time= 10.885780572891235  loss= 0.10842496400317249  val_loss= 0.10444169626725049  f1= 0.007360057920224128  f2= 0.01078957136459261  f3= 0.01761333953040942  f4= 0.07266199518794635 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 530  time= 11.079671382904053  loss= 0.09522163932143195  val_loss= 0.1220316470636098  f1= 0.012300532671056392  f2= 0.011607392821009145  f3= 0.03870502974411509  f4= 0.03260868408525131 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 540  time= 11.296336650848389  loss= 0.05117403129286463  val_loss= 0.09556125703997925  f1= 0.016226498924029772  f2= 0.008020725159936886  f3= 0.004268035264642559  f4= 0.022658771944255408 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 550  time= 11.485020875930786  loss= 0.07260752303392624  val_loss= 0.11615397833134514  f1= 0.013168836075918627  f2= 0.012587460665881555  f3= 0.0010034109755458801  f4= 0.045847815316580186 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 560  time= 11.698632001876831  loss= 0.08531010412795055  val_loss= 0.07369821208068641  f1= 0.00847632338798647  f2= 0.009746742146344153  f3= 0.0038635878165101454  f4= 0.0632234507771098 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 570  time= 11.924318790435791  loss= 0.1467317930709563  val_loss= 0.20790314716010677  f1= 0.026216631367985053  f2= 0.018106235908200857  f3= 0.03405704960930153  f4= 0.06835187618546887 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 580  time= 12.14998197555542  loss= 0.06000672691092223  val_loss= 0.05645865886193848  f1= 0.006730493917541834  f2= 0.011712242802778496  f3= 0.005196934607711597  f4= 0.036367055582890294 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 590  time= 12.351078510284424  loss= 0.170857153166751  val_loss= 0.12141375704229096  f1= 0.018528937442181592  f2= 0.04181266910653666  f3= 0.010969103617155784  f4= 0.09954644300087696 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 600  time= 12.549729585647583  loss= 0.07712568578367411  val_loss= 0.06829741013020477  f1= 0.009466598977526568  f2= 0.007405237753512927  f3= 0.002722341918481718  f4= 0.0575315071341529 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 610  time= 12.769880771636963  loss= 0.31506130243458963  val_loss= 0.1284140128136733  f1= 0.025517058764271915  f2= 0.04397168158220686  f3= 0.008906637148339707  f4= 0.2366659249397712 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 620  time= 12.966423511505127  loss= 0.028376515040719457  val_loss= 0.05575218909055209  f1= 0.005615852997731108  f2= 0.004666276909070842  f3= 0.0026778511406119637  f4= 0.015416533993305545 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 630  time= 13.18254804611206  loss= 0.04647293111990836  val_loss= 0.040609175378903614  f1= 0.007832465615684776  f2= 0.003963996567712171  f3= 0.005031237068461353  f4= 0.02964523186805006 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 640  time= 13.367371559143066  loss= 0.07074337579463055  val_loss= 0.06560382637807849  f1= 0.01629275573368556  f2= 0.006164496912237741  f3= 0.023513402269095287  f4= 0.024772720879611975 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 650  time= 13.588279485702515  loss= 0.11883990148499568  val_loss= 0.14035301126429806  f1= 0.033969481632096725  f2= 0.012060914931170652  f3= 0.005691822909387168  f4= 0.06711768201234113 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 660  time= 13.780161142349243  loss= 0.09938357359980265  val_loss= 0.11302482809813214  f1= 0.015006405545020645  f2= 0.012779244261523007  f3= 0.0012718926730979085  f4= 0.0703260311201611 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 670  time= 13.986788034439087  loss= 0.03722017179344953  val_loss= 0.045301908654149944  f1= 0.008523321965875878  f2= 0.003624344478774407  f3= 0.001696304813436196  f4= 0.023376200535363056 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 680  time= 14.169014692306519  loss= 0.10140771240794623  val_loss= 0.05370810952061228  f1= 0.024029534165866093  f2= 0.007127009243978755  f3= 0.006011933208282889  f4= 0.0642392357898185 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 690  time= 14.417778730392456  loss= 0.062389771879351824  val_loss= 0.10134161465708552  f1= 0.021720245467530767  f2= 0.00685016033691999  f3= 0.002181775720215623  f4= 0.03163759035468544 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 700  time= 14.599530220031738  loss= 0.08761563956801315  val_loss= 0.07958100634140591  f1= 0.02433541802674143  f2= 0.008469007860085613  f3= 0.006083134112053104  f4= 0.048728079569133005 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 710  time= 14.804290771484375  loss= 0.054491088815038606  val_loss= 0.09574748872375494  f1= 0.010261788797202496  f2= 0.005995904236386906  f3= 0.01649681882416984  f4= 0.021736576957279357 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 720  time= 15.004754781723022  loss= 0.02403927514385847  val_loss= 0.056628054501662396  f1= 0.009140609411880274  f2= 0.0035236951215835612  f3= 0.00039058039940703844  f4= 0.010984390210987599 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 730  time= 15.213453769683838  loss= 0.04526126873073452  val_loss= 0.08609228319867492  f1= 0.008166721464031322  f2= 0.0061687238343790425  f3= 0.013403900287671848  f4= 0.017521923144652306 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 740  time= 15.404768228530884  loss= 0.07539110893758136  val_loss= 0.10905464805674897  f1= 0.020352816284080075  f2= 0.004684524309692889  f3= 0.011485406691921494  f4= 0.03886836165188689 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 750  time= 15.598782300949097  loss= 0.05714521163353178  val_loss= 0.08901550551748157  f1= 0.014666944491142331  f2= 0.0032373547065015352  f3= 0.0008170019009212977  f4= 0.03842391053496662 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 760  time= 15.786067962646484  loss= 0.02502594726225044  val_loss= 0.0538625812854357  f1= 0.009014296738487217  f2= 0.0040163777600353305  f3= 0.00017470871660648304  f4= 0.01182056404712141 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 770  time= 15.980153560638428  loss= 0.07546581974442264  val_loss= 0.11249895076817504  f1= 0.01891410607342375  f2= 0.0070656532170004565  f3= 0.014292182304387344  f4= 0.035193878149611064 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 780  time= 16.17055058479309  loss= 0.0478221066780507  val_loss= 0.061513212118826655  f1= 0.005620548693966518  f2= 0.004759607121334211  f3= 0.0020998132239150318  f4= 0.03534213763883494 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 790  time= 16.39488172531128  loss= 0.1866077166609863  val_loss= 0.08361957057378905  f1= 0.012898642470303472  f2= 0.02002134429198854  f3= 0.003757653284958538  f4= 0.14993007661373578 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 800  time= 16.59566307067871  loss= 0.02739027603141779  val_loss= 0.048119743043265356  f1= 0.007877362274418492  f2= 0.003263152928981134  f3= 0.001391884441998584  f4= 0.014857876386019575 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 810  time= 16.79260540008545  loss= 0.07279184697794548  val_loss= 0.05501579728788898  f1= 0.006798267930003047  f2= 0.008440489630464919  f3= 0.0010308492120707292  f4= 0.0565222402054068 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 820  time= 16.974191665649414  loss= 0.052434737099301194  val_loss= 0.09139266977568847  f1= 0.011828921760283698  f2= 0.005515397175965346  f3= 0.005436887541380536  f4= 0.02965353062167163 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 830  time= 17.184263229370117  loss= 0.07085546118859332  val_loss= 0.06716795085808974  f1= 0.01775263832995254  f2= 0.004985673022029752  f3= 0.0005483387225667605  f4= 0.04756881111404426 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 840  time= 17.394394159317017  loss= 0.05520975541537366  val_loss= 0.07295884449395608  f1= 0.011672843818473255  f2= 0.00809567611980916  f3= 0.004079035790567824  f4= 0.03136219968652342 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 850  time= 17.596081733703613  loss= 0.026566128131376748  val_loss= 0.053656685325462264  f1= 0.007287585611822234  f2= 0.0027625191160535398  f3= 0.008020695524640008  f4= 0.008495327878860969 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 860  time= 17.80119037628174  loss= 0.032444547950278145  val_loss= 0.048691646962317106  f1= 0.0057827457690627785  f2= 0.00369776035976962  f3= 0.0013939555501102747  f4= 0.021570086271335474 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 870  time= 18.019001722335815  loss= 0.06298701144100075  val_loss= 0.10121056674952067  f1= 0.02569109401109693  f2= 0.0041909058588207124  f3= 0.0006240270376145688  f4= 0.03248098453346853 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 880  time= 18.277465105056763  loss= 0.018626377154934735  val_loss= 0.040846750925306596  f1= 0.006679234810412542  f2= 0.002173079951913366  f3= 0.0004365342041127925  f4= 0.009337528188496038 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 890  time= 18.471800327301025  loss= 0.07537088910306457  val_loss= 0.10532001830672226  f1= 0.0228728547379994  f2= 0.006715846444841543  f3= 0.004847802880860956  f4= 0.040934385039362677 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 900  time= 18.663694620132446  loss= 0.03177703897421638  val_loss= 0.048518518204437186  f1= 0.00994290723878036  f2= 0.002834305354454778  f3= 0.002298532357123387  f4= 0.016701294023857862 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 910  time= 18.849901914596558  loss= 0.043939400631628496  val_loss= 0.06790999376312708  f1= 0.0059997142239959665  f2= 0.00427407745015542  f3= 0.000801396924168593  f4= 0.032864212033308506 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 920  time= 19.044636249542236  loss= 0.038888630502632505  val_loss= 0.054030946296485624  f1= 0.009114624138308875  f2= 0.003165152330959719  f3= 0.012577087923047348  f4= 0.014031766110316564 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 930  time= 19.23922324180603  loss= 0.06214000118350685  val_loss= 0.078628064135851  f1= 0.01815462158337453  f2= 0.0027996153109662906  f3= 0.009852454057867556  f4= 0.03133331023129846 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 940  time= 19.442964792251587  loss= 0.022300581979474193  val_loss= 0.03305284434608673  f1= 0.006937748864102776  f2= 0.002659025403568389  f3= 0.00048339314983174715  f4= 0.012220414561971283 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 950  time= 19.65805697441101  loss= 0.030448012922576914  val_loss= 0.08173029796761427  f1= 0.00834927885290069  f2= 0.002443663553119584  f3= 0.0062687138809399525  f4= 0.013386356635616688 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 960  time= 19.854047775268555  loss= 0.0854753734475608  val_loss= 0.11332522061883303  f1= 0.016108971695767426  f2= 0.007315246948732521  f3= 0.0055763034586013395  f4= 0.056474851344459516 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 970  time= 20.072781085968018  loss= 0.048349480191127815  val_loss= 0.0645369026086418  f1= 0.008378175294950083  f2= 0.0029772151027777378  f3= 0.006384843523313367  f4= 0.03060924627008663 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 980  time= 20.31720805168152  loss= 0.1491453919485466  val_loss= 0.14419460884493635  f1= 0.012757685376037184  f2= 0.005227289027592354  f3= 0.010752520297032678  f4= 0.12040789724788435 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 990  time= 20.537056922912598  loss= 0.0483134754891709  val_loss= 0.0628405692446021  f1= 0.006130674382356969  f2= 0.003215222518841794  f3= 0.0015284858261510506  f4= 0.03743909276182109 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1000  time= 20.734137773513794  loss= 0.0361293193125316  val_loss= 0.07051610564392183  f1= 0.010159921818676768  f2= 0.003473557120144138  f3= 0.005899521739868442  f4= 0.016596318633842245 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1010  time= 20.945868492126465  loss= 0.10942700855179054  val_loss= 0.10964281104607917  f1= 0.003474572848117963  f2= 0.0111563060445232  f3= 0.007209417170151353  f4= 0.08758671248899802 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1020  time= 21.180219888687134  loss= 0.04539182854851417  val_loss= 0.08661409750199285  f1= 0.009416460471147458  f2= 0.0052319262456445095  f3= 0.004718025497321069  f4= 0.02602541633440113 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1030  time= 21.38152241706848  loss= 0.03463602468349967  val_loss= 0.06689919866838455  f1= 0.006614015506421146  f2= 0.0027344638330373022  f3= 0.004161679586081025  f4= 0.021125865757960192 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1040  time= 21.592170000076294  loss= 0.05726662746049429  val_loss= 0.07364971894174413  f1= 0.006452449184109493  f2= 0.003915154586485907  f3= 0.015419481025224736  f4= 0.031479542664674155 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1050  time= 21.79488444328308  loss= 0.04738370391292571  val_loss= 0.12747676014924236  f1= 0.007103179722026845  f2= 0.0032652128222615544  f3= 0.020399190714817265  f4= 0.01661612065382005 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1060  time= 21.994346857070923  loss= 0.05235500366743031  val_loss= 0.08869376361038044  f1= 0.006534311343679899  f2= 0.0051733607662364555  f3= 0.0003547449614669902  f4= 0.04029258659604696 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1070  time= 22.21018362045288  loss= 0.029611437034319982  val_loss= 0.058560941181088774  f1= 0.0071304464637564425  f2= 0.002053884281330736  f3= 0.00010126530211751671  f4= 0.020325840987115287 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1080  time= 22.492942094802856  loss= 0.08035144575902535  val_loss= 0.10151638284437829  f1= 0.017574283402104665  f2= 0.006671816447228743  f3= 0.0009927761103367201  f4= 0.05511256979935523 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1090  time= 22.857901096343994  loss= 0.06314054139298739  val_loss= 0.06375055252860366  f1= 0.009559671124248843  f2= 0.0038896807322025503  f3= 0.00025831123610684344  f4= 0.049432878300429146 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1100  time= 23.083319187164307  loss= 0.06931665727322692  val_loss= 0.10396896068615118  f1= 0.03438293072568274  f2= 0.002109527030434261  f3= 0.0027878408804321783  f4= 0.030036358636677724 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1110  time= 23.329845666885376  loss= 0.04062115677257863  val_loss= 0.06095226179489527  f1= 0.005129537048176089  f2= 0.003292115546231929  f3= 0.0016550598960133835  f4= 0.03054444428215723 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1120  time= 23.546486616134644  loss= 0.03025579781510081  val_loss= 0.06342057632063941  f1= 0.0082270068015426  f2= 0.003379648343279086  f3= 0.001972454064740601  f4= 0.016676688605538528 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1130  time= 23.74447989463806  loss= 0.05262895145029053  val_loss= 0.05495410065397277  f1= 0.005199480484533106  f2= 0.0031023460356143003  f3= 0.004617726322555446  f4= 0.03970939860758768 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1140  time= 23.966004848480225  loss= 0.05466152075402889  val_loss= 0.08716451334553017  f1= 0.02175459273040832  f2= 0.0019847503063802987  f3= 0.0033445301399744336  f4= 0.02757764757726583 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1150  time= 24.1702938079834  loss= 0.05516691843918107  val_loss= 0.06506433091819488  f1= 0.008828469020489497  f2= 0.005183643548454525  f3= 0.001776648127665682  f4= 0.03937815774257136 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1160  time= 24.375911712646484  loss= 0.0222804954584909  val_loss= 0.04215551280882149  f1= 0.007940691246574991  f2= 0.002293691410282087  f3= 0.000691568630014261  f4= 0.011354544171619562 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1170  time= 24.571871757507324  loss= 0.024186334950794308  val_loss= 0.06861871761911292  f1= 0.007168936746305302  f2= 0.0016155307105776093  f3= 0.002230801625674199  f4= 0.013171065868237195 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1180  time= 24.791038751602173  loss= 0.041808462805254965  val_loss= 0.04538482502658357  f1= 0.00805057648196454  f2= 0.00227831681984512  f3= 0.0013722456586585871  f4= 0.03010732384478672 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1190  time= 25.006799459457397  loss= 0.05699136938701074  val_loss= 0.09511598019704182  f1= 0.005687572257995231  f2= 0.0048490879633422725  f3= 0.0025338934191348806  f4= 0.04392081574653837 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1200  time= 25.26977825164795  loss= 0.015022232128523176  val_loss= 0.05353138467400491  f1= 0.007322317965014633  f2= 0.001784366354531274  f3= 0.0004983269811323875  f4= 0.005417220827844882 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1210  time= 25.470393657684326  loss= 0.013247093287627047  val_loss= 0.0308775025267938  f1= 0.003058533655824005  f2= 0.001146415026977167  f3= 0.00016728807664539706  f4= 0.008874856528180474 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1220  time= 25.70516014099121  loss= 0.03720904202130332  val_loss= 0.0686770314077035  f1= 0.010521523814979943  f2= 0.0012061247892631727  f3= 0.004515793765907771  f4= 0.020965599651152435 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1230  time= 25.91458773612976  loss= 0.040293644744182594  val_loss= 0.04998584943218106  f1= 0.010304179931895542  f2= 0.00379525113465933  f3= 0.0024143729743101674  f4= 0.023779840703317554 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1240  time= 26.121368408203125  loss= 0.01780138293990376  val_loss= 0.05619580835009538  f1= 0.0052877628984457845  f2= 0.001385583856686162  f3= 0.0017301576754763567  f4= 0.009397878509295458 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1250  time= 26.340761184692383  loss= 0.018260713595291824  val_loss= 0.05618501296814876  f1= 0.0046001691277595515  f2= 0.0011614952620131456  f3= 0.004291037423943358  f4= 0.008208011781575769 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1260  time= 26.53766918182373  loss= 0.01984353472167966  val_loss= 0.04224282372150316  f1= 0.0060487136341414866  f2= 0.0013696519256914163  f3= 0.0032617826868931755  f4= 0.009163386474953584 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1270  time= 26.753295183181763  loss= 0.06362444785510882  val_loss= 0.060196743053274514  f1= 0.00910653090991755  f2= 0.004528612173644318  f3= 0.0008404842717780765  f4= 0.049148820499768854 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1280  time= 26.9533531665802  loss= 0.030586977573909102  val_loss= 0.0714054341998479  f1= 0.009958517349705853  f2= 0.000928572118264338  f3= 0.007424505363418619  f4= 0.012275382742520288 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1290  time= 27.160543203353882  loss= 0.034308962668605995  val_loss= 0.0842955217429259  f1= 0.007826610095630262  f2= 0.004139487388486709  f3= 0.0026439531990927194  f4= 0.019698911985396306 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1300  time= 27.41655707359314  loss= 0.0625297981216482  val_loss= 0.03378060992302683  f1= 0.0059326020900151025  f2= 0.003955717374601179  f3= 0.0001612442146431485  f4= 0.05248023444238877 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1310  time= 27.621796131134033  loss= 0.0445165829014549  val_loss= 0.04856166442059478  f1= 0.007553027822574718  f2= 0.003430096644666241  f3= 0.0007916789070668978  f4= 0.03274177952714705 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1320  time= 27.84197688102722  loss= 0.023259071153253843  val_loss= 0.06530742460446484  f1= 0.005947008732406458  f2= 0.0028539841992382396  f3= 0.0011074087964795714  f4= 0.013350669425129566 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1330  time= 28.072693586349487  loss= 0.018098065548326017  val_loss= 0.07049153125637052  f1= 0.00573139904034541  f2= 0.0019036914347958084  f3= 0.0005141657472076303  f4= 0.009948809325977168 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1340  time= 28.315319061279297  loss= 0.018168944333054816  val_loss= 0.0504622667717543  f1= 0.005948974237319284  f2= 0.0014759997530934628  f3= 0.0015152240267500637  f4= 0.009228746315892005 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1350  time= 28.537396669387817  loss= 0.0468622710908173  val_loss= 0.05800846041034238  f1= 0.00580001163929334  f2= 0.00299646232720501  f3= 0.0019311977067605478  f4= 0.0361345994175584 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1360  time= 28.752788066864014  loss= 0.03174509014638127  val_loss= 0.06276370177753055  f1= 0.006499644093454754  f2= 0.003280664895064997  f3= 0.0024883756284853685  f4= 0.019476405529376146 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1370  time= 28.964754104614258  loss= 0.0658671359068112  val_loss= 0.0507445404407646  f1= 0.011143375043400692  f2= 0.00269495746288304  f3= 0.002143281943823257  f4= 0.04988552145670421 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1380  time= 29.153138875961304  loss= 0.027374450608690235  val_loss= 0.048448504997785684  f1= 0.003414481753372885  f2= 0.0020435628421374753  f3= 0.0005810042332333834  f4= 0.02133540177994649 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1390  time= 29.351025104522705  loss= 0.018699848329971607  val_loss= 0.03049981233374822  f1= 0.003452400341184757  f2= 0.0010237020899104834  f3= 0.001423951052936804  f4= 0.012799794845939563 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1400  time= 29.53286123275757  loss= 0.015262184888768326  val_loss= 0.04875979636038413  f1= 0.0029971717123721386  f2= 0.0014440597064980026  f3= 0.0007128808101101066  f4= 0.01010807265978808 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1410  time= 29.740140914916992  loss= 0.0163818097937552  val_loss= 0.050961254789526775  f1= 0.004735090510528788  f2= 0.001604481613039625  f3= 0.002212334171536791  f4= 0.007829903498649997 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1420  time= 29.961329698562622  loss= 0.015914728631896063  val_loss= 0.049605269481277206  f1= 0.005599224348290438  f2= 0.0020224820983839437  f3= 8.082892047452483e-05  f4= 0.00821219326474716 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1430  time= 30.161851167678833  loss= 0.03313819601693112  val_loss= 0.07141351934968582  f1= 0.012742080723948663  f2= 0.0018007149160438882  f3= 0.0025808007423582845  f4= 0.016014599634580292 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1440  time= 30.392786026000977  loss= 0.020752653197009956  val_loss= 0.057601631935220415  f1= 0.0037231839418603043  f2= 0.0018262513797113963  f3= 0.0011135959041798577  f4= 0.014089621971258397 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1450  time= 30.600762128829956  loss= 0.05247969966457406  val_loss= 0.05963557271237506  f1= 0.013695956740378564  f2= 0.0035010780126417374  f3= 0.0070640506243093105  f4= 0.028218614287244453 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1460  time= 30.82447648048401  loss= 0.03934300703968221  val_loss= 0.06448497472346038  f1= 0.00712504236499623  f2= 0.0034992147972121124  f3= 0.00036844812843782847  f4= 0.028350301749036036 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1470  time= 31.020533084869385  loss= 0.048840672519453086  val_loss= 0.043565453714115604  f1= 0.00917955837537128  f2= 0.00355605553241032  f3= 0.007816455655283787  f4= 0.028288602956387687 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1480  time= 31.226954460144043  loss= 0.02535907381594854  val_loss= 0.0494435773638098  f1= 0.006652317883594348  f2= 0.0019355643207552165  f3= 0.006438910009605291  f4= 0.010332281601993688 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1490  time= 31.454805612564087  loss= 0.017643658392994633  val_loss= 0.06684876296199066  f1= 0.005206548679733498  f2= 0.0027182767366504585  f3= 0.0007575484177726024  f4= 0.008961284558838073 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1500  time= 31.66452169418335  loss= 0.03874162868111612  val_loss= 0.058611180106055316  f1= 0.0027414276433902138  f2= 0.003600741814009665  f3= 0.0014354399774611285  f4= 0.030964019246255112 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1510  time= 31.86955976486206  loss= 0.015083667054526082  val_loss= 0.0370348781354432  f1= 0.003932461374613386  f2= 0.0012060839774107434  f3= 0.0018691386844220017  f4= 0.008075983018079954 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1520  time= 32.06720161437988  loss= 0.019189906066036985  val_loss= 0.0563181418505617  f1= 0.0026489145693564865  f2= 0.0024162884848731823  f3= 0.0010175760954155917  f4= 0.013107126916391723 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1530  time= 32.294589042663574  loss= 0.021036544792533113  val_loss= 0.05065432986195997  f1= 0.005711975769930884  f2= 0.0011435875934316662  f3= 0.00020643560381448578  f4= 0.013974545825356078 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1540  time= 32.54693341255188  loss= 0.033758689236736646  val_loss= 0.08406638437708758  f1= 0.0056027585569666015  f2= 0.004696081382468849  f3= 0.0038449271724004717  f4= 0.01961492212490072 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1550  time= 32.75828671455383  loss= 0.034333179018850406  val_loss= 0.10867742399276824  f1= 0.0038080158201894555  f2= 0.00391590150239838  f3= 0.008927085739970626  f4= 0.017682175956291946 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1560  time= 32.95297956466675  loss= 0.020094469297314285  val_loss= 0.060515150040765124  f1= 0.004483748336868209  f2= 0.001922164300764798  f3= 0.0015238370607699614  f4= 0.012164719598911316 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1570  time= 33.163447856903076  loss= 0.01742744867259918  val_loss= 0.05655898809606551  f1= 0.003801668250119531  f2= 0.0014493379441320386  f3= 0.0007375649943848922  f4= 0.011438877483962717 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1580  time= 33.355735063552856  loss= 0.024364976909714595  val_loss= 0.060903057298421256  f1= 0.007696093085086477  f2= 0.0015292417887682308  f3= 0.0014405174429034417  f4= 0.013699124592956446 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1590  time= 33.56380271911621  loss= 0.014214505062133084  val_loss= 0.0480870413626745  f1= 0.0043677752220566066  f2= 0.0015800019454515818  f3= 0.0012313948656672428  f4= 0.00703533302895765 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1600  time= 33.76057577133179  loss= 0.024144335041715394  val_loss= 0.05305563153451241  f1= 0.003409309534586726  f2= 0.002050204735471277  f3= 0.001154654711149229  f4= 0.017530166060508164 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1610  time= 33.97915554046631  loss= 0.03792163366910129  val_loss= 0.04925206723309244  f1= 0.006099444568264716  f2= 0.001276263786736725  f3= 0.0003176854589778599  f4= 0.030228239855121988 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1620  time= 34.20084071159363  loss= 0.02761961010835411  val_loss= 0.05398059353255112  f1= 0.006402094360498258  f2= 0.0025491490840352693  f3= 0.0004569299515068739  f4= 0.018211436712313706 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1630  time= 34.429776668548584  loss= 0.03430870910963408  val_loss= 0.06427532805344606  f1= 0.0060933449809992745  f2= 0.0026039844817469775  f3= 0.003977227831983154  f4= 0.021634151814904672 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1640  time= 34.64895725250244  loss= 0.023416067090416864  val_loss= 0.04752557831612831  f1= 0.0037043672809907737  f2= 0.00222949840224601  f3= 0.0005501713354050679  f4= 0.016932030071775014 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1650  time= 34.85331964492798  loss= 0.054043511650060126  val_loss= 0.08012933071156297  f1= 0.018432406228381607  f2= 0.0037097077498094017  f3= 0.0072328740747680175  f4= 0.024668523597101092 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1660  time= 35.037041664123535  loss= 0.015135460715240833  val_loss= 0.0581778854942063  f1= 0.005601410747501157  f2= 0.0014232656378699382  f3= 0.00047106675932593635  f4= 0.0076397175705438004 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1670  time= 35.232019901275635  loss= 0.03970686177998576  val_loss= 0.0585761321432757  f1= 0.010144954537645719  f2= 0.002974960388983361  f3= 0.0017722577468025915  f4= 0.02481468910655409 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1680  time= 35.422054052352905  loss= 0.015787913743063018  val_loss= 0.06680266773426541  f1= 0.004031442033045848  f2= 0.0011694615142835846  f3= 0.0007002040892195976  f4= 0.009886806106513985 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1690  time= 35.619340658187866  loss= 0.01512814127871031  val_loss= 0.051090185408815214  f1= 0.0024494622763348256  f2= 0.0016996259173881183  f3= 0.00014914043194339728  f4= 0.010829912653043968 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1700  time= 35.80134081840515  loss= 0.02731142589820731  val_loss= 0.05152691192524717  f1= 0.0030585228104383854  f2= 0.0011081181571772007  f3= 0.0114189112500041  f4= 0.011725873680587627 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1710  time= 36.02476119995117  loss= 0.038155347182486095  val_loss= 0.08618417088226468  f1= 0.0108114145879984  f2= 0.0015461255105371542  f3= 0.004020472869069824  f4= 0.021777334214880723 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1720  time= 36.24364972114563  loss= 0.021194803874023102  val_loss= 0.06161769421622773  f1= 0.002779755010514305  f2= 0.0012518950343698256  f3= 0.0001971750901210426  f4= 0.016965978739017933 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1730  time= 36.4656126499176  loss= 0.07772053710417214  val_loss= 0.075137974581999  f1= 0.007325286478273824  f2= 0.002539079558574886  f3= 8.862429882896114e-05  f4= 0.06776754676849447 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1740  time= 36.70770716667175  loss= 0.0206787201721824  val_loss= 0.07791868445199732  f1= 0.005164661373838927  f2= 0.0018794432410581335  f3= 0.0007152671749800316  f4= 0.012919348382305304 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1750  time= 36.93960118293762  loss= 0.01216013712120387  val_loss= 0.0872342963878234  f1= 0.0028455976850250146  f2= 0.0010135008673408975  f3= 0.0008184429161114897  f4= 0.007482595652726466 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1760  time= 37.14284038543701  loss= 0.05542605461156732  val_loss= 0.04551974947793226  f1= 0.00499628956823096  f2= 0.0026147150000907352  f3= 0.0010760373760808873  f4= 0.04673901266716473 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1770  time= 37.377912282943726  loss= 0.0237963760974536  val_loss= 0.056722233506104794  f1= 0.003747828818397378  f2= 0.0021630942112875  f3= 0.0011399952885446233  f4= 0.016745457779224097 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1780  time= 37.60163950920105  loss= 0.030946628097719597  val_loss= 0.10307935284389044  f1= 0.010968587104547095  f2= 0.0015285229998388526  f3= 0.005745175012615934  f4= 0.012704342980717716 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1790  time= 37.82136583328247  loss= 0.03598206665893585  val_loss= 0.07154120539087336  f1= 0.0033988116328395614  f2= 0.00283859523823048  f3= 0.002392103205914001  f4= 0.027352556581951806 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1800  time= 38.010326623916626  loss= 0.023579437538303497  val_loss= 0.10402481213405877  f1= 0.007279618095776802  f2= 0.0015903822574001578  f3= 0.0068249779303309775  f4= 0.007884459254795558 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1810  time= 38.219945669174194  loss= 0.03405029146419521  val_loss= 0.06829254794777474  f1= 0.008435651885086506  f2= 0.0011467205968599228  f3= 0.0057893216991285225  f4= 0.018678597283120255 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1820  time= 38.4122154712677  loss= 0.021493433186681444  val_loss= 0.059317212759915444  f1= 0.00542049978665437  f2= 0.001691265299298226  f3= 0.0016807416829768683  f4= 0.012700926417751978 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1830  time= 38.62782907485962  loss= 0.05761163096252187  val_loss= 0.0907984402996696  f1= 0.010000305261012872  f2= 0.0024515689167381167  f3= 0.004685938267245068  f4= 0.04047381851752582 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1840  time= 38.82075214385986  loss= 0.02008816368657518  val_loss= 0.07295850964499968  f1= 0.003236263770057849  f2= 0.0017354472680513645  f3= 0.0011570528853808283  f4= 0.01395939976308514 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1850  time= 39.063762187957764  loss= 0.021996077709227766  val_loss= 0.06282601573496509  f1= 0.004129043309880317  f2= 0.0019159977157252636  f3= 0.0035209591043388355  f4= 0.012430077579283348 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1860  time= 39.39085531234741  loss= 0.05669248363224172  val_loss= 0.09915018144716191  f1= 0.012629908618506887  f2= 0.0021993085186862503  f3= 0.003160830586154044  f4= 0.03870243590889453 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1870  time= 39.725513219833374  loss= 0.02687746649230617  val_loss= 0.053523667231479166  f1= 0.003729106578196122  f2= 0.002174098365074134  f3= 0.0006833887057902076  f4= 0.020290872843245704 num_batches= 6 percent lr= 0.01\n","EarlyStopping counter: 996 out of 1000\n","EarlyStopping counter: 997 out of 1000\n","EarlyStopping counter: 998 out of 1000\n","EarlyStopping counter: 999 out of 1000\n","EarlyStopping counter: 1000 out of 1000\n","Early Stopping\n","[[ 0.39050803  1.72151493  0.82210701 ... -0.4309159  -3.16297689\n","  -1.21219209]\n"," [ 1.9207802   1.44411585  0.97907543 ...  0.9639992   1.11697794\n","   3.58832241]\n"," [ 2.22620934  2.78676216 -0.07664073 ... -3.40938975 -0.07226892\n","   1.740476  ]\n"," [ 1.90521237  3.251953    2.39892348 ...  3.55062071  1.49426694\n","  -1.69939694]] [[ 2.22620675  2.78675457 -0.0766604  ... -3.40937145 -0.07226473\n","   1.74047712]\n"," [ 1.90519961  3.25194664  2.39890006 ...  3.55057977  1.49426546\n","  -1.69940024]\n"," [-0.05186317 -0.15170663 -0.39334925 ...  0.36591158  0.08379871\n","   0.0223103 ]\n"," [-0.25503777 -0.12726516 -0.46851267 ... -0.81840556 -0.02959473\n","  -0.06604598]] [ 3.78273647e+00  8.72558631e+00  2.09816006e+00  3.73139654e+00\n","  7.45661341e+00  5.11838387e+00  3.54079714e+00  8.73667676e+00\n","  6.04721091e+00  7.71259891e+00  5.52053503e+00  3.69514107e-01\n","  2.41175588e+00  5.37516054e+00  7.12887538e+00  7.75705776e+00\n","  7.76739678e+00  9.47346610e-01  4.72712937e+00  5.05912242e+00\n","  1.31910988e+01  5.37564333e-01  5.95137986e+00  1.05241125e+01\n","  3.30780582e-01  7.66341027e+00  8.14112516e+00  7.38243204e+00\n"," -1.72377658e-01  1.98687809e+00  8.82049009e+00  4.20317367e+00\n","  1.52404746e+00  2.59246605e+00  1.50524969e+00  1.31737186e+00\n","  1.29750400e+00  5.32533905e-01  1.49776591e+01  1.41033584e+01\n","  1.05836328e+00  5.51124630e+00  6.52450923e+00  9.05563030e+00\n","  6.04688569e+00  7.29479656e+00  9.13494166e+00  1.33515480e+01\n","  6.09299930e+00  3.07811901e+00  1.16493694e+00  1.15039091e+01\n","  9.46944644e+00  1.13510320e+01  2.04583401e+00  3.08073222e+00\n","  5.05450290e-01  5.11582092e+00  8.98014648e+00  8.73072386e+00\n","  3.97959556e+00  1.59601757e+00  1.19466600e+01  4.98550333e+00\n","  1.08137512e+01 -3.71269262e-02  1.50938572e+00  1.06777605e+01\n","  9.52199550e+00  7.35759230e+00  5.64038808e+00  6.80706111e+00\n","  7.94976886e+00  9.95173051e+00  1.27197860e+00  1.10500005e-01\n","  1.51466996e+00  6.03852609e+00  1.61407899e+00  7.87798335e+00\n","  1.29233532e+01  7.56740728e+00  2.99353333e+00  1.06030273e+01\n","  2.21707961e+00  3.46250977e-01  1.81949656e+00  5.24237837e-01\n","  1.99777803e+00  8.21916457e+00  7.27599513e+00  2.38193272e+00\n","  3.50648691e+00  8.81344865e+00  6.05181448e+00  7.41514433e+00\n","  5.35361106e+00  1.16977285e+01  8.10346451e+00  4.66035227e+00\n","  9.11838392e-01  1.27218605e+01  8.30719485e+00  3.00044334e+00\n","  7.12892070e+00 -2.56395074e-01  7.04066847e+00  2.72748185e+00\n","  9.13624057e+00  7.45857849e+00  2.62472179e+00  6.96788438e+00\n","  9.15086516e+00  7.39793701e+00  2.85482149e+00  6.01598625e+00\n","  7.53978748e+00  5.09852834e-02  3.21497329e+00  7.20159118e-01\n","  1.12883767e+01 -3.17485187e+00  3.57695591e+00  4.12642350e+00\n","  5.23889410e-01  1.11767988e+00  7.27333987e+00  3.40565368e-01\n","  2.09424625e+00  1.67425911e+00  2.95071638e+00  6.84571191e+00\n","  6.05576165e+00  6.66788598e+00  8.33305009e+00  2.23602955e+00\n","  3.20675822e+00  3.97915822e+00  5.16145308e+00  6.99548266e+00\n","  5.18311758e+00  9.56292043e+00  6.56695719e+00  7.24691046e-02\n","  3.40212106e+00  8.90343862e+00  4.96043237e+00  7.18796703e+00\n","  1.55096775e+00  8.73411587e-01  1.03512094e+01  3.89505394e+00\n","  2.32342422e+00  9.77552913e-01  1.34867358e+01  7.97660167e+00\n","  6.32972476e+00  9.69356691e+00  3.58590362e+00  1.13495970e+00\n","  7.22969022e+00  4.11703934e-01  1.25377331e+00  2.07125007e+00\n","  1.07611829e+01  6.28792092e+00  6.91137645e+00  1.16589912e+01\n","  2.25100498e+00  2.18140713e-01  4.21994807e-01  2.18051817e+00\n","  1.83275465e+00  7.55124145e+00  8.39489699e+00  6.01131560e-01\n","  8.79689216e+00  9.85321246e+00  8.33885679e+00  6.76707494e+00\n","  2.29495128e+00  4.70164621e+00  1.28921517e+00  5.58840257e+00\n","  7.95781863e+00  8.48282940e+00  8.34002363e+00  5.40897405e+00\n","  4.43928009e+00  9.03509209e+00  1.68880334e+00  3.30717325e+00\n","  6.61881496e+00  3.75821131e+00  6.89204410e+00  8.75921512e-01\n","  7.39390969e+00  1.43869075e+01  7.94140038e+00  8.51645328e+00\n","  5.77096046e+00  4.86169475e+00  2.96036062e+00  7.46840202e+00\n","  2.87839899e+00  3.43481152e+00  4.82209399e+00  6.75665662e+00\n","  9.09840962e+00  3.43289850e+00  1.26026981e+01  7.63649114e+00\n","  3.26373848e+00  1.71498583e+00  5.67978197e+00  6.32166800e+00\n","  2.50337899e+00  3.49056436e+00  9.32827220e+00  4.66148089e+00\n","  7.04314490e+00  6.68300674e+00  2.33929185e+00  2.30944100e+00\n","  8.44131673e+00  5.11342462e+00  5.93879927e+00  6.97318678e-01\n","  5.68443194e+00  1.35863546e+01  1.37172112e+00  8.09576700e+00\n"," -7.33548951e-01  2.35686498e+00  1.41674038e+00  5.03490435e+00\n","  1.62400616e+00  9.28564301e-01  2.94694258e+00  2.38234173e+00\n","  6.55314683e+00  8.05274980e+00  9.45230674e+00  4.19527388e+00\n","  6.67425117e+00  4.31113231e-01  7.02932394e+00  1.24002736e+01\n","  4.36300743e+00 -1.14796873e+00  2.36519398e+00  4.22422801e+00\n","  5.16180476e+00  5.07656148e+00  6.39848515e+00  6.24052375e-01\n","  5.97901337e+00  3.66474323e+00  7.60048449e+00  9.92933930e+00\n","  3.03629781e+00  7.34449642e+00  7.58289771e+00  2.23228035e+00\n","  5.81111334e+00  1.83579173e+00  7.69441241e+00  8.43516631e+00\n","  1.10956738e+01 -2.67695436e-01  7.01457948e+00  1.03711131e+01\n","  6.45834926e-01  8.13278859e+00  8.83972480e+00  5.39571403e+00\n","  1.06864078e+01  7.82176145e+00  2.64935687e+00  2.88835713e+00\n","  4.65660357e-01  1.34733285e+01  5.75966444e-01  1.26388855e+01\n","  7.29775812e+00  2.84468938e+00  5.08037720e+00  4.37107900e+00\n","  1.97708162e+00  9.88620760e+00  7.03206850e+00  1.05975284e+00\n","  9.50324429e+00  6.64065817e+00  3.25122234e+00  7.42199713e+00\n","  1.45876999e+00  1.07685656e+01  1.10913407e+01  1.62523654e+00\n","  3.39283389e+00  4.17633046e+00  1.18543280e+01  8.93358893e+00\n","  7.50989487e+00  1.21068734e+01  4.30756798e+00  2.67056671e+00\n","  8.09575056e+00  2.63188893e+00  4.37930667e-01  4.76720350e-01\n","  4.69910683e+00 -1.67600129e-01  6.64436670e+00  2.40976022e+00\n","  2.95618485e+00 -2.30089159e-01  6.90873611e+00  9.43014383e+00\n","  1.13027066e+00  2.64002105e+00  9.65859617e+00  4.18743301e+00\n","  5.56436772e+00  6.45204051e+00  8.93015047e-01  8.74226757e-01\n","  2.82308438e+00  3.46700068e+00  1.27954450e-02  9.74916430e+00\n","  1.20371576e+01  2.50948041e-01  6.31350107e+00  1.08519725e+00\n","  1.65310968e+00  4.29431504e+00  1.37774472e+00 -2.89331953e-01\n","  6.45674440e+00  5.50387893e+00  6.70395081e+00  3.74803343e+00\n","  6.11005352e+00  1.48201211e+00  4.85596005e+00  7.44555131e+00\n","  1.21421552e+01  1.13379345e+01  7.62659714e+00  6.05640847e+00\n","  6.45082053e+00  6.19794696e-01  5.36832501e+00  1.58563480e+00\n","  7.71361005e+00  8.12250836e+00  8.95596372e+00 -1.77073856e-02\n","  1.12618704e+00  2.68477588e+00  7.19502192e+00  4.75817217e-01\n","  8.16358097e+00  2.16368515e+00  3.26400083e+00 -3.37265961e-01\n","  8.68221379e-01  7.61328248e+00  1.20809492e+01  5.19047658e-01\n","  4.93582723e+00  8.23005833e+00  1.04293599e+01  1.94602039e+00\n","  2.30203936e+00  1.84245480e+00  1.78576527e+00 -2.55140716e-01\n","  7.54665715e+00  4.95599566e+00  6.10510159e+00  9.95097586e+00\n","  1.10574406e+01 -6.39852835e-02  5.65066899e+00  8.31527069e+00\n","  6.14927093e+00  3.92999207e+00  6.92590500e+00  9.67647275e+00\n","  9.88553889e+00  4.18154825e+00  9.50170685e+00  5.52522290e+00\n","  6.59087502e+00  9.52381038e+00  8.04739820e+00  4.20608213e+00\n","  8.56915462e+00  1.76574688e+00  2.44465572e+00  1.38437433e+01\n","  1.39836579e-01  1.18726224e+01  3.63273735e+00  6.25316147e+00\n","  4.66431249e-03  4.23537375e+00  7.54618292e+00  8.18806356e+00\n"," -2.05608569e-01  1.20287530e+01  5.05519133e+00  2.53615171e+00\n"," -2.67703773e-01  4.33395160e+00  4.58226871e+00  5.61603220e+00\n","  3.40856553e+00  1.27634030e+01  5.30605204e+00  3.51414080e+00\n","  4.41097603e+00  9.27552049e+00  5.88844248e+00  3.33847795e+00\n","  4.46550305e+00  5.42470790e+00  5.30819417e+00  3.54752128e+00\n","  6.44131449e+00  7.66456610e-01  1.17953960e+01  6.28623492e+00\n","  6.99420367e+00  9.23592123e+00  7.54475154e+00  3.05770059e+00\n","  6.74467220e+00  2.26925981e+00  2.05979835e+00  6.04022083e+00\n","  6.71963482e+00  4.51122261e+00  8.53050744e+00  1.85265219e+00\n","  1.59688844e+00  5.57234590e+00  1.23017257e+00  4.03376395e-01\n","  3.52378513e-02  8.98004369e+00  4.59104088e+00  7.64451943e+00\n","  1.31282562e+01  2.00052115e+00  6.68342635e+00  9.13506008e+00\n","  2.20724851e+00  5.87594828e+00  4.51497576e-01  2.60095461e+00\n","  3.13752096e+00  3.81147025e-01 -1.26581620e-01  2.20961910e+00\n","  7.54229773e-01  6.06190135e+00  7.10672728e+00  1.02676363e+01\n","  6.43235903e+00  4.69123472e+00  6.65671270e+00  3.79344719e+00\n","  4.07537387e+00  1.01973585e+01  9.50920312e+00  9.90417655e-01\n","  5.70644298e+00  1.92976313e+00  2.56629052e+00  5.00307846e+00\n","  1.39610548e+01  3.23743141e+00  7.02151905e+00  2.82252715e+00\n","  4.00961242e+00  7.92313726e-01  3.53687199e+00  5.83616442e+00\n","  4.50873885e+00  2.83229818e+00 -1.00708234e+00  1.45933347e+01\n","  8.21799732e+00  8.39840366e+00  2.12081139e+00  4.52878147e+00\n","  4.22306528e+00  4.44530940e+00  1.09893182e+01 -1.68241483e-02\n"," -2.52575129e-01  1.15785024e+01  4.91531065e+00  8.79473935e+00\n","  6.34901965e+00  1.11683883e+01  8.20913173e-01  2.69457979e+00]\n","training sepNet\n"," \n","epoch= 0  time= 0.0393979549407959  loss= 8.523741485469833  val_loss= 11.199116260659919  f1= 6.126922571571966  f2= 1.9447848620398236  f3= 0.4518161466390395  f4= 0.00021790521900254058 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 10  time= 0.5401768684387207  loss= 5.371128547198077  val_loss= 3.7997158921857466  f1= 1.8453658379546953  f2= 1.9373823281241658  f3= 0.009142007047200792  f4= 1.5792383740720144 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 20  time= 1.0096559524536133  loss= 5.258860493068216  val_loss= 3.876050122913367  f1= 1.322368862678153  f2= 1.9854364304416443  f3= 0.18317571407497066  f4= 1.7678794858734486 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 30  time= 1.4707672595977783  loss= 5.71975334556486  val_loss= 3.2193187406590758  f1= 0.8270528325902101  f2= 1.929939360366878  f3= 0.014592725855730161  f4= 2.9481684267520425 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 40  time= 1.9341776371002197  loss= 5.037404132419221  val_loss= 3.9878334284429813  f1= 1.3202239271697624  f2= 1.9175192636329819  f3= 0.11172131844020348  f4= 1.687939623176274 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 50  time= 2.3904430866241455  loss= 5.832968530411924  val_loss= 2.593829258165682  f1= 0.7391015033546265  f2= 1.9065674799384666  f3= 0.012975701734043056  f4= 3.174323845384787 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 60  time= 2.868140935897827  loss= 4.091742736370039  val_loss= 2.2088976216939926  f1= 0.9216583705242617  f2= 1.7969284467566418  f3= 0.01102774582805364  f4= 1.3621281732610815 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 70  time= 3.3820953369140625  loss= 4.58826699523916  val_loss= 1.166873551851539  f1= 0.4808686501730888  f2= 1.6064351618269177  f3= 0.0737481691804593  f4= 2.4272150140586946 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 80  time= 3.864551544189453  loss= 4.02014707519158  val_loss= 1.1208152882264075  f1= 0.4187310733634709  f2= 1.3826146267183022  f3= 0.039706926201784155  f4= 2.1790944489080224 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 90  time= 4.1411683559417725  loss= 3.3014803478487083  val_loss= 1.427441937731072  f1= 0.4170478728924059  f2= 1.2530751724579707  f3= 0.06484674535298718  f4= 1.5665105571453442 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 100  time= 4.4764404296875  loss= 2.5488734526811045  val_loss= 1.531320012320109  f1= 0.2883317341171529  f2= 1.0292285956939518  f3= 0.017974717547073374  f4= 1.2133384053229264 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 110  time= 4.809952974319458  loss= 2.238907675563659  val_loss= 1.6801811438696839  f1= 0.2461974191692825  f2= 0.7709067856408134  f3= 0.3352683055431034  f4= 0.8865351652104593 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 120  time= 5.133017063140869  loss= 2.469325433840426  val_loss= 1.5443905060524161  f1= 0.2633947149534221  f2= 0.8534182061614871  f3= 0.15193407395308905  f4= 1.2005784387724274 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 130  time= 5.447182655334473  loss= 2.684157686892491  val_loss= 0.34803149581589643  f1= 0.11557986812163552  f2= 0.7297105217854122  f3= 0.023079802589703328  f4= 1.81578749439574 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 140  time= 5.793004035949707  loss= 1.8160846140501985  val_loss= 0.6255693653360346  f1= 0.15030085126587414  f2= 0.6614658386276345  f3= 0.09590186502974811  f4= 0.9084160591269419 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 150  time= 6.112111806869507  loss= 1.631320451730333  val_loss= 0.42643460576049086  f1= 0.10895257331847885  f2= 0.5245676820360049  f3= 0.030558774532872878  f4= 0.9672414218429766 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 160  time= 6.436806917190552  loss= 1.1471080413425077  val_loss= 1.528425270602448  f1= 0.13002220114102145  f2= 0.4631039131321108  f3= 0.171648472506292  f4= 0.3823334545630835 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 170  time= 6.76838755607605  loss= 1.4160083928164484  val_loss= 0.34438599612362714  f1= 0.06329569141265183  f2= 0.4201325045275078  f3= 0.005887075168278553  f4= 0.9266931217080102 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 180  time= 7.085599899291992  loss= 0.7783693634578763  val_loss= 0.4767813145395122  f1= 0.11987585819425373  f2= 0.3489121238057737  f3= 0.015681137302424125  f4= 0.29390024415542476 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 190  time= 7.405894756317139  loss= 0.6777221558509917  val_loss= 0.5769546106025243  f1= 0.107440767186958  f2= 0.24544835764602435  f3= 0.10341003876138787  f4= 0.22142299225662146 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 200  time= 7.720496654510498  loss= 1.0912471498410976  val_loss= 0.3887987578497952  f1= 0.05570640613262445  f2= 0.3022083194284946  f3= 0.07647657501673218  f4= 0.6568558492632465 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 210  time= 8.087847471237183  loss= 0.6052932567140766  val_loss= 0.7070114268710248  f1= 0.05489119552659508  f2= 0.19985942987147054  f3= 0.12003359624256411  f4= 0.2305090350734468 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 220  time= 8.455336809158325  loss= 0.4001676311584577  val_loss= 0.3596384708711171  f1= 0.039505070841591446  f2= 0.1161759355915707  f3= 0.01679399675360929  f4= 0.22769262797168624 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 230  time= 8.8011474609375  loss= 0.3934261756194237  val_loss= 0.48367176219952623  f1= 0.036472818875458196  f2= 0.1312516361351736  f3= 0.04123770256416489  f4= 0.184464018044627 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 240  time= 9.152208805084229  loss= 0.36027619838814773  val_loss= 0.47628965190857836  f1= 0.030097796502273743  f2= 0.08123752885248949  f3= 0.06884958092673603  f4= 0.18009129210664845 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 250  time= 9.486676692962646  loss= 0.20315919404919544  val_loss= 0.2721355499974116  f1= 0.03155292331962278  f2= 0.06729415018450587  f3= 0.03468614106838335  f4= 0.06962597947668346 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 260  time= 9.813678741455078  loss= 0.2931704940084993  val_loss= 0.2177947337998258  f1= 0.04490306894349888  f2= 0.07119766580403801  f3= 0.010640888488677926  f4= 0.1664288707722845 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 270  time= 10.176116466522217  loss= 0.268632421807015  val_loss= 0.17906268815924395  f1= 0.021939358314467444  f2= 0.054980914508340965  f3= 0.07900792408445058  f4= 0.11270422489975605 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 280  time= 10.495865821838379  loss= 0.137191543508433  val_loss= 0.43142263485908217  f1= 0.0151478561089389  f2= 0.03208642067663095  f3= 0.04151055825535054  f4= 0.048446708467512596 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 290  time= 10.83334231376648  loss= 0.14333290023449927  val_loss= 0.18181034142698418  f1= 0.011912198799141141  f2= 0.026217060665736786  f3= 0.007927324404303436  f4= 0.09727631636531792 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 300  time= 11.175242185592651  loss= 0.15402521138763617  val_loss= 0.18613908930595718  f1= 0.01763679546198434  f2= 0.015247124927777252  f3= 0.05750743116198467  f4= 0.06363385983588991 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 310  time= 11.507367610931396  loss= 0.16152044563654822  val_loss= 0.18349920493559815  f1= 0.05757219726050969  f2= 0.01316028239324235  f3= 0.006392543046614896  f4= 0.08439542293618128 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 320  time= 11.84898591041565  loss= 0.45213647263214757  val_loss= 0.38043110799125984  f1= 0.021606185934291616  f2= 0.030790297050517047  f3= 0.047604682724144066  f4= 0.3521353069231949 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 330  time= 12.214545726776123  loss= 0.11030453829691624  val_loss= 0.09346695112738329  f1= 0.01486233913499963  f2= 0.016907483088488433  f3= 0.003970693258412456  f4= 0.07456402281501574 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 340  time= 12.552363634109497  loss= 0.15452721660732968  val_loss= 0.14122329296482405  f1= 0.03520030384096985  f2= 0.008423720748684587  f3= 0.008903519013243348  f4= 0.10199967300443191 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 350  time= 12.884936094284058  loss= 0.28561663236797985  val_loss= 0.23825299122034316  f1= 0.015748762220010443  f2= 0.0175888322591456  f3= 0.044759235541577146  f4= 0.20751980234724662 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 360  time= 13.261912107467651  loss= 0.1504531328368428  val_loss= 0.11276600193292691  f1= 0.009831077909813761  f2= 0.009356515583377635  f3= 0.07029103817029772  f4= 0.060974501173353714 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 370  time= 13.594666481018066  loss= 0.24204078661889283  val_loss= 0.08747867216885266  f1= 0.009628553987975387  f2= 0.02843688624052904  f3= 0.023217601526442866  f4= 0.18075774486394555 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 380  time= 13.92800521850586  loss= 0.11257736506411907  val_loss= 0.13144932512386734  f1= 0.02777904794275116  f2= 0.010386850370825335  f3= 0.020983956681012148  f4= 0.05342751006953043 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 390  time= 14.265486478805542  loss= 0.5136607316353509  val_loss= 0.24227921680033399  f1= 0.030934536332199197  f2= 0.0852776794144358  f3= 0.032441865501561105  f4= 0.3650066503871548 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 400  time= 14.58754587173462  loss= 0.06559589308738556  val_loss= 0.08339131929921866  f1= 0.009482431876302214  f2= 0.005844706839161507  f3= 0.0014763655373061168  f4= 0.04879238883461571 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 410  time= 14.913634300231934  loss= 0.22535927566804884  val_loss= 0.07456718419727096  f1= 0.010592492464990258  f2= 0.010981526494835407  f3= 0.0049311764853271454  f4= 0.19885408022289605 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 420  time= 15.224998474121094  loss= 0.06515125722686434  val_loss= 0.15989658845475446  f1= 0.010767321810621538  f2= 0.005204253585849967  f3= 0.009990619875461902  f4= 0.03918906195493094 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 430  time= 15.568607091903687  loss= 0.0637602459881688  val_loss= 0.1225103461910783  f1= 0.010987527423105228  f2= 0.013640906861524625  f3= 0.0020920051096540507  f4= 0.037039806593884894 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 440  time= 15.8888099193573  loss= 0.08021242599006233  val_loss= 0.06284205969715242  f1= 0.01572085870881697  f2= 0.008708728155886986  f3= 0.0026959655243364304  f4= 0.05308687360102193 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 450  time= 16.20243215560913  loss= 0.05361830511150854  val_loss= 0.10185226005763814  f1= 0.00906348550473126  f2= 0.005418263739388972  f3= 0.00757905147275581  f4= 0.031557504394632487 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 460  time= 16.504751443862915  loss= 0.1080627770847517  val_loss= 0.07827067272366107  f1= 0.014633564746681251  f2= 0.0078028115014393265  f3= 0.034468901992817304  f4= 0.05115749884381384 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 470  time= 16.803629636764526  loss= 0.15303163161050967  val_loss= 0.11944721224180715  f1= 0.01651469184651476  f2= 0.008102342411560989  f3= 0.04169353028156337  f4= 0.08672106707087059 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 480  time= 17.125168323516846  loss= 0.09099392305902458  val_loss= 0.058878096805232635  f1= 0.012872916626182394  f2= 0.010794369597531342  f3= 0.019314999050748054  f4= 0.04801163778456278 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 490  time= 17.443272352218628  loss= 0.08789924082824525  val_loss= 0.07864113062938248  f1= 0.023759807787380613  f2= 0.0077456461060519755  f3= 0.010163808928700446  f4= 0.046229978006112206 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 500  time= 17.74866223335266  loss= 0.06576233076556509  val_loss= 0.09765846181333991  f1= 0.011498732179263308  f2= 0.005867745066437099  f3= 0.00884692883958087  f4= 0.039548924680283815 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 510  time= 18.08855152130127  loss= 0.13539522034216026  val_loss= 0.12994493760325343  f1= 0.03260522382211776  f2= 0.018745112599513173  f3= 0.032348201076156664  f4= 0.051696682844372656 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 520  time= 18.393977165222168  loss= 0.08602101899321911  val_loss= 0.10880988429186458  f1= 0.02060148839209899  f2= 0.007007884405693626  f3= 0.008481576867775231  f4= 0.04993006932765127 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 530  time= 18.697407960891724  loss= 0.22370944028233006  val_loss= 0.31857678642812415  f1= 0.029983532299144567  f2= 0.008757397310898566  f3= 0.04377910987997909  f4= 0.14118940079230777 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 540  time= 19.007288694381714  loss= 0.3730441912341651  val_loss= 0.056026698303885236  f1= 0.014014257459315059  f2= 0.026515098158436723  f3= 0.010330142925899405  f4= 0.32218469269051386 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 550  time= 19.309473276138306  loss= 0.16763419489573209  val_loss= 0.07024130380449628  f1= 0.014777800600115935  f2= 0.007739478551337984  f3= 0.019070363072012313  f4= 0.12604655267226586 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 560  time= 19.60703206062317  loss= 0.1676447032362427  val_loss= 0.07258215417162311  f1= 0.011921039503091015  f2= 0.012826324029813289  f3= 0.026839910312628112  f4= 0.11605742939071029 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 570  time= 19.906121969223022  loss= 0.04207719661581139  val_loss= 0.052150586416196534  f1= 0.0077990328120629325  f2= 0.004137186704396343  f3= 0.002718867526771166  f4= 0.027422109572580955 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 580  time= 20.212321043014526  loss= 0.09087567579904156  val_loss= 0.073368479827366  f1= 0.016875819884556525  f2= 0.008407336955589892  f3= 0.01071479716197405  f4= 0.054877721796921096 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 590  time= 20.514177322387695  loss= 0.05172969675417166  val_loss= 0.041375136483635264  f1= 0.01130102329848053  f2= 0.003583823521836335  f3= 0.002602591111252033  f4= 0.034242258822602754 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 600  time= 20.847167015075684  loss= 0.1744079073739164  val_loss= 0.09508768882763881  f1= 0.014727903590069391  f2= 0.011377302846029884  f3= 0.030516812891138962  f4= 0.11778588804667817 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 610  time= 21.14362597465515  loss= 0.09172629120179938  val_loss= 0.07750267584664619  f1= 0.007887472362060928  f2= 0.0073659674687948035  f3= 0.0015702601228120647  f4= 0.0749025912481316 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 620  time= 21.447443962097168  loss= 0.1258242558723416  val_loss= 0.04820881277456866  f1= 0.015940865120611675  f2= 0.004814512811446049  f3= 0.0012583851660421803  f4= 0.10381049277424174 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 630  time= 21.745381355285645  loss= 0.20411934067919754  val_loss= 0.04815760954411393  f1= 0.01627460064164075  f2= 0.011660860494217919  f3= 0.004596145932481772  f4= 0.1715877336108571 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 640  time= 22.05245327949524  loss= 0.16362247054895526  val_loss= 0.07222635155843744  f1= 0.01600668468282093  f2= 0.01456371805320268  f3= 0.008150047709027679  f4= 0.12490202010390396 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 650  time= 22.367084503173828  loss= 0.08424632570777585  val_loss= 0.0678447123928901  f1= 0.017594675852934196  f2= 0.0026278658135197586  f3= 0.009052960913626078  f4= 0.05497082312769582 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 660  time= 22.671021699905396  loss= 0.11904105158510696  val_loss= 0.0786696923291865  f1= 0.027185565798461153  f2= 0.006085325351008323  f3= 0.017505037511270912  f4= 0.06826512292436658 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 670  time= 23.00305199623108  loss= 0.18147908763873621  val_loss= 0.05858391970358876  f1= 0.014191708837167352  f2= 0.013168397786164944  f3= 0.0001902891322050391  f4= 0.15392869188319885 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 680  time= 23.330484628677368  loss= 0.23974506567003642  val_loss= 0.127490070240115  f1= 0.017518399477984392  f2= 0.01323586111862961  f3= 0.02026318622807326  f4= 0.18872761884534914 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 690  time= 23.657320499420166  loss= 0.5525784639158412  val_loss= 0.04687889402146106  f1= 0.008932379436033323  f2= 0.06697352217349874  f3= 0.004250777264810186  f4= 0.47242178504149895 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 700  time= 24.00100541114807  loss= 0.16578975655048542  val_loss= 0.04020655953928376  f1= 0.008038392883942428  f2= 0.013690538070833516  f3= 0.005793822407575218  f4= 0.13826700318813426 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 710  time= 24.33949089050293  loss= 0.19300697081986476  val_loss= 0.09001901357196354  f1= 0.012739106512048758  f2= 0.014562716746013527  f3= 0.015544088852135917  f4= 0.15016105870966653 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 720  time= 24.67024517059326  loss= 0.04143243131244842  val_loss= 0.05992982509577856  f1= 0.008152366035622666  f2= 0.003330795041550362  f3= 0.003802239798888385  f4= 0.026147030436387 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 730  time= 25.00799250602722  loss= 0.03785157437272039  val_loss= 0.05856978881967116  f1= 0.010636254551830681  f2= 0.0042196705117260825  f3= 0.0004722041969430664  f4= 0.022523445112220566 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 740  time= 25.37845230102539  loss= 0.028708681495202535  val_loss= 0.06135627756202749  f1= 0.004474570476464196  f2= 0.002836461192792118  f3= 0.002982266001522025  f4= 0.018415383824424196 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 750  time= 25.743775129318237  loss= 0.17309666635226284  val_loss= 0.06373009733508533  f1= 0.014617438645553545  f2= 0.007764597445021652  f3= 0.004486307655706806  f4= 0.14622832260598084 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 760  time= 26.099217891693115  loss= 0.04621728863303718  val_loss= 0.049952891839897345  f1= 0.005130010760310705  f2= 0.0031217551174354615  f3= 0.0045057590576990745  f4= 0.03345976369759194 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 770  time= 26.44495463371277  loss= 0.06181056585989305  val_loss= 0.030127516662090625  f1= 0.008782669810499117  f2= 0.002127711513930951  f3= 0.0009661723068153292  f4= 0.049934012228647656 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 780  time= 26.755250215530396  loss= 0.2533448150879793  val_loss= 0.11559700884644189  f1= 0.009234412442507034  f2= 0.024565314549158862  f3= 0.011008626989401924  f4= 0.20853646110691149 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 790  time= 27.097123384475708  loss= 0.1971769477742591  val_loss= 0.06999727518383461  f1= 0.009396850833766382  f2= 0.01900319536152077  f3= 0.0009333005424560326  f4= 0.16784360103651588 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 800  time= 27.421417474746704  loss= 0.27574858950374265  val_loss= 0.08464432314763382  f1= 0.00962647499208148  f2= 0.020356344296318628  f3= 0.01537636044850685  f4= 0.23038940976683567 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 810  time= 27.720271825790405  loss= 0.07849849928766156  val_loss= 0.05074141816455442  f1= 0.009424080526153143  f2= 0.00812830083178058  f3= 0.0043216916071404515  f4= 0.056624426322587386 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 820  time= 28.03339385986328  loss= 0.10801165981366179  val_loss= 0.07672431219615304  f1= 0.013039993093320162  f2= 0.006865071419412898  f3= 0.007998126713875436  f4= 0.0801084685870533 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 830  time= 28.35420846939087  loss= 0.04750139470250409  val_loss= 0.028337727199577136  f1= 0.009982793482816868  f2= 0.0043018813897015154  f3= 0.0027036462226537276  f4= 0.03051307360733198 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 840  time= 28.668771028518677  loss= 0.0962291742257163  val_loss= 0.032994866199828235  f1= 0.007972870094366048  f2= 0.005538051049495767  f3= 0.0009200142133820784  f4= 0.08179823886847239 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 850  time= 28.966456651687622  loss= 0.05166091139528569  val_loss= 0.02819638319169169  f1= 0.010692465090592832  f2= 0.0028257294691431535  f3= 0.0001889006130187836  f4= 0.03795381622253091 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 860  time= 29.261675119400024  loss= 0.03050857739559487  val_loss= 0.0405350180280015  f1= 0.008718850733557222  f2= 0.002960488817170133  f3= 0.002969072248862649  f4= 0.01586016559600487 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 870  time= 29.567405462265015  loss= 0.06460272146212581  val_loss= 0.0668262356108019  f1= 0.007937417111499518  f2= 0.005820512804908127  f3= 0.004809612454736721  f4= 0.04603517909098145 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 880  time= 29.879014253616333  loss= 0.1713605465723935  val_loss= 0.04404843231711919  f1= 0.04686581150350483  f2= 0.005132955679811262  f3= 0.01941654114269697  f4= 0.09994523824638041 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 890  time= 30.17442011833191  loss= 0.0518854681852117  val_loss= 0.0504644130255545  f1= 0.006655824425975988  f2= 0.0036122004079252278  f3= 0.011406992307778085  f4= 0.030210451043532396 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 900  time= 30.50494885444641  loss= 0.03295724769895982  val_loss= 0.06164743640808541  f1= 0.006541649288468075  f2= 0.0028656381659145727  f3= 0.003088553383851863  f4= 0.02046140686072531 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 910  time= 30.846606016159058  loss= 0.21197024340876083  val_loss= 0.1492065482807748  f1= 0.03926360794087994  f2= 0.01046743277323161  f3= 0.03644896779911133  f4= 0.12579023489553795 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 920  time= 31.192639350891113  loss= 0.07360389447544398  val_loss= 0.02254751584675933  f1= 0.015171424903377187  f2= 0.0031570677009644197  f3= 0.0019255166284360048  f4= 0.05334988524266637 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 930  time= 31.525734663009644  loss= 0.07264086272677607  val_loss= 0.10255849869209141  f1= 0.02475456806932229  f2= 0.0055914050035859785  f3= 0.0027285002384388503  f4= 0.03956638941542895 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 940  time= 31.82993769645691  loss= 0.17551062301453388  val_loss= 0.06082505499465371  f1= 0.01653080444109108  f2= 0.0067199518119812176  f3= 0.004813127557729178  f4= 0.1474467392037324 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 950  time= 32.14818072319031  loss= 0.1046359311321186  val_loss= 0.04196059547566682  f1= 0.014097990436867347  f2= 0.003043077284785475  f3= 0.005099237913658106  f4= 0.08239562549680766 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 960  time= 32.48729228973389  loss= 0.030124655182448334  val_loss= 0.03572919110233759  f1= 0.009007097661367607  f2= 0.002284922180328596  f3= 0.0031260049289000714  f4= 0.015706630411852062 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 970  time= 32.812278270721436  loss= 0.030466511150883984  val_loss= 0.041560752488017  f1= 0.006959784151710383  f2= 0.0034574522551689903  f3= 0.00014602562588416  f4= 0.01990324911812045 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 980  time= 33.14419388771057  loss= 0.06987659024509771  val_loss= 0.06945053464206706  f1= 0.009878891106024088  f2= 0.004890894295428935  f3= 0.003627932949662936  f4= 0.05147887189398175 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 990  time= 33.47494339942932  loss= 0.08544465370491616  val_loss= 0.025221436652139835  f1= 0.007965241267321831  f2= 0.009986472043259174  f3= 6.492153139203936e-05  f4= 0.06742801886294313 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1000  time= 33.78003239631653  loss= 0.12109073632067534  val_loss= 0.09254822634779249  f1= 0.01566506663469243  f2= 0.009246098349719581  f3= 0.018743955037541643  f4= 0.07743561629872167 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1010  time= 34.10637950897217  loss= 0.059380467253745094  val_loss= 0.034940329028744437  f1= 0.007257475443247476  f2= 0.0036042897460808517  f3= 0.014153455503467242  f4= 0.03436524656094952 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1020  time= 34.42896246910095  loss= 0.10369102353778763  val_loss= 0.04600791633057118  f1= 0.0057080903127475005  f2= 0.009124222175107426  f3= 0.004656183849892669  f4= 0.08420252720004003 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1030  time= 34.747193813323975  loss= 0.04701318800572546  val_loss= 0.03290572525479083  f1= 0.004536214416652927  f2= 0.0025727794772312405  f3= 0.005013940443368709  f4= 0.034890253668472586 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1040  time= 35.05749297142029  loss= 0.08243581227321632  val_loss= 0.05051056914572445  f1= 0.016621696718640302  f2= 0.008809323577924433  f3= 0.00130522776732727  f4= 0.05569956420932431 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1050  time= 35.36933183670044  loss= 0.05904646374843489  val_loss= 0.02602654811987027  f1= 0.008636948694116598  f2= 0.00688590863546281  f3= 0.002702608124029664  f4= 0.04082099829482582 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1060  time= 35.720285415649414  loss= 0.11380012967802661  val_loss= 0.08398896845678908  f1= 0.019082212589384254  f2= 0.009701608698343809  f3= 0.004601894309947632  f4= 0.0804144140803509 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1070  time= 36.06957221031189  loss= 0.04622978435071016  val_loss= 0.044871128844043456  f1= 0.01121248107820271  f2= 0.0051248670478917  f3= 0.005583434908013231  f4= 0.024309001316602522 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1080  time= 36.408316135406494  loss= 0.08869465888496919  val_loss= 0.07084454248316077  f1= 0.008054680581761156  f2= 0.007800176374382534  f3= 0.002886038098602016  f4= 0.06995376383022346 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1090  time= 36.773377418518066  loss= 0.06075854668894898  val_loss= 0.11125921241224304  f1= 0.010834901799704514  f2= 0.004353734326817999  f3= 0.016752667367738037  f4= 0.028817243194688424 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1100  time= 37.133753061294556  loss= 0.14292633367661858  val_loss= 0.08986227139842154  f1= 0.021926581057197855  f2= 0.006756380270417532  f3= 0.012017294968897451  f4= 0.10222607738010574 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1110  time= 37.46936273574829  loss= 0.025663538953029513  val_loss= 0.06759278976625  f1= 0.006108351608800641  f2= 0.0032146456257970413  f3= 0.0009181565201949957  f4= 0.015422385198236839 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1120  time= 37.787898778915405  loss= 0.0881599863293902  val_loss= 0.07488145598701529  f1= 0.01270531483290053  f2= 0.0065055508619478244  f3= 0.00789556818691059  f4= 0.06105355244763127 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1130  time= 38.101136207580566  loss= 0.024734397645493993  val_loss= 0.04265275382966403  f1= 0.011751100686865967  f2= 0.002149086433168033  f3= 0.0003471610413413218  f4= 0.010487049484118669 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1140  time= 38.413625717163086  loss= 0.0798902362645258  val_loss= 0.05339326247843759  f1= 0.016628664855663722  f2= 0.002582389855251152  f3= 0.0029396830492503354  f4= 0.057739498504360574 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1150  time= 38.72510743141174  loss= 0.03623650309624344  val_loss= 0.03474340401160193  f1= 0.01617410422871795  f2= 0.0017683359284190311  f3= 0.0058530841829398704  f4= 0.012440978756166591 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1160  time= 39.03258442878723  loss= 0.18219337319059423  val_loss= 0.051206800238502284  f1= 0.01952900416480312  f2= 0.010472374446648084  f3= 0.0052207212035252455  f4= 0.14697127337561774 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1170  time= 39.34139060974121  loss= 0.062079666843142256  val_loss= 0.0562734377489088  f1= 0.005621627365684442  f2= 0.006951712052958813  f3= 0.009022518106668915  f4= 0.04048380931783009 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1180  time= 39.671350717544556  loss= 0.04357384845887823  val_loss= 0.029601464752800098  f1= 0.004687760173356033  f2= 0.0032716052711956457  f3= 0.0013649128483644469  f4= 0.034249570165962105 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1190  time= 39.9801242351532  loss= 0.07402773336449722  val_loss= 0.06535715151567548  f1= 0.016171744293683094  f2= 0.004723703503299099  f3= 0.002005960292462799  f4= 0.05112632527505224 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1200  time= 40.28814077377319  loss= 0.24475032842306157  val_loss= 0.23747003835542813  f1= 0.014295021631272173  f2= 0.01224718095563012  f3= 0.0414439569905948  f4= 0.17676416884556448 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1210  time= 40.62907099723816  loss= 0.04267884120826735  val_loss= 0.03846896128489642  f1= 0.00961458073325167  f2= 0.005592946976025506  f3= 0.0012623074886441788  f4= 0.026209006010346 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1220  time= 40.99289107322693  loss= 0.09668210289638857  val_loss= 0.03598128552421152  f1= 0.013163327607476205  f2= 0.004157823113049316  f3= 0.0007960380283623535  f4= 0.0785649141475007 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1230  time= 41.36536455154419  loss= 0.09295035005179136  val_loss= 0.06199932256743006  f1= 0.013193258125947625  f2= 0.006354665429698432  f3= 0.001996258888598232  f4= 0.07140616760754707 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1240  time= 41.73297667503357  loss= 0.06661923556216319  val_loss= 0.03989939150726673  f1= 0.008997679111923599  f2= 0.0022082850288859474  f3= 0.008653357110356988  f4= 0.04675991431099666 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1250  time= 42.07519602775574  loss= 0.2665482914407908  val_loss= 0.05092756368714088  f1= 0.023410952519447147  f2= 0.013621720689512966  f3= 0.001658525506763797  f4= 0.2278570927250669 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1260  time= 42.43509268760681  loss= 0.0292896735280696  val_loss= 0.026437720093106614  f1= 0.004902483541412511  f2= 0.002316660347928021  f3= 0.001420454976044704  f4= 0.02065007466268436 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1270  time= 42.788066148757935  loss= 0.03702732573061945  val_loss= 0.04432751435327467  f1= 0.009031678279304546  f2= 0.004329609564044905  f3= 0.005791929218804471  f4= 0.01787410866846553 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1280  time= 43.131975412368774  loss= 0.08317541507408639  val_loss= 0.07557497113163308  f1= 0.02899857790420919  f2= 0.006191990308242675  f3= 0.018951822174775118  f4= 0.02903302468685941 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1290  time= 43.50036096572876  loss= 0.05257527140585053  val_loss= 0.03611963148125004  f1= 0.009124978668465464  f2= 0.002110654237598368  f3= 0.010639681504644498  f4= 0.030699956995142202 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1300  time= 43.85908031463623  loss= 0.05865020247353506  val_loss= 0.03148901239661809  f1= 0.010018483850055849  f2= 0.002509921954128543  f3= 0.007726266564219586  f4= 0.03839553010513108 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1310  time= 44.219197511672974  loss= 0.040608086214485806  val_loss= 0.030526571908129914  f1= 0.0033193424132811843  f2= 0.003574229519205139  f3= 0.0009309625919787113  f4= 0.03278355169002077 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1320  time= 44.58114314079285  loss= 0.09139553914465699  val_loss= 0.061131784910626236  f1= 0.013627049754244143  f2= 0.007407007222644461  f3= 0.010605719735395043  f4= 0.05975576243237335 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1330  time= 44.91745591163635  loss= 0.1047133158788463  val_loss= 0.04621750898579759  f1= 0.00565918258208514  f2= 0.010115069335970839  f3= 0.0003267307804810593  f4= 0.08861233318030926 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1340  time= 45.23103046417236  loss= 0.027445598005963485  val_loss= 0.01952110822115266  f1= 0.0049374824354624074  f2= 0.0023720695603858368  f3= 0.0022229724518445116  f4= 0.01791307355827073 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1350  time= 45.54966926574707  loss= 0.10928639200964425  val_loss= 0.06527074099042812  f1= 0.017208193739715674  f2= 0.008756444192333093  f3= 0.006050120769866694  f4= 0.0772716333077288 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1360  time= 45.87662601470947  loss= 0.03683927299841266  val_loss= 0.023713325328612055  f1= 0.004928362715616933  f2= 0.0017300728168147068  f3= 0.003337228074972054  f4= 0.026843609391008966 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1370  time= 46.20249676704407  loss= 0.30819002884134966  val_loss= 0.03932823206647776  f1= 0.008101334593764576  f2= 0.024206567031587387  f3= 0.001447143590739476  f4= 0.27443498362525826 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1380  time= 46.51799726486206  loss= 0.22810358479305895  val_loss= 0.03508927866222683  f1= 0.006404982554029014  f2= 0.0245596660684109  f3= 0.002005194145441034  f4= 0.19513374202517797 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1390  time= 46.84963893890381  loss= 0.1387755354109634  val_loss= 0.10273536690713896  f1= 0.02502528427752303  f2= 0.009568661745862033  f3= 0.011773989064027558  f4= 0.09240760032355076 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1400  time= 47.18834447860718  loss= 0.5885215407456997  val_loss= 0.041029352265065946  f1= 0.005207131423816329  f2= 0.030802521128404328  f3= 0.0021355578482550454  f4= 0.550376330345224 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1410  time= 47.54480504989624  loss= 0.03711730088556347  val_loss= 0.02517122262106849  f1= 0.008067239819270417  f2= 0.005032612782342044  f3= 0.00021031080133785798  f4= 0.023807137482613146 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1420  time= 47.90636348724365  loss= 0.03249367071891016  val_loss= 0.030221303811660036  f1= 0.004888310543815227  f2= 0.005172918340964216  f3= 0.0031716763676375623  f4= 0.019260765466493156 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1430  time= 48.26935386657715  loss= 0.3967053810742592  val_loss= 0.04703729794818377  f1= 0.031748945091127224  f2= 0.01298658869473466  f3= 0.004940814493022123  f4= 0.3470290327953751 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1440  time= 48.62019085884094  loss= 0.09945819407605705  val_loss= 0.0393603861178964  f1= 0.007868676933444094  f2= 0.007074536127889771  f3= 0.002528618520468528  f4= 0.08198636249425466 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1450  time= 48.967265367507935  loss= 0.029706388128863965  val_loss= 0.04085989297988395  f1= 0.004050783137234284  f2= 0.002535707920852508  f3= 0.00295465169729474  f4= 0.020165245373482434 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1460  time= 49.27455806732178  loss= 0.09631701405207603  val_loss= 0.016694135522695174  f1= 0.007508649503739831  f2= 0.011784779443169734  f3= 0.0002070301683004793  f4= 0.07681655493686597 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1470  time= 49.58279061317444  loss= 0.08790208428661234  val_loss= 0.038574075976098035  f1= 0.011753847214616258  f2= 0.009076406254040215  f3= 0.003512925980436763  f4= 0.0635589048375191 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1480  time= 49.93133568763733  loss= 0.021495369935983766  val_loss= 0.028850473989958467  f1= 0.005699988310177587  f2= 0.0017630526929299946  f3= 0.0001444096936403645  f4= 0.013887919239235824 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1490  time= 50.28134870529175  loss= 0.06589091602420216  val_loss= 0.052980547650246804  f1= 0.013923821260813034  f2= 0.004091950873982132  f3= 0.0014166959754281173  f4= 0.04645844791397888 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1500  time= 50.63555026054382  loss= 0.17976212050644103  val_loss= 0.047339972391825275  f1= 0.010300168284594586  f2= 0.016067609734096377  f3= 0.004592116146745494  f4= 0.14880222634100457 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1510  time= 51.00675845146179  loss= 0.06092500671808181  val_loss= 0.04350563000988658  f1= 0.01864622441189349  f2= 0.0018342054587543836  f3= 0.0003914991315546361  f4= 0.040053077715879286 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1520  time= 51.37278437614441  loss= 0.07111289061130761  val_loss= 0.08284254907651183  f1= 0.012173373456477727  f2= 0.0027260457663211505  f3= 0.014297506780653932  f4= 0.0419159646078548 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1530  time= 51.7111394405365  loss= 0.06913197752879303  val_loss= 0.02260852399843344  f1= 0.006460105119351317  f2= 0.004450678162787742  f3= 0.006752100304373764  f4= 0.05146909394228022 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1540  time= 52.06022381782532  loss= 0.019352914026734423  val_loss= 0.019781407484784103  f1= 0.006196505842654988  f2= 0.0027038688647777493  f3= 0.0003507354375890629  f4= 0.010101803881712624 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1550  time= 52.4282009601593  loss= 0.08685945995003963  val_loss= 0.050702073332853884  f1= 0.023566521651879373  f2= 0.003608779113739965  f3= 0.010692163301379275  f4= 0.04899199588304102 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1560  time= 52.77964401245117  loss= 0.058217679975064995  val_loss= 0.03135412896352396  f1= 0.006472909894535102  f2= 0.00344259569557937  f3= 0.0016922835822347742  f4= 0.04660989080271576 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1570  time= 53.133227825164795  loss= 0.21968034070812822  val_loss= 0.0444697215224069  f1= 0.01851384519199031  f2= 0.008784428333181984  f3= 0.005525954719677431  f4= 0.1868561124632785 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1580  time= 53.45280885696411  loss= 0.051913944638696606  val_loss= 0.03673785457496677  f1= 0.007205045946550876  f2= 0.005313174749302558  f3= 0.0010438394280143728  f4= 0.0383518845148288 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1590  time= 53.7642879486084  loss= 0.022034873512567707  val_loss= 0.032132235480064074  f1= 0.004221456607821182  f2= 0.001259011870709997  f3= 0.0036769409366860495  f4= 0.01287746409735048 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1600  time= 54.099995136260986  loss= 0.32434023869699663  val_loss= 0.043078530347785075  f1= 0.008935289562796683  f2= 0.018794023926161352  f3= 0.005029187055446012  f4= 0.2915817381525926 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1610  time= 54.42995238304138  loss= 0.13035117122802695  val_loss= 0.23461874802038465  f1= 0.012931456417501931  f2= 0.009264965638975366  f3= 0.01876120331930531  f4= 0.08939354585224435 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1620  time= 54.73569416999817  loss= 0.028955258868688185  val_loss= 0.06584116950588155  f1= 0.014645537223528385  f2= 0.0027921086949922133  f3= 0.0009738010520079971  f4= 0.010543811898159589 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1630  time= 55.07537293434143  loss= 0.06284821710512171  val_loss= 0.029041666528241826  f1= 0.008206143335116265  f2= 0.0028923669746519828  f3= 0.0006527508876284441  f4= 0.05109695590772503 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1640  time= 55.401416540145874  loss= 0.014155087220003387  val_loss= 0.024103734647806963  f1= 0.0047273895993481556  f2= 0.0023327719027413187  f3= 9.831811395589888e-05  f4= 0.006996607603958017 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1650  time= 55.75021576881409  loss= 0.033643028606151176  val_loss= 0.039688717215133125  f1= 0.006218104193242163  f2= 0.0016754541962358703  f3= 0.01230609253803127  f4= 0.013443377678641871 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1660  time= 56.10923171043396  loss= 0.02844040188958931  val_loss= 0.0380830723986811  f1= 0.005489294221589565  f2= 0.0018061082518431605  f3= 0.005030130214589884  f4= 0.0161148692015667 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1670  time= 56.474820613861084  loss= 0.030184403321340304  val_loss= 0.028250301235050936  f1= 0.006025378402659014  f2= 0.0026390317776210028  f3= 0.004799380502544565  f4= 0.016720612638515726 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1680  time= 56.840795040130615  loss= 0.10459065412308512  val_loss= 0.033930933080838326  f1= 0.005255820182806491  f2= 0.015508705660045344  f3= 0.00479821921020178  f4= 0.0790279090700315 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1690  time= 57.207271099090576  loss= 0.037992420981182974  val_loss= 0.018100166841056465  f1= 0.005184925016619499  f2= 0.002510304782648207  f3= 0.0031937052992338233  f4= 0.027103485882681442 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1700  time= 57.56093120574951  loss= 0.03424397049878808  val_loss= 0.023849350208850414  f1= 0.008240064494968505  f2= 0.0015964736209397813  f3= 0.004104880808121703  f4= 0.020302551574758094 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1710  time= 57.897079944610596  loss= 0.05140666871457755  val_loss= 0.03503352852780604  f1= 0.0055044762700811566  f2= 0.0034972585629800025  f3= 0.0005322029880976651  f4= 0.04187273089341873 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1720  time= 58.2444429397583  loss= 0.07467568368684199  val_loss= 0.02246424981998709  f1= 0.005284109220464688  f2= 0.004748298507240946  f3= 0.005284747510908228  f4= 0.059358528448228136 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1730  time= 58.55069279670715  loss= 0.03515678478571648  val_loss= 0.029613130806883672  f1= 0.003799361401755207  f2= 0.002732336340561973  f3= 0.00812258969076654  f4= 0.020502497352632754 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1740  time= 58.86620783805847  loss= 0.04706955477520531  val_loss= 0.041178318542809816  f1= 0.00522433077886188  f2= 0.002531492978687323  f3= 0.0097555833710942  f4= 0.02955814764656191 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1750  time= 59.19012808799744  loss= 0.2335110348887778  val_loss= 0.10142444773964382  f1= 0.009710067643854035  f2= 0.022575115583945118  f3= 0.004581931323668661  f4= 0.19664392033731007 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1760  time= 59.50536394119263  loss= 0.086772455723766  val_loss= 0.058519686730620364  f1= 0.009631325416504377  f2= 0.004706437830825233  f3= 0.011922227466701985  f4= 0.0605124650097344 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1770  time= 59.840184926986694  loss= 0.09337531375259868  val_loss= 0.08526439725660423  f1= 0.008928635966054853  f2= 0.006139015741405032  f3= 0.005043079577504593  f4= 0.07326458246763419 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1780  time= 60.18188810348511  loss= 0.03678886693656847  val_loss= 0.023593219026269447  f1= 0.006967751253113855  f2= 0.004618971367130387  f3= 0.0025746852148586736  f4= 0.022627459101465557 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1790  time= 60.52598214149475  loss= 0.028063315915323305  val_loss= 0.03237342999943389  f1= 0.007903609222016836  f2= 0.0035284779368029086  f3= 0.003306761632514402  f4= 0.013324467123989154 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1800  time= 60.891486167907715  loss= 0.10891214999469982  val_loss= 0.0412947409806714  f1= 0.010375949674969838  f2= 0.003820632320837581  f3= 0.0011184325486234075  f4= 0.09359713545026897 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1810  time= 61.26530313491821  loss= 0.04367296251048359  val_loss= 0.06182246044106476  f1= 0.007808706109613557  f2= 0.0030394042403137506  f3= 0.008244221032543995  f4= 0.024580631128012283 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1820  time= 61.589627504348755  loss= 0.09044400095776887  val_loss= 0.03977963057317176  f1= 0.007142681740467283  f2= 0.006654520687921353  f3= 0.0025483562579385645  f4= 0.07409844227144166 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1830  time= 61.933719635009766  loss= 0.013363481500175613  val_loss= 0.01865428138790015  f1= 0.003634195764702334  f2= 0.0020233656483219235  f3= 0.0006889718781236737  f4= 0.007016948209027682 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1840  time= 62.28961992263794  loss= 0.03986249541931403  val_loss= 0.05451085250678241  f1= 0.008662366531501329  f2= 0.007086882116474523  f3= 0.0018995376618436844  f4= 0.0222137091094945 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1850  time= 62.64615845680237  loss= 0.042620391197000285  val_loss= 0.029318556478420542  f1= 0.014860490393552828  f2= 0.004350428003833449  f3= 0.0037371473400171135  f4= 0.019672325459596886 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1860  time= 63.01310610771179  loss= 0.016774833381921105  val_loss= 0.0192543551248512  f1= 0.003149273436342293  f2= 0.0013825543906309014  f3= 0.0009420934667372821  f4= 0.011300912088210628 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1870  time= 63.35752534866333  loss= 0.015565909324380086  val_loss= 0.017551401124841  f1= 0.003853166057826375  f2= 0.0014248748401916751  f3= 0.0006895328023086792  f4= 0.009598335624053354 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1880  time= 63.691200256347656  loss= 0.1274372532757529  val_loss= 0.056611572450147134  f1= 0.013079387100445344  f2= 0.007492364240058349  f3= 0.0026165434714929287  f4= 0.10424895846375626 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1890  time= 64.00803971290588  loss= 0.024775425689978186  val_loss= 0.03674445020165  f1= 0.0071677955004346705  f2= 0.0014978177228647035  f3= 0.001456205540496547  f4= 0.01465360692618226 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1900  time= 64.32513380050659  loss= 0.1363465687600974  val_loss= 0.05634158847339917  f1= 0.007250614099828825  f2= 0.020190848722674775  f3= 0.005488882122955892  f4= 0.10341622381463789 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1910  time= 64.62516832351685  loss= 0.059875636394801396  val_loss= 0.030349589313304413  f1= 0.0054339133549938775  f2= 0.0037550065995423146  f3= 0.00524773394807641  f4= 0.0454389824921888 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1920  time= 64.94479727745056  loss= 0.05351380164406492  val_loss= 0.056308178103729095  f1= 0.00821373707542297  f2= 0.003395834169793281  f3= 0.0013300378063198744  f4= 0.04057419259252879 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1930  time= 65.27211403846741  loss= 0.1290349391242782  val_loss= 0.044449643388385494  f1= 0.00835780156205612  f2= 0.011433062253919012  f3= 0.007689415880988228  f4= 0.10155465942731483 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1940  time= 65.61907768249512  loss= 0.032640001511786795  val_loss= 0.030952700906399424  f1= 0.0066460821699418715  f2= 0.003666892994978266  f3= 0.0040173726002886425  f4= 0.018309653746578017 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1950  time= 65.9944543838501  loss= 0.0346342731796721  val_loss= 0.03247645887171057  f1= 0.005990919615056445  f2= 0.0025672071925949397  f3= 0.0001368477949632655  f4= 0.025939298577057446 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1960  time= 66.37878227233887  loss= 0.0832321867723619  val_loss= 0.1036938230206254  f1= 0.010466186334501165  f2= 0.014552480004511379  f3= 0.000980281862243394  f4= 0.05723323857110598 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1970  time= 66.75184941291809  loss= 0.031032268949261282  val_loss= 0.026995555181771776  f1= 0.0045142964792789335  f2= 0.002060859446465621  f3= 0.002192803656469867  f4= 0.02226430936704686 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1980  time= 67.13893270492554  loss= 0.13226165919876368  val_loss= 0.07220902106299402  f1= 0.0038740353079070046  f2= 0.013983735049878772  f3= 0.014890324857202536  f4= 0.09951356398377535 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1990  time= 67.5236759185791  loss= 0.03968018074707284  val_loss= 0.03717966716912769  f1= 0.005710723167982786  f2= 0.0024860546339022585  f3= 0.0030834415739643614  f4= 0.028399961371223425 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2000  time= 67.87418937683105  loss= 0.027937526415245398  val_loss= 0.022144725625822086  f1= 0.0037339238578217716  f2= 0.0014488978971498856  f3= 0.0024249685847931267  f4= 0.020329736075480612 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2010  time= 68.24929094314575  loss= 0.02573598195995318  val_loss= 0.024376990702359085  f1= 0.004356177804947943  f2= 0.002352416941017996  f3= 0.00041952635526797344  f4= 0.018607860858719273 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2020  time= 68.627037525177  loss= 0.03864201655550138  val_loss= 0.046910820969551584  f1= 0.011438026580134505  f2= 0.003267254605550888  f3= 0.0007496624428993443  f4= 0.023187072926916647 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2030  time= 68.98208451271057  loss= 0.10738419028305868  val_loss= 0.02319978666424906  f1= 0.01000572327534159  f2= 0.005901462100983413  f3= 0.005729262721648934  f4= 0.08574774218508473 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2040  time= 69.3378427028656  loss= 0.050806569383226  val_loss= 0.02183577697873891  f1= 0.005317536197592387  f2= 0.002602069424203051  f3= 0.0026394087792468127  f4= 0.04024755498218375 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2050  time= 69.70436763763428  loss= 0.0190636778353073  val_loss= 0.03015060087550659  f1= 0.004046937374674408  f2= 0.0013870991408437407  f3= 0.001964932646301563  f4= 0.011664708673487588 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2060  time= 70.0572829246521  loss= 0.046873536082567085  val_loss= 0.02022569568915087  f1= 0.005224453977496463  f2= 0.002272873809293857  f3= 0.0003063660519741069  f4= 0.039069842243802666 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2070  time= 70.4137556552887  loss= 0.09914692388275803  val_loss= 0.08665039269841017  f1= 0.010609915611973275  f2= 0.004979927213672726  f3= 0.013318620458035136  f4= 0.07023846059907689 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2080  time= 70.77585005760193  loss= 0.043805811587518055  val_loss= 0.04625281570986724  f1= 0.007090573701366995  f2= 0.0029221036076733653  f3= 0.00887504652401413  f4= 0.024918087754463563 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2090  time= 71.13136506080627  loss= 0.04769181515971424  val_loss= 0.023407595970266624  f1= 0.005942011183151252  f2= 0.001966369823159378  f3= 0.0027028731958463367  f4= 0.037080560957557275 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2100  time= 71.47062826156616  loss= 0.0530562700906859  val_loss= 0.0543678918562395  f1= 0.006311292329877888  f2= 0.0026975423519289116  f3= 0.0096575465995872  f4= 0.034389888809291905 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2110  time= 71.79134464263916  loss= 0.05560837488071494  val_loss= 0.13858287332383884  f1= 0.008042580549576352  f2= 0.005969086543271764  f3= 0.0027031699235078224  f4= 0.038893537864359 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2120  time= 72.16622877120972  loss= 0.020278562716021128  val_loss= 0.02533284649798204  f1= 0.002537282784559457  f2= 0.0010917891795926879  f3= 0.0025664199690738736  f4= 0.014083070782795108 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2130  time= 72.5202271938324  loss= 0.037400649148055236  val_loss= 0.025767456592635508  f1= 0.0048782849174639105  f2= 0.0019008543666340567  f3= 0.0017138702985247118  f4= 0.02890763956543256 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2140  time= 72.86979579925537  loss= 0.046084285831240406  val_loss= 0.039409566740679504  f1= 0.005858995956015924  f2= 0.001532462707732959  f3= 0.007312272550687953  f4= 0.03138055461680356 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2150  time= 73.22081899642944  loss= 0.06364460432643305  val_loss= 0.04905567997681885  f1= 0.01311099518059663  f2= 0.003942577376779161  f3= 0.008856893756602124  f4= 0.03773413801245515 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2160  time= 73.57869338989258  loss= 0.08230057421038225  val_loss= 0.04740135769221356  f1= 0.006390666608763542  f2= 0.002871413859111046  f3= 0.006048336445269006  f4= 0.06699015729723866 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2170  time= 73.90047287940979  loss= 0.04012583421344353  val_loss= 0.04069394980557491  f1= 0.004356425329446891  f2= 0.0031388183976182246  f3= 0.006890640772545544  f4= 0.025739949713832866 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2180  time= 74.22736835479736  loss= 0.1496959712963222  val_loss= 0.038473757867150904  f1= 0.011612711650240854  f2= 0.0038466654568059754  f3= 0.0020884645430390533  f4= 0.1321481296462363 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2190  time= 74.55981302261353  loss= 0.05215031534335892  val_loss= 0.056066416578675865  f1= 0.011066707054451591  f2= 0.002534082313028469  f3= 0.0021236102284528722  f4= 0.03642591574742599 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2200  time= 74.89774060249329  loss= 0.07102009246478029  val_loss= 0.05914211825714467  f1= 0.005269646518791978  f2= 0.006223909899105238  f3= 0.00028560562435623097  f4= 0.05924093042252684 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2210  time= 75.2366464138031  loss= 0.027216178324168696  val_loss= 0.032454801260478436  f1= 0.007392223752982392  f2= 0.0019224972244211685  f3= 0.0017350101995920247  f4= 0.016166447147173104 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2220  time= 75.60796618461609  loss= 0.01621294616752817  val_loss= 0.023251509442656023  f1= 0.0026849002645066666  f2= 0.001781963692211538  f3= 0.002320083170278356  f4= 0.00942599904053161 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2230  time= 75.99604845046997  loss= 0.08305205911645841  val_loss= 0.08681510366367942  f1= 0.009601056614854308  f2= 0.0015276047856263584  f3= 0.01174664736246976  f4= 0.060176750353507974 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2240  time= 76.35790944099426  loss= 0.04034120249962994  val_loss= 0.055095437395974646  f1= 0.008397984125215245  f2= 0.00408090627060768  f3= 0.008649625551896392  f4= 0.019212686551910624 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2250  time= 76.73949646949768  loss= 0.027265411928864978  val_loss= 0.03295338128210188  f1= 0.0032175371029469625  f2= 0.0023581391743953887  f3= 0.0020796215019851646  f4= 0.01961011414953747 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2260  time= 77.0880651473999  loss= 0.036293096639470994  val_loss= 0.019288435473879297  f1= 0.0035391386713396755  f2= 0.004154214759764471  f3= 0.0016353636113508293  f4= 0.026964379597016013 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2270  time= 77.4228298664093  loss= 0.08699309174865155  val_loss= 0.09340150520229307  f1= 0.01574968921983937  f2= 0.0023124134410335735  f3= 0.011941557354126685  f4= 0.0569894317336519 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2280  time= 77.77347087860107  loss= 0.06975595732544071  val_loss= 0.018969384587167827  f1= 0.0048151728109123776  f2= 0.002793509407271181  f3= 0.0018948190683550955  f4= 0.06025245603890206 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2290  time= 78.13795876502991  loss= 0.036323080844136706  val_loss= 0.040680400735884276  f1= 0.004304016352600818  f2= 0.0023818341213830735  f3= 0.007750639178662645  f4= 0.021886591191490168 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2300  time= 78.48407816886902  loss= 0.17757248137668938  val_loss= 0.02948017663483042  f1= 0.008413550206899054  f2= 0.013747771127139138  f3= 0.00397673569534688  f4= 0.15143442434730436 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2310  time= 78.83416557312012  loss= 0.2025361629757151  val_loss= 0.05799250739831877  f1= 0.021824301365589428  f2= 0.009206587707525542  f3= 0.005540297677032474  f4= 0.1659649762255677 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2320  time= 79.20101475715637  loss= 0.02206283164942284  val_loss= 0.033318266208481745  f1= 0.004536723708380055  f2= 0.0019162692626819077  f3= 0.002464546874405856  f4= 0.013145291803955022 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2330  time= 79.54431629180908  loss= 0.03462757362249588  val_loss= 0.0379832748555999  f1= 0.00354728978491148  f2= 0.0035955425294389907  f3= 0.001973307028485968  f4= 0.025511434279659444 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2340  time= 79.90559768676758  loss= 0.04209935945784885  val_loss= 0.047646457863893696  f1= 0.0061463378280237685  f2= 0.0029652286781567962  f3= 0.009913633453930095  f4= 0.023074159497738198 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2350  time= 80.25868463516235  loss= 0.030380164431523592  val_loss= 0.020251618324825818  f1= 0.003404781937787576  f2= 0.00208066791544619  f3= 0.002162614412152294  f4= 0.02273210016613753 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2360  time= 80.58934783935547  loss= 0.015221048275067225  val_loss= 0.015280604803540626  f1= 0.0023362687402188292  f2= 0.0017944234713162211  f3= 0.0029535000165255266  f4= 0.008136856047006648 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2370  time= 80.91403841972351  loss= 0.09905209675837416  val_loss= 0.04093177617520764  f1= 0.012450392553814374  f2= 0.0033860414241812797  f3= 0.01265886727142431  f4= 0.07055679550895418 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2380  time= 81.23031497001648  loss= 0.05101553658454614  val_loss= 0.03979295647919007  f1= 0.0094463329173658  f2= 0.0008904391756795706  f3= 0.0020458289379053037  f4= 0.03863293555359546 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2390  time= 81.53607082366943  loss= 0.025039419954958245  val_loss= 0.01740993436260431  f1= 0.00256850541498033  f2= 0.0015092255758787547  f3= 0.0015766074253345617  f4= 0.0193850815387646 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2400  time= 81.90344452857971  loss= 0.08893009680781434  val_loss= 0.02617867366563153  f1= 0.010552664723313343  f2= 0.004773079941027198  f3= 0.001853043661487365  f4= 0.07175130848198645 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2410  time= 82.25205516815186  loss= 0.06074331426275489  val_loss= 0.06920847473694829  f1= 0.00979084644381281  f2= 0.005377766518621748  f3= 0.005047615656926911  f4= 0.04052708564339341 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2420  time= 82.59727478027344  loss= 0.015615329172739098  val_loss= 0.017547598066772307  f1= 0.0037549973447518117  f2= 0.0009877502008693137  f3= 0.005234644112900978  f4= 0.0056379375142169945 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2430  time= 82.95171356201172  loss= 0.029823417653273645  val_loss= 0.031728929896222446  f1= 0.007163200987546139  f2= 0.0025297741123019393  f3= 0.00368390091920141  f4= 0.01644654163422415 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2440  time= 83.28531241416931  loss= 0.05409208806361251  val_loss= 0.03177766260144031  f1= 0.006446527777876008  f2= 0.0046177607409432055  f3= 0.0024130906141451827  f4= 0.04061470893064811 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2450  time= 83.622398853302  loss= 0.03134863594455888  val_loss= 0.018803487562290937  f1= 0.0037843396256396016  f2= 0.002381362657479747  f3= 0.010016394406253593  f4= 0.015166539255185933 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2460  time= 83.99340295791626  loss= 0.01685723953983768  val_loss= 0.023992662626791502  f1= 0.0031349803150952584  f2= 0.0014268517015528024  f3= 0.00390193885662966  f4= 0.008393468666559963 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2470  time= 84.35524225234985  loss= 0.043451168103072486  val_loss= 0.016511706554685663  f1= 0.002738629727367836  f2= 0.009223565530853238  f3= 0.0011275118541845424  f4= 0.030361460990666878 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2480  time= 84.71099710464478  loss= 0.6306527303196674  val_loss= 0.06126768026846087  f1= 0.016694248061287865  f2= 0.007370291093506509  f3= 0.012738036164480833  f4= 0.5938501550003922 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2490  time= 85.1023805141449  loss= 0.03893326956893395  val_loss= 0.03694173175123672  f1= 0.007693328133859483  f2= 0.0027130946764809326  f3= 0.0050492902535711  f4= 0.023477556505022437 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2500  time= 85.42212057113647  loss= 0.026159609317950926  val_loss= 0.052078661582026065  f1= 0.004589538056995498  f2= 0.006458315157894152  f3= 0.0006233706428983193  f4= 0.014488385460162961 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2510  time= 85.75921154022217  loss= 0.05299053770068956  val_loss= 0.02659400937327964  f1= 0.006536264053517899  f2= 0.00293104389448812  f3= 0.0027212856160474795  f4= 0.04080194413663605 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2520  time= 86.08559060096741  loss= 0.13381704992032722  val_loss= 0.08839454558667628  f1= 0.008188447219541711  f2= 0.012095292919604412  f3= 0.0020542219505163433  f4= 0.11147908783066475 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2530  time= 86.41580319404602  loss= 0.017236603631939077  val_loss= 0.01786431960661123  f1= 0.002175166027595978  f2= 0.0017523254152607155  f3= 0.001853877678864371  f4= 0.011455234510218014 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2540  time= 86.7279953956604  loss= 0.024281289039559586  val_loss= 0.023540009954435694  f1= 0.005875641604841279  f2= 0.001905935240067442  f3= 0.0048754116144348065  f4= 0.011624300580216055 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2550  time= 87.0813467502594  loss= 0.07858303590323582  val_loss= 0.031416783252186394  f1= 0.006227936651613703  f2= 0.0025309407456987564  f3= 0.00142968768265389  f4= 0.06839447082326948 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2560  time= 87.4103364944458  loss= 0.04108929977302483  val_loss= 0.020360688343305802  f1= 0.005226923789078522  f2= 0.0011531197470237568  f3= 0.0008058460061599868  f4= 0.033903410230762564 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2570  time= 87.75834822654724  loss= 0.13285399453889976  val_loss= 0.034224149802874776  f1= 0.005893406130898035  f2= 0.006477284559272664  f3= 0.0012683438208234818  f4= 0.11921496002790559 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2580  time= 88.0947892665863  loss= 0.12142809482058241  val_loss= 0.13728162516289927  f1= 0.007754503926087113  f2= 0.012851369520004647  f3= 0.013614769548624062  f4= 0.08720745182586659 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2590  time= 88.41825222969055  loss= 0.029942292201918408  val_loss= 0.038697175333069524  f1= 0.004898715059897221  f2= 0.0021750347911110414  f3= 0.0024131223082131414  f4= 0.020455420042697004 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2600  time= 88.72285604476929  loss= 0.037137580438589446  val_loss= 0.017208135062106483  f1= 0.003814359218081351  f2= 0.002240865801720612  f3= 0.0004830728572979394  f4= 0.030599282561489544 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2610  time= 89.04524660110474  loss= 0.12595893782689568  val_loss= 0.016775069015455155  f1= 0.006414280908392065  f2= 0.004371125653514647  f3= 0.0010794622478329404  f4= 0.11409406901715603 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2620  time= 89.36158323287964  loss= 0.022390667255448863  val_loss= 0.030612083723837523  f1= 0.0034957122831296614  f2= 0.0028611299384368027  f3= 0.0010112010127574299  f4= 0.01502262402112497 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2630  time= 89.6794445514679  loss= 0.11655660428144282  val_loss= 0.023866526079293897  f1= 0.004613602566551433  f2= 0.014920672668306873  f3= 0.0040735732165708425  f4= 0.09294875583001368 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2640  time= 90.00032019615173  loss= 0.09290961179306752  val_loss= 0.027878187528687343  f1= 0.004766502779057711  f2= 0.005211936485063899  f3= 0.0017778898211675664  f4= 0.08115328270777834 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2650  time= 90.30591344833374  loss= 0.08347414318574155  val_loss= 0.03885740892190574  f1= 0.009867731223552641  f2= 0.006044419098436016  f3= 0.0008906763463258667  f4= 0.06667131651742701 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2660  time= 90.61851048469543  loss= 0.03888536312884492  val_loss= 0.020660512254579842  f1= 0.003082216821732284  f2= 0.0029801648233019865  f3= 0.004486162785401802  f4= 0.028336818698408848 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2670  time= 90.9352662563324  loss= 0.09521558892261257  val_loss= 0.04488277096205117  f1= 0.006303246582603239  f2= 0.005410466106128207  f3= 0.0005075967598007088  f4= 0.08299427947408043 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2680  time= 91.25111436843872  loss= 0.08591028987244691  val_loss= 0.06256440870162716  f1= 0.008303103103372707  f2= 0.0029173878363565314  f3= 0.00282074337017507  f4= 0.0718690555625426 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2690  time= 91.57083439826965  loss= 0.03367328407053568  val_loss= 0.03464407054326506  f1= 0.002705559110838923  f2= 0.003192528575191185  f3= 0.0017526847429586425  f4= 0.02602251164154694 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2700  time= 91.89641523361206  loss= 0.02975865387462068  val_loss= 0.02005170488764047  f1= 0.004387135452907609  f2= 0.0015586575415447522  f3= 0.003419335462730055  f4= 0.020393525417438267 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2710  time= 92.23862743377686  loss= 0.01687990636306194  val_loss= 0.021795203192729415  f1= 0.002349479407985516  f2= 0.0015527756801227636  f3= 0.0011828168855235802  f4= 0.011794834389430084 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2720  time= 92.58086347579956  loss= 0.022473858451922527  val_loss= 0.019423405515542284  f1= 0.0027999604142569484  f2= 0.001682604909100019  f3= 0.0005488962227216217  f4= 0.017442396905843937 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2730  time= 92.90018486976624  loss= 0.022126310884498138  val_loss= 0.019314928917261828  f1= 0.004911913836750695  f2= 0.0016860459913855136  f3= 0.0011747178990043512  f4= 0.014353633157357576 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2740  time= 93.22339105606079  loss= 0.0734428292893634  val_loss= 0.03122942640559723  f1= 0.004186708534593959  f2= 0.005212893720028436  f3= 0.0035652162097043467  f4= 0.06047801082503665 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2750  time= 93.55145168304443  loss= 0.040052012983994446  val_loss= 0.03096157015673466  f1= 0.004642265663310864  f2= 0.004719764059779501  f3= 0.00041370191384616984  f4= 0.030276281347057916 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2760  time= 93.88244891166687  loss= 0.062327639121549344  val_loss= 0.0357791474955942  f1= 0.0033427531406408774  f2= 0.0029149789445411215  f3= 0.00971286022059753  f4= 0.04635704681576982 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2770  time= 94.23935103416443  loss= 0.08514463849397506  val_loss= 0.04498912638559049  f1= 0.007281060759737443  f2= 0.005319294405131565  f3= 0.0070283199803655855  f4= 0.06551596334874048 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2780  time= 94.5844075679779  loss= 0.06433678111397821  val_loss= 0.0418752846259068  f1= 0.004491731093413604  f2= 0.004488988536172426  f3= 0.003521367035490804  f4= 0.05183469444890137 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2790  time= 94.93598127365112  loss= 0.05775816327305373  val_loss= 0.019334402099540706  f1= 0.002537652768249838  f2= 0.003990353730732092  f3= 0.0034939846364837753  f4= 0.04773617213758802 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2800  time= 95.30918002128601  loss= 0.02695705804920551  val_loss= 0.0343887544939345  f1= 0.003685091708305019  f2= 0.0011490553866087751  f3= 0.0009634357540712031  f4= 0.021159475200220514 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2810  time= 95.66134309768677  loss= 0.08402369519158108  val_loss= 0.033887238275310046  f1= 0.01050175207261756  f2= 0.002328997854585068  f3= 0.0005309680214768388  f4= 0.07066197724290162 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2820  time= 96.03248190879822  loss= 0.030701497123804433  val_loss= 0.024502939910891884  f1= 0.004408472305875034  f2= 0.001746596847261646  f3= 0.0013988681820947836  f4= 0.023147559788572965 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2830  time= 96.39787030220032  loss= 0.07303370366179403  val_loss= 0.01720119716073424  f1= 0.003720701208227732  f2= 0.005478496635034544  f3= 0.0011419192577915831  f4= 0.06269258656074017 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2840  time= 96.73586845397949  loss= 0.05103944655973939  val_loss= 0.057195841674531865  f1= 0.013484284835251223  f2= 0.00412627730387868  f3= 0.003208485731651776  f4= 0.03022039868895771 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2850  time= 97.06205224990845  loss= 0.05981720315498983  val_loss= 0.08352422128255543  f1= 0.002321067375945993  f2= 0.004856307776103967  f3= 0.011146122198293557  f4= 0.041493705804646314 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2860  time= 97.40942025184631  loss= 0.05783900125976261  val_loss= 0.08777477503645974  f1= 0.00831124051146194  f2= 0.0026511802144842025  f3= 0.008239526349290871  f4= 0.0386370541845256 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2870  time= 97.73997759819031  loss= 0.14773440266613944  val_loss= 0.034200522773884495  f1= 0.0038061248003250264  f2= 0.01284796517445277  f3= 0.00138700827613547  f4= 0.12969330441522617 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2880  time= 98.08509039878845  loss= 0.07340251147157902  val_loss= 0.058560473859426664  f1= 0.011613462487197359  f2= 0.004490773256238712  f3= 0.004918490743278377  f4= 0.05237978498486456 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2890  time= 98.40840816497803  loss= 0.012198109972891892  val_loss= 0.02844521082332962  f1= 0.003386513675592468  f2= 0.0008132453367708818  f3= 0.002244713599819351  f4= 0.0057536373607091915 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2900  time= 98.706303358078  loss= 0.03037906371667365  val_loss= 0.02380483317331205  f1= 0.0071050794409740865  f2= 0.0019299245628041006  f3= 0.004680372662527466  f4= 0.016663687050367994 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2910  time= 99.01493763923645  loss= 0.022402530972577092  val_loss= 0.017233236044100424  f1= 0.0024277199735905568  f2= 0.001290134029847927  f3= 0.0026873670850043586  f4= 0.01599730988413425 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2920  time= 99.33569240570068  loss= 0.0715273067519269  val_loss= 0.06537291602494427  f1= 0.00822312635905405  f2= 0.00329738551572525  f3= 0.003988608769263788  f4= 0.056018186107883804 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2930  time= 99.65099287033081  loss= 0.03821908273185036  val_loss= 0.05330933469813136  f1= 0.003051128364198116  f2= 0.005023131923758995  f3= 0.00398983188954677  f4= 0.026154990554346478 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2940  time= 99.96543598175049  loss= 0.01747956180667767  val_loss= 0.019792060114904374  f1= 0.0025655470290114643  f2= 0.0013738979544644337  f3= 0.0026085876928359803  f4= 0.010931529130365792 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2950  time= 100.34328722953796  loss= 0.050984479660088594  val_loss= 0.019722896984938322  f1= 0.0049827712615115064  f2= 0.002679542806774324  f3= 0.00238730432797519  f4= 0.04093486126382758 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2960  time= 100.69122266769409  loss= 0.09087489515549002  val_loss= 0.03180572104662041  f1= 0.006479054562148135  f2= 0.005081361247087393  f3= 0.0008091912795418027  f4= 0.0785052880667127 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2970  time= 101.03637528419495  loss= 0.058268959368508955  val_loss= 0.02676892777188724  f1= 0.0052284160699881755  f2= 0.0021638200525459336  f3= 0.004409023407451053  f4= 0.04646769983852378 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2980  time= 101.35395884513855  loss= 0.04185118760637022  val_loss= 0.020613270957310758  f1= 0.004685161201058041  f2= 0.0019272387681730366  f3= 4.1027201928410284e-05  f4= 0.03519776043521074 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 2990  time= 101.66016578674316  loss= 0.016534371656508663  val_loss= 0.021916299684000023  f1= 0.002954897306944129  f2= 0.0011483827520550416  f3= 0.00038880603159796237  f4= 0.012042285565911531 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3000  time= 101.98624062538147  loss= 0.02721520283174106  val_loss= 0.04246130788576039  f1= 0.00567535804061614  f2= 0.0019332438175035259  f3= 0.0014149646344897406  f4= 0.018191636339131654 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3010  time= 102.33475637435913  loss= 0.01637976310643697  val_loss= 0.019674200252338715  f1= 0.0032245091536240284  f2= 0.001051529493117409  f3= 0.0019170987985970272  f4= 0.010186625661098505 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3020  time= 102.67409014701843  loss= 0.05336281325663476  val_loss= 0.027637553897309756  f1= 0.00276873612035203  f2= 0.0035958044455084268  f3= 0.004105943639092452  f4= 0.04289232905168185 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3030  time= 103.01926636695862  loss= 0.015212499566531963  val_loss= 0.036555755034470484  f1= 0.0023195612520698944  f2= 0.001450593010170225  f3= 0.0021149739337070956  f4= 0.00932737137058475 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3040  time= 103.36385941505432  loss= 0.06801079113730583  val_loss= 0.061713304374682296  f1= 0.006722266126504412  f2= 0.002609180626713645  f3= 0.005682416483994109  f4= 0.052996927900093665 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3050  time= 103.72339153289795  loss= 0.03843775898232429  val_loss= 0.057369977179443865  f1= 0.0026581503688681775  f2= 0.00277545601505599  f3= 0.0007712850562984055  f4= 0.032232867542101716 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3060  time= 104.0907769203186  loss= 0.05187420804602907  val_loss= 0.01862208156588877  f1= 0.005613535843623982  f2= 0.002329932857974707  f3= 0.0026207139904046418  f4= 0.04131002535402574 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3070  time= 104.44815063476562  loss= 0.0678820463204997  val_loss= 0.018498317176694135  f1= 0.0034681580704569548  f2= 0.0036219117850203406  f3= 0.0002006645956681537  f4= 0.06059131186935426 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3080  time= 104.78610587120056  loss= 0.02534328446690293  val_loss= 0.018256877441823714  f1= 0.0024610819544134336  f2= 0.0018953405934430867  f3= 0.0018990368636496273  f4= 0.019087825055396784 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3090  time= 105.13948059082031  loss= 0.024496735043826934  val_loss= 0.027190478873640556  f1= 0.0027988530202482883  f2= 0.002132489446609612  f3= 0.0014249505902712747  f4= 0.01814044198669776 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3100  time= 105.50489401817322  loss= 0.025772526418253654  val_loss= 0.02943844524364047  f1= 0.0037171970879113793  f2= 0.0016683812637259508  f3= 0.0067689131199477155  f4= 0.013618034946668604 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3110  time= 105.86374092102051  loss= 0.13325204624134182  val_loss= 0.02515460486400331  f1= 0.0033453105371868616  f2= 0.010774528331907937  f3= 0.0012732818818392319  f4= 0.11785892549040779 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3120  time= 106.24062848091125  loss= 0.08563018525876782  val_loss= 0.03074961947487756  f1= 0.010233711610607415  f2= 0.0026751760823566432  f3= 0.001025307741063648  f4= 0.07169598982474011 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3130  time= 106.6092255115509  loss= 0.048316621249470504  val_loss= 0.060136180817410734  f1= 0.004727916181097935  f2= 0.001697438722746295  f3= 0.009035307004328107  f4= 0.03285595934129817 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3140  time= 106.96238899230957  loss= 0.07219823529319226  val_loss= 0.032447379055523175  f1= 0.006006687281632984  f2= 0.002793636540665282  f3= 0.0020147264535523807  f4= 0.061383185017341614 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3150  time= 107.29724359512329  loss= 0.00916882041697078  val_loss= 0.02620615474407498  f1= 0.0018551582844809494  f2= 0.0012503971153957603  f3= 0.00027654322534829745  f4= 0.005786721791745772 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3160  time= 107.62784576416016  loss= 0.029114435798936755  val_loss= 0.0342785556259733  f1= 0.003466353002527742  f2= 0.0015409270490037731  f3= 0.007273737313594787  f4= 0.016833418433810452 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3170  time= 107.95497059822083  loss= 0.03663506905495655  val_loss= 0.024003614426696865  f1= 0.005843844796912516  f2= 0.0017570690259438315  f3= 0.005167026173999203  f4= 0.023867129058100997 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3180  time= 108.2791256904602  loss= 0.051528104501318006  val_loss= 0.050991806315664676  f1= 0.004808365950370274  f2= 0.0024339399176152164  f3= 0.005515782183376357  f4= 0.038770016449956174 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3190  time= 108.61735033988953  loss= 0.036309656984061765  val_loss= 0.031292626075982016  f1= 0.004957427418706325  f2= 0.004711106417607073  f3= 0.0035185407695471223  f4= 0.02312258237820124 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3200  time= 108.93107676506042  loss= 0.05145930886330802  val_loss= 0.039084662143069056  f1= 0.0028116413958264117  f2= 0.0035360923662977103  f3= 0.009022576479318757  f4= 0.036088998621865154 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3210  time= 109.2300009727478  loss= 0.03665005755394211  val_loss= 0.0266842880485183  f1= 0.0020646654746635457  f2= 0.003692093286418622  f3= 0.0009211371724372392  f4= 0.029972161620422696 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3220  time= 109.5433132648468  loss= 0.043347936999107606  val_loss= 0.09698580700072414  f1= 0.004219849489518171  f2= 0.004074943690017589  f3= 0.0029303602915723656  f4= 0.03212278352799947 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3230  time= 109.87551689147949  loss= 0.2017482841445373  val_loss= 0.04553475136167139  f1= 0.002511403861595717  f2= 0.012201451500282729  f3= 0.0021034735029491747  f4= 0.1849319552797097 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3240  time= 110.18683338165283  loss= 0.07318332773676013  val_loss= 0.022530518520997137  f1= 0.00888160609051255  f2= 0.004908023179997304  f3= 0.0014905121803776044  f4= 0.05790318628587266 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3250  time= 110.5141773223877  loss= 0.10153245540296402  val_loss= 0.06001970259166692  f1= 0.0059188836876741474  f2= 0.01065626243002251  f3= 0.0002323484305345449  f4= 0.08472496085473281 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3260  time= 110.82879161834717  loss= 0.06623377659686959  val_loss= 0.01846166938965491  f1= 0.002845112973776069  f2= 0.0034839317645299333  f3= 0.001462456920643597  f4= 0.05844227493791999 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3270  time= 111.1331434249878  loss= 0.05168834289584792  val_loss= 0.08108102793761471  f1= 0.002147104485553108  f2= 0.0032549561885212848  f3= 0.010148389399771205  f4= 0.036137892822002325 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3280  time= 111.48396563529968  loss= 0.10001987740692204  val_loss= 0.057801753242534434  f1= 0.012883550678773663  f2= 0.005328598402620032  f3= 0.004128017123657479  f4= 0.07767971120187088 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3290  time= 111.81723594665527  loss= 0.34034954637834924  val_loss= 0.13729361853728453  f1= 0.006926247936137699  f2= 0.07305429551694637  f3= 0.008553858442400024  f4= 0.2518151444828651 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3300  time= 112.1576189994812  loss= 0.03437113195183789  val_loss= 0.04582201037457384  f1= 0.004862114198022748  f2= 0.0016138378842115294  f3= 0.0009965219882281267  f4= 0.02689865788137549 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3310  time= 112.5103018283844  loss= 0.02246757407509931  val_loss= 0.028321856493736978  f1= 0.003901809038164892  f2= 0.002017564643823621  f3= 0.000323443347797139  f4= 0.01622475704531366 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3320  time= 112.8734028339386  loss= 0.009674704255589767  val_loss= 0.015766751189471434  f1= 0.0021816409279981704  f2= 0.0009089548015557528  f3= 0.002190907475135168  f4= 0.004393201050900676 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3330  time= 113.2373514175415  loss= 0.027477873244033803  val_loss= 0.012806939143966953  f1= 0.003158038441226292  f2= 0.001118049732470383  f3= 3.95565147507648e-05  f4= 0.023162228555586358 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3340  time= 113.56032514572144  loss= 0.06209122999808639  val_loss= 0.0742075679610445  f1= 0.0035235840247096043  f2= 0.005676773592986427  f3= 0.00541719406575472  f4= 0.04747367831463564 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3350  time= 113.96249580383301  loss= 0.028392213263884126  val_loss= 0.023361669399218184  f1= 0.002659049406723584  f2= 0.0018085156751360418  f3= 0.0009646186202389022  f4= 0.022960029561785594 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3360  time= 114.32268857955933  loss= 0.055115755703322646  val_loss= 0.07523546777285901  f1= 0.005407152117433363  f2= 0.0038828701565199104  f3= 0.007735013175507441  f4= 0.03809072025386194 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3370  time= 114.67983508110046  loss= 0.06850878062081797  val_loss= 0.017626807302237715  f1= 0.00249094160822414  f2= 0.006692420137907193  f3= 0.001708406882456025  f4= 0.05761701199223062 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3380  time= 115.02172422409058  loss= 0.02615947437759029  val_loss= 0.023932848744799944  f1= 0.002660097473566363  f2= 0.0015194057466814276  f3= 0.003263967143490336  f4= 0.018716004013852167 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3390  time= 115.36561870574951  loss= 0.03295427992986521  val_loss= 0.07413297746811937  f1= 0.005184859466582392  f2= 0.0009514292181606894  f3= 0.011036716604111851  f4= 0.015781274641010274 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3400  time= 115.6909294128418  loss= 0.1310791404494574  val_loss= 0.03239036356197012  f1= 0.009578865042142773  f2= 0.005621485484510144  f3= 0.0016486179654551993  f4= 0.11423017195734926 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3410  time= 116.00470066070557  loss= 0.02521938206940956  val_loss= 0.023418984338804544  f1= 0.002614429009286046  f2= 0.0013281166089182022  f3= 0.0028104468019459245  f4= 0.018466389649259382 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3420  time= 116.32655239105225  loss= 0.11016627165406555  val_loss= 0.01355114012120422  f1= 0.0037743958310261605  f2= 0.008464808886816987  f3= 0.0008312404018086085  f4= 0.09709582653441379 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3430  time= 116.69872045516968  loss= 0.01984806630475106  val_loss= 0.0224366497397479  f1= 0.0029225863468183215  f2= 0.001951184932076449  f3= 0.00023707992171406365  f4= 0.014737215104142222 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3440  time= 117.02888917922974  loss= 0.02162014965275223  val_loss= 0.019633107697707944  f1= 0.001157477689944496  f2= 0.0022894202867561387  f3= 0.0007410004467031695  f4= 0.01743225122934842 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3450  time= 117.34789967536926  loss= 0.06524460570421502  val_loss= 0.05848859258201886  f1= 0.006183058764212807  f2= 0.0037101227055220173  f3= 0.006879729707874103  f4= 0.0484716945266061 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3460  time= 117.7380211353302  loss= 0.020458697605666574  val_loss= 0.03567281569487791  f1= 0.002643384567102621  f2= 0.001188380210465974  f3= 0.0028831741930352133  f4= 0.013743758635062768 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3470  time= 118.09395718574524  loss= 0.02260495469370369  val_loss= 0.020129150758513646  f1= 0.002552594709126284  f2= 0.0008273027037212917  f3= 0.0035946917890018492  f4= 0.01563036549185427 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3480  time= 118.4570562839508  loss= 0.03181644063879587  val_loss= 0.03850809316260283  f1= 0.004156014039632023  f2= 0.001100359925876906  f3= 0.0052862145668641424  f4= 0.021273852106422807 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3490  time= 118.80709958076477  loss= 0.008390681543831486  val_loss= 0.020996006036587675  f1= 0.001912628027507146  f2= 0.0008588878949959744  f3= 0.00021820003196811535  f4= 0.005400965589360249 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3500  time= 119.14291858673096  loss= 0.08027428751491696  val_loss= 0.06414402100836189  f1= 0.006631857164155391  f2= 0.0052274106168979685  f3= 0.008942032156920854  f4= 0.05947298757694274 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3510  time= 119.47443723678589  loss= 0.03240256206001578  val_loss= 0.06547372055773273  f1= 0.006216348969490874  f2= 0.0015220167455426788  f3= 0.006360470161173758  f4= 0.018303726183808475 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3520  time= 119.79688715934753  loss= 0.020897146400909315  val_loss= 0.019345223131090442  f1= 0.003486871592391144  f2= 0.0019277575921374825  f3= 0.00012839321665906087  f4= 0.01535412399972163 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3530  time= 120.10507941246033  loss= 0.3325822398504709  val_loss= 0.01632860490999784  f1= 0.003866867627354545  f2= 0.005873963239772014  f3= 0.0009961346602187246  f4= 0.32184527432312565 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3540  time= 120.41419506072998  loss= 0.032455692945918636  val_loss= 0.02880547123668569  f1= 0.0011097754845238513  f2= 0.0027430338543824426  f3= 0.00021709995686106784  f4= 0.028385783650151267 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3550  time= 120.78364443778992  loss= 0.026831779307180878  val_loss= 0.02109524240077302  f1= 0.0031814382426475717  f2= 0.0018721159007323567  f3= 0.0007776263600881942  f4= 0.021000598803712748 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3560  time= 121.13980603218079  loss= 0.057912606518230346  val_loss= 0.03311985454227991  f1= 0.0076700888389136775  f2= 0.0051275351502893035  f3= 0.0024509388697287466  f4= 0.04266404365929862 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3570  time= 121.48489689826965  loss= 0.043788425391781814  val_loss= 0.06711876588709734  f1= 0.003318428566827476  f2= 0.002651980508059425  f3= 0.00595621413891593  f4= 0.03186180217797898 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3580  time= 121.81171035766602  loss= 0.030343107051597124  val_loss= 0.03103171609763488  f1= 0.00413334650167223  f2= 0.0014992751644084203  f3= 0.0027041438645131494  f4= 0.022006341521003323 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3590  time= 122.13349890708923  loss= 0.031781443279224005  val_loss= 0.04595609253411782  f1= 0.002537738250594479  f2= 0.002198905228784958  f3= 0.0015659229463144738  f4= 0.02547887685353009 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3600  time= 122.47634506225586  loss= 0.03790662941161932  val_loss= 0.025682527956657514  f1= 0.003997167477303208  f2= 0.0018640492638119927  f3= 0.0033012565314237733  f4= 0.02874415613908035 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3610  time= 122.78204822540283  loss= 0.0447225619365521  val_loss= 0.050616805120681244  f1= 0.0056065905554550585  f2= 0.0043310372154890696  f3= 0.0009191152473596786  f4= 0.033865818918248296 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3620  time= 123.1142930984497  loss= 0.10142261508405659  val_loss= 0.04384855935254002  f1= 0.009691037804251398  f2= 0.005150241214635291  f3= 0.003483340853041011  f4= 0.08309799521212888 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3630  time= 123.4522397518158  loss= 0.016914397637454285  val_loss= 0.020087560256840647  f1= 0.004446185985024877  f2= 0.0013225239951308268  f3= 0.002987659810224707  f4= 0.008158027847073875 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3640  time= 123.7902295589447  loss= 0.15642040156598938  val_loss= 0.06671563400490543  f1= 0.01109448746472517  f2= 0.0076445754380634985  f3= 0.01582275454475597  f4= 0.12185858411844473 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3650  time= 124.17303943634033  loss= 0.021375417843895034  val_loss= 0.05344197568256721  f1= 0.0026524372331833464  f2= 0.001368909196342915  f3= 0.0011390921086065658  f4= 0.016214979305762208 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3660  time= 124.51142692565918  loss= 0.033681575325769327  val_loss= 0.018191232061607585  f1= 0.0028574072717024277  f2= 0.001937648704086363  f3= 0.0002373536893869691  f4= 0.02864916566059356 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3670  time= 124.85986614227295  loss= 0.06370404249770012  val_loss= 0.019861193035506472  f1= 0.0021368931395349617  f2= 0.004902149731138274  f3= 0.0009898091816532553  f4= 0.05567519044537364 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3680  time= 125.19155931472778  loss= 0.04619102199407255  val_loss= 0.04190430272074151  f1= 0.00762687487974295  f2= 0.0018532653700181819  f3= 0.016364829906136757  f4= 0.020346051838174662 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3690  time= 125.52826857566833  loss= 0.07078048998051441  val_loss= 0.02590999021105335  f1= 0.0034226174988271547  f2= 0.004240051611984411  f3= 0.0005527180228438751  f4= 0.06256510284685897 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3700  time= 125.84645557403564  loss= 0.07476898726737745  val_loss= 0.03165003321607843  f1= 0.004436770952228582  f2= 0.007445257421260472  f3= 0.004292461962389474  f4= 0.05859449693149893 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3710  time= 126.1674256324768  loss= 0.012144032989394643  val_loss= 0.017070760938364175  f1= 0.0019502251779333877  f2= 0.0015232012106918217  f3= 0.0035112357687228906  f4= 0.005159370832046545 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3720  time= 126.50930261611938  loss= 0.029606213771617314  val_loss= 0.02957102151029115  f1= 0.004856070842561024  f2= 0.0019502884336363676  f3= 0.003794145506307227  f4= 0.019005708989112698 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3730  time= 126.83231997489929  loss= 0.034487882626044186  val_loss= 0.0424425292442267  f1= 0.003907781582546265  f2= 0.0016830526751452706  f3= 0.0011032843637210528  f4= 0.027793764004631596 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3740  time= 127.15896391868591  loss= 0.09122373978100128  val_loss= 0.045605064639323456  f1= 0.0036499773532304423  f2= 0.0072125432208655195  f3= 0.002123088807455043  f4= 0.07823813039945027 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3750  time= 127.48674511909485  loss= 0.09022067908237101  val_loss= 0.02092715642065828  f1= 0.005147035681586039  f2= 0.0031139879277017612  f3= 0.0016103020658856705  f4= 0.08034935340719752 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3760  time= 127.85406398773193  loss= 0.02631964085768468  val_loss= 0.03669379692754679  f1= 0.0036187074766493143  f2= 0.0017644207481705525  f3= 7.204030095801165e-05  f4= 0.0208644723319068 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3770  time= 128.21016335487366  loss= 0.10589983200783247  val_loss= 0.039343110638016925  f1= 0.0071220925788984465  f2= 0.004262675917195023  f3= 0.0009565409195584537  f4= 0.09355852259218052 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3780  time= 128.56779980659485  loss= 0.06338477726187954  val_loss= 0.032087625217761026  f1= 0.006899375516408369  f2= 0.002894558647792427  f3= 0.002765371054711731  f4= 0.050825472042967 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3790  time= 128.9023311138153  loss= 0.03607106942226033  val_loss= 0.027347592163204714  f1= 0.004039291907663843  f2= 0.0019353407560284122  f3= 0.007051200701713565  f4= 0.023045236056854504 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3800  time= 129.27128744125366  loss= 0.03835652535479638  val_loss= 0.06961881489652957  f1= 0.0026281481080362417  f2= 0.0028481077105205047  f3= 0.00823648337283019  f4= 0.024643786163409443 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 3810  time= 129.62007117271423  loss= 0.02006436147796756  val_loss= 0.01441103602676586  f1= 0.0024022070683677546  f2= 0.001273182979407736  f3= 0.0002605734008158523  f4= 0.01612839802937622 num_batches= 6 percent lr= 0.01\n","EarlyStopping counter: 996 out of 1000\n","EarlyStopping counter: 997 out of 1000\n","EarlyStopping counter: 998 out of 1000\n","EarlyStopping counter: 999 out of 1000\n","EarlyStopping counter: 1000 out of 1000\n","Early Stopping\n","training PINN Net\n"," \n","epoch= 0  time= 0.017470598220825195  loss= 8.337060764470317  val_loss= 11.060962534794388  f1= 6.0755139961816385  f2= 1.9485107523758876  f3= 0.29906043315782893  f4= 0.013975582754963323 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 10  time= 0.25441622734069824  loss= 7.469675315694434  val_loss= 4.938356963821958  f1= 1.1961535176254539  f2= 1.9444934636991131  f3= 0.2238035796548629  f4= 4.105224754715004 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 20  time= 0.4366037845611572  loss= 5.0299628426779  val_loss= 3.5258738199853408  f1= 0.9613416724091356  f2= 1.9297122949765406  f3= 0.003396790920829724  f4= 2.135512084371394 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 30  time= 0.6481375694274902  loss= 9.017313185459889  val_loss= 4.593713642149037  f1= 1.1150020616931575  f2= 4.411576903748357  f3= 0.04198301493423349  f4= 3.448751205084141 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 40  time= 0.8599536418914795  loss= 3.9794600900361297  val_loss= 4.303026210658471  f1= 1.155490431314406  f2= 1.9533805022419066  f3= 0.01698458616428174  f4= 0.8536045703155349 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 50  time= 1.0953845977783203  loss= 3.6605812014381396  val_loss= 2.8429132213435304  f1= 0.9087314039292197  f2= 1.9355806091267154  f3= 0.006732787418826459  f4= 0.8095364009633781 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 60  time= 1.4942846298217773  loss= 2.8243763713768844  val_loss= 1.7228342381417827  f1= 0.533832502337454  f2= 1.838131015714154  f3= 0.0045398363220428735  f4= 0.44787301700323406 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 70  time= 1.8535373210906982  loss= 5.197340773967267  val_loss= 1.4167614107714417  f1= 0.263237351147458  f2= 3.1632215477350236  f3= 0.005530718012858358  f4= 1.7653511570719271 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 80  time= 2.240020513534546  loss= 3.9125524959252354  val_loss= 1.1891733106012174  f1= 0.36919331428336166  f2= 3.0550512723984737  f3= 0.01628333311068795  f4= 0.47202457613271304 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 90  time= 2.8726205825805664  loss= 2.464959492026902  val_loss= 1.3890500016674876  f1= 0.2144328086729389  f2= 1.7132254609973077  f3= 0.0007028051530248942  f4= 0.5365984172036301 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 100  time= 3.543391466140747  loss= 2.2146569422177365  val_loss= 0.7618283056875101  f1= 0.15842992226241667  f2= 1.6233939832790363  f3= 0.0019054912890072698  f4= 0.4309275453872758 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 110  time= 3.835287570953369  loss= 2.5598327097806557  val_loss= 0.840437896367705  f1= 0.1426588730865128  f2= 1.635754543978601  f3= 0.01576823718075451  f4= 0.7656510555347871 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 120  time= 4.253108978271484  loss= 1.9224727693371069  val_loss= 0.700731237885182  f1= 0.12587537579657937  f2= 1.5165320369875335  f3= 0.011702852788969119  f4= 0.26836250376402454 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 130  time= 4.48331093788147  loss= 1.8245884549982094  val_loss= 0.6728288379574432  f1= 0.12295130274464766  f2= 1.4233409569583466  f3= 0.0013324862356055244  f4= 0.2769637090596098 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 140  time= 4.825809955596924  loss= 1.8459811295633781  val_loss= 0.8581971617758029  f1= 0.08132700872791528  f2= 1.328212095365436  f3= 0.01667982737399378  f4= 0.4197621980960333 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 150  time= 5.016761302947998  loss= 1.5472828399935137  val_loss= 0.4507337488807889  f1= 0.11130151495806069  f2= 1.1482102121699784  f3= 0.009398018318429248  f4= 0.2783730945470453 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 160  time= 5.711496591567993  loss= 1.4941576710963014  val_loss= 0.5153504757989512  f1= 0.07124049449232585  f2= 1.0679833926745712  f3= 0.027406355007459138  f4= 0.3275274289219453 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 170  time= 6.305442571640015  loss= 1.972229748520932  val_loss= 0.4532894224294371  f1= 0.0691142770626275  f2= 1.0780905945468167  f3= 0.00259244338768287  f4= 0.8224324335238052 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 180  time= 6.609094858169556  loss= 1.4279752226717977  val_loss= 0.412734204565099  f1= 0.15322100242285006  f2= 0.8410461158530743  f3= 0.0041365232668618005  f4= 0.42957158112901156 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 190  time= 6.998165845870972  loss= 1.3045672664846537  val_loss= 0.4177735261607379  f1= 0.05388000604557802  f2= 0.8394731809543314  f3= 0.0023282924349783286  f4= 0.40888578704976614 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 200  time= 7.344977855682373  loss= 1.6502076826994685  val_loss= 2.0902223588504416  f1= 0.0278712515762273  f2= 0.8931906867634726  f3= 0.006053457940629237  f4= 0.723092286419139 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 210  time= 7.542776346206665  loss= 1.4562328718810968  val_loss= 1.2898858410053677  f1= 0.06539868634005001  f2= 0.6533783416048943  f3= 0.007069339898412186  f4= 0.7303865040377402 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 220  time= 8.077391147613525  loss= 1.2904539094132061  val_loss= 4.091960822888551  f1= 0.03553037967302677  f2= 0.5439395523153979  f3= 0.0027572289271455695  f4= 0.7082267484976358 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 230  time= 8.267139196395874  loss= 0.8802144946705358  val_loss= 4.4401685592233635  f1= 0.02475349114200626  f2= 0.6305145980336547  f3= 0.030118548674486684  f4= 0.19482785682038806 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 240  time= 8.642658948898315  loss= 0.8058041803694801  val_loss= 11.827180328406522  f1= 0.021437880720018343  f2= 0.6013278832981553  f3= 0.001995241629077554  f4= 0.18104317472222894 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 250  time= 9.36006474494934  loss= 0.8622811857895666  val_loss= 10.545247213659337  f1= 0.04575699622423557  f2= 0.4770411800696659  f3= 0.04136026453403844  f4= 0.29812274496162644 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 260  time= 9.895690202713013  loss= 0.6851860411355234  val_loss= 17.207991019196243  f1= 0.033260789218508405  f2= 0.5225497198159016  f3= 0.0014942952040567297  f4= 0.12788123689705672 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 270  time= 10.266415357589722  loss= 1.077632817523966  val_loss= 5.710106779934392  f1= 0.03964107344215966  f2= 0.44019225343818746  f3= 0.003812105034341881  f4= 0.5939873856092769 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 280  time= 10.446621894836426  loss= 0.7274812092270474  val_loss= 4.018867308346583  f1= 0.02937865590019895  f2= 0.43396681641426277  f3= 0.0035268088168245917  f4= 0.260608928095761 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 290  time= 10.88462781906128  loss= 0.6811865573621922  val_loss= 8.605376382664948  f1= 0.05286630605414344  f2= 0.4068500074364005  f3= 0.0006013154314249293  f4= 0.22086892844022346 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 300  time= 11.143008470535278  loss= 0.7382637512196267  val_loss= 5.024372757409191  f1= 0.03558077476064212  f2= 0.3623591663000408  f3= 0.0018899834850892062  f4= 0.3384338266738545 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 310  time= 11.461722612380981  loss= 0.6678559063675279  val_loss= 6.651953505024084  f1= 0.02550472997741651  f2= 0.3410296075504971  f3= 0.03674421907531969  f4= 0.2645773497642947 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 320  time= 11.735607385635376  loss= 0.8313796942984074  val_loss= 12.073402173051328  f1= 0.023265000220686016  f2= 0.38763615173892624  f3= 0.014943721744959748  f4= 0.4055348205938354 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 330  time= 12.039756536483765  loss= 0.6109822588771064  val_loss= 4.790759286712476  f1= 0.041292060783733535  f2= 0.2861052417333315  f3= 0.0021181461628012317  f4= 0.2814668101972402 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 340  time= 12.56541895866394  loss= 0.5504725637877088  val_loss= 5.282380892474998  f1= 0.02580334890524689  f2= 0.2863844807597042  f3= 0.0049103865465941095  f4= 0.2333743475761636 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 350  time= 13.169950723648071  loss= 1.1514733733521403  val_loss= 4.59095907917388  f1= 0.02908155115993417  f2= 0.4322083191959614  f3= 0.005240743851383491  f4= 0.6849427591448611 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 360  time= 13.749366044998169  loss= 1.2642778881635826  val_loss= 6.421991519344329  f1= 0.06098149708112688  f2= 0.4592976085589566  f3= 0.004296580855044978  f4= 0.7397022016684541 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 370  time= 13.991276264190674  loss= 1.0690081957904354  val_loss= 4.48799510823813  f1= 0.021902421036605727  f2= 0.2791729314107747  f3= 0.010806471561265891  f4= 0.7571263717817889 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 380  time= 14.214129447937012  loss= 0.4709227711154677  val_loss= 4.3745705871641025  f1= 0.028455601815223955  f2= 0.2733706767980107  f3= 0.004175095223269164  f4= 0.16492139727896393 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 390  time= 14.405159950256348  loss= 0.8708399136309585  val_loss= 4.80434430571023  f1= 0.050693697335140515  f2= 0.25115401752336125  f3= 0.001149310666953248  f4= 0.5678428881055034 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 400  time= 14.594491004943848  loss= 0.3535429859663039  val_loss= 1.9551644982618919  f1= 0.016560929681055228  f2= 0.23006817051935902  f3= 0.0009931620376250044  f4= 0.10592072372826461 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 410  time= 14.80748200416565  loss= 0.504479516380785  val_loss= 2.105574955959974  f1= 0.017525488864158268  f2= 0.28094173752205753  f3= 0.006022041840210925  f4= 0.19999024815435826 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 420  time= 14.991105079650879  loss= 0.6696840345039087  val_loss= 2.739915129210323  f1= 0.025568780246187167  f2= 0.25388374098173944  f3= 0.0005810593528640791  f4= 0.38965045392311803 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 430  time= 15.195783853530884  loss= 0.6911658188032136  val_loss= 1.6688649776139854  f1= 0.016001414241777517  f2= 0.23327254615467496  f3= 0.0009478127682111637  f4= 0.44094404563855 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 440  time= 15.42160415649414  loss= 0.2919627691843539  val_loss= 1.7994637251473313  f1= 0.02449422668359696  f2= 0.18165695360331457  f3= 0.00351525885820719  f4= 0.08229633003923525 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 450  time= 15.640728950500488  loss= 0.30088867823862864  val_loss= 1.29742369265759  f1= 0.018378040638157702  f2= 0.1629183110264069  f3= 0.0008809552119769759  f4= 0.11871137136208709 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 460  time= 15.873642683029175  loss= 0.3122004284287491  val_loss= 1.8530507397281548  f1= 0.018577125400464314  f2= 0.15734106724226132  f3= 0.003600302530750585  f4= 0.1326819332552728 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 470  time= 16.092679738998413  loss= 0.2952298160713936  val_loss= 1.3480937542170983  f1= 0.02481264762691938  f2= 0.110065292556759  f3= 0.0031893902232920934  f4= 0.15716248566442312 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 480  time= 16.339099168777466  loss= 1.2583856872033004  val_loss= 1.1491754060391632  f1= 0.05591646143623578  f2= 0.8489981430549732  f3= 0.00795616889658067  f4= 0.34551491381551075 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 490  time= 16.75591254234314  loss= 0.6050536476143991  val_loss= 1.0013450121316094  f1= 0.014854166628599364  f2= 0.1343879625377172  f3= 0.0016769609589507909  f4= 0.4541345574891318 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 500  time= 17.19922423362732  loss= 0.21858790929872737  val_loss= 1.3024623840391252  f1= 0.025966227179469725  f2= 0.07260504438095948  f3= 0.0069832893739211616  f4= 0.11303334836437702 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 510  time= 17.524983406066895  loss= 0.4063538339255264  val_loss= 0.6846082128501848  f1= 0.021213075904932432  f2= 0.10746969322462234  f3= 0.0020873517172275663  f4= 0.27558371307874396 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 520  time= 17.902016162872314  loss= 0.4807947260647036  val_loss= 2.1997060233971153  f1= 0.03616170346322695  f2= 0.051126424354294424  f3= 0.0005854591697537398  f4= 0.3929211390774285 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 530  time= 18.172496795654297  loss= 0.18100075403331908  val_loss= 1.3931715096594306  f1= 0.017997094840554298  f2= 0.048968293138087655  f3= 0.00407947580309538  f4= 0.10995589025158176 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 540  time= 18.689613580703735  loss= 0.327547430422877  val_loss= 1.9698588283344833  f1= 0.01824769689324811  f2= 0.10117550583451784  f3= 0.004511023741180779  f4= 0.20361320395393023 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 550  time= 18.938889980316162  loss= 0.15246865912819924  val_loss= 1.42368777501629  f1= 0.019440204115926462  f2= 0.06427529956073792  f3= 0.0031398249591870985  f4= 0.06561333049234776 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 560  time= 19.279515504837036  loss= 0.32318751898607767  val_loss= 1.1392583988631089  f1= 0.01721856060007054  f2= 0.09018944338393685  f3= 0.001736165143046509  f4= 0.21404334985902385 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 570  time= 19.632654428482056  loss= 0.1307731678378008  val_loss= 0.7477994774940365  f1= 0.009951515503229967  f2= 0.06049705595094884  f3= 0.010572490288814863  f4= 0.049752106094807125 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 580  time= 20.01793909072876  loss= 0.2474517598492155  val_loss= 1.8424105222149694  f1= 0.07025984428175834  f2= 0.07792958817830113  f3= 0.004471000726342095  f4= 0.09479132666281392 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 590  time= 20.921185970306396  loss= 1.4925302050536973  val_loss= 0.621786527040437  f1= 0.018918433109456494  f2= 0.1520962951699227  f3= 0.0015556108972936205  f4= 1.3199598658770249 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 600  time= 21.335633516311646  loss= 0.11120150826111343  val_loss= 0.6004522531836638  f1= 0.01277650624863094  f2= 0.04498299533676736  f3= 0.0024347324413824677  f4= 0.05100727423433265 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 610  time= 21.633490085601807  loss= 0.7805416530556473  val_loss= 0.4188763714561043  f1= 0.03307564788392603  f2= 0.08950267743819558  f3= 0.00039561092984906706  f4= 0.6575677168036765 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 620  time= 22.19098973274231  loss= 0.21392442703048684  val_loss= 0.4630896176486827  f1= 0.03254175838286308  f2= 0.07121496769884035  f3= 0.01929114124319691  f4= 0.09087655970558646 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 630  time= 22.57913303375244  loss= 0.26771648668770415  val_loss= 0.5001075018705554  f1= 0.02595382712860392  f2= 0.06207569201572472  f3= 0.009896026209401027  f4= 0.16979094133397446 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 640  time= 22.842278718948364  loss= 0.3357710230353394  val_loss= 1.1069897138897569  f1= 0.03098247183910631  f2= 0.06289548918588954  f3= 0.0010382510396958124  f4= 0.24085481097064776 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 650  time= 23.142943620681763  loss= 0.19249336452851173  val_loss= 1.230609466045682  f1= 0.019876336889303767  f2= 0.04121665443606116  f3= 0.0013415791079622698  f4= 0.1300587940951845 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 660  time= 23.589449882507324  loss= 0.4446585667465845  val_loss= 0.49637683262888666  f1= 0.015117996595620603  f2= 0.05201942409712413  f3= 0.0009584753345094323  f4= 0.37656267071933036 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 670  time= 23.793100118637085  loss= 1.0104540439187228  val_loss= 0.44793368241144726  f1= 0.04602409616716336  f2= 0.1392557721812806  f3= 0.001519670673350914  f4= 0.8236545048969277 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 680  time= 24.20728850364685  loss= 0.20667434726792752  val_loss= 0.8617236916724886  f1= 0.014442766822239885  f2= 0.04466333042330899  f3= 0.0006037804966538338  f4= 0.1469644695257248 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 690  time= 24.844356298446655  loss= 0.2587807911387115  val_loss= 1.2337878611126203  f1= 0.027019469707338286  f2= 0.0473161607864781  f3= 0.002240864325771559  f4= 0.18220429631912352 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 700  time= 25.180779218673706  loss= 0.25129622014639924  val_loss= 0.3211307854400216  f1= 0.019509968639204064  f2= 0.036009736198297754  f3= 0.0016566514486562871  f4= 0.19411986386024108 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 710  time= 25.54016375541687  loss= 0.1172454741235651  val_loss= 0.17691815387309884  f1= 0.009060775244157012  f2= 0.039552078371189854  f3= 8.261124327661093e-05  f4= 0.06855000926494163 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 720  time= 25.80294370651245  loss= 0.20357314309456043  val_loss= 0.30970828087114877  f1= 0.02082993134634934  f2= 0.04772424595247277  f3= 8.204654847391401e-05  f4= 0.1349369192472644 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 730  time= 26.153297662734985  loss= 0.858932060498546  val_loss= 0.3254543202477508  f1= 0.028891557559903604  f2= 0.13819347134059504  f3= 0.00024648205565892795  f4= 0.6916005495423886 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 740  time= 26.507290840148926  loss= 0.11382782744659432  val_loss= 0.27312913319891985  f1= 0.013180461870556404  f2= 0.048202583065258126  f3= 1.2128257456822203e-05  f4= 0.05243265425332295 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 750  time= 26.912442207336426  loss= 0.16318583765321434  val_loss= 0.3306614014980773  f1= 0.03567610425212792  f2= 0.05160247475076609  f3= 0.006183109940790381  f4= 0.06972414870952992 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 760  time= 27.18811058998108  loss= 0.19746351062789172  val_loss= 0.5712201287845905  f1= 0.02482324329784938  f2= 0.04735344077702958  f3= 0.0026492568141682185  f4= 0.12263756973884456 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 770  time= 27.52156972885132  loss= 0.1627067804167082  val_loss= 0.24555372810850745  f1= 0.017327530057743516  f2= 0.036322136001735575  f3= 0.0020521434700057337  f4= 0.10700497088722337 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 780  time= 27.842790842056274  loss= 0.13720951638360596  val_loss= 0.3996053596852503  f1= 0.016613946781109192  f2= 0.030774657617515605  f3= 0.002770432616137772  f4= 0.08705047936884337 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 790  time= 28.394794940948486  loss= 0.22416555098962457  val_loss= 0.49647760542704295  f1= 0.010162232949198508  f2= 0.031504736836244294  f3= 1.2383737853393485e-05  f4= 0.1824861974663284 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 800  time= 28.599725484848022  loss= 0.1833687152568935  val_loss= 0.25232789044528137  f1= 0.01164048686731356  f2= 0.02925008035753779  f3= 0.0017931533764571912  f4= 0.14068499465558493 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 810  time= 29.03307056427002  loss= 0.12108001029513148  val_loss= 0.18198689210862068  f1= 0.00900043668613647  f2= 0.04208375513225729  f3= 2.2691124465470042e-05  f4= 0.06997312735227225 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 820  time= 29.297723054885864  loss= 0.16056133137225403  val_loss= 0.25150411460192823  f1= 0.03260705514364357  f2= 0.03685537772672016  f3= 0.0036174752240099865  f4= 0.08748142327788033 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 830  time= 29.586989879608154  loss= 0.20615080832673294  val_loss= 0.1973943637499887  f1= 0.012753209767945244  f2= 0.029380760831497044  f3= 0.011342872193186401  f4= 0.15267396553410426 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 840  time= 29.90882897377014  loss= 0.3281244879073915  val_loss= 0.34020532401067205  f1= 0.012285606112067873  f2= 0.039085349383388444  f3= 0.003997024421355383  f4= 0.27275650799057977 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 850  time= 30.14644169807434  loss= 0.37876673897246116  val_loss= 0.38511044790658716  f1= 0.02243182716172816  f2= 0.021601722411497517  f3= 0.00021386693738032844  f4= 0.33451932246185506 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 860  time= 30.60775113105774  loss= 0.23149511640090606  val_loss= 0.22570309593107027  f1= 0.014556477245339683  f2= 0.045467368013271704  f3= 0.00034460374745166905  f4= 0.17112666739484306 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 870  time= 30.857789278030396  loss= 0.20986971347128988  val_loss= 0.3534397004610797  f1= 0.010931663997964169  f2= 0.05891258289513934  f3= 0.0022652382598630466  f4= 0.13776022831832332 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 880  time= 31.256062507629395  loss= 0.13208796406090528  val_loss= 0.2237545143216915  f1= 0.016646768670009558  f2= 0.03014861243573089  f3= 0.0015036225302879243  f4= 0.08378896042487692 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 890  time= 31.481166124343872  loss= 0.20291674322620556  val_loss= 0.12126315251089242  f1= 0.014813248866450199  f2= 0.0396035546259773  f3= 0.003591703665952383  f4= 0.14490823606782569 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 900  time= 31.779416799545288  loss= 0.3314956663630289  val_loss= 0.26217911880344147  f1= 0.00963092186499537  f2= 0.03718124971930038  f3= 7.884537734627162e-05  f4= 0.2846046494013869 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 910  time= 32.10421657562256  loss= 0.08296965000008139  val_loss= 0.1263942307335792  f1= 0.012555958799875972  f2= 0.02413618723229243  f3= 0.0015200127946025238  f4= 0.04475749117331046 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 920  time= 32.979196310043335  loss= 0.11006718780294067  val_loss= 0.06837930421080447  f1= 0.008470917400181301  f2= 0.026330665931022942  f3= 0.002564504330898411  f4= 0.07270110014083801 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 930  time= 33.36782217025757  loss= 0.11310265904519741  val_loss= 0.22117163385206973  f1= 0.02638390645746706  f2= 0.026266047600006646  f3= 0.0009593846891434503  f4= 0.05949332029858026 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 940  time= 33.67480516433716  loss= 0.0898833411581393  val_loss= 0.15669600626501395  f1= 0.01259554255627608  f2= 0.02452312306314777  f3= 0.0016032822716821084  f4= 0.05116139326703336 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 950  time= 33.962209939956665  loss= 0.19659668384082082  val_loss= 0.18885433108239413  f1= 0.00898286111460289  f2= 0.032907425639261446  f3= 0.0060769030677542525  f4= 0.14862949401920225 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 960  time= 34.460766553878784  loss= 0.10692150648031755  val_loss= 0.09736228258930191  f1= 0.00724118552795347  f2= 0.022864223699982335  f3= 0.0001659031827306385  f4= 0.07665019406965114 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 970  time= 34.87737846374512  loss= 0.1673999844059553  val_loss= 0.11596720016881838  f1= 0.007414808551840925  f2= 0.025643403966647643  f3= 0.0010274952166133145  f4= 0.13331427667085344 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 980  time= 35.19246459007263  loss= 0.200220592002872  val_loss= 0.07139867500702361  f1= 0.010127596821304754  f2= 0.013976821032866413  f3= 0.0014130319359080372  f4= 0.1747031422127928 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 990  time= 35.599377393722534  loss= 0.20203474425744075  val_loss= 0.13264626350778566  f1= 0.013253905290531104  f2= 0.035518293109633274  f3= 0.000960861009284417  f4= 0.15230168484799197 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1000  time= 35.96436786651611  loss= 0.1270545524136053  val_loss= 0.18598566480389533  f1= 0.024960360995088554  f2= 0.030011207428368433  f3= 0.0024193564767966795  f4= 0.06966362751335163 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1010  time= 36.18795919418335  loss= 0.20907610233630994  val_loss= 0.0714061478985048  f1= 0.012117893924079066  f2= 0.03009618740083858  f3= 0.004021160528248624  f4= 0.16284086048314364 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1020  time= 36.41012144088745  loss= 0.2166994118267833  val_loss= 0.2013081573007905  f1= 0.008004791734887858  f2= 0.04019691869388373  f3= 8.362931354317968e-05  f4= 0.16841407208446849 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1030  time= 36.70793867111206  loss= 0.17229451636029616  val_loss= 0.10178682350041074  f1= 0.009290058934084992  f2= 0.017899319420374777  f3= 0.0012295034866403052  f4= 0.1438756345191961 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1040  time= 37.120962619781494  loss= 0.08430230954772854  val_loss= 0.07139359493920928  f1= 0.027523707191948313  f2= 0.023116255693557422  f3= 0.0023778540934668193  f4= 0.031284492568756005 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1050  time= 37.50193977355957  loss= 1.0398731036891886  val_loss= 0.10144542511176438  f1= 0.024499883283110365  f2= 0.40213511157165155  f3= 0.0015107892948080792  f4= 0.6117273195396186 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1060  time= 37.71333122253418  loss= 0.13965090133222438  val_loss= 0.14166246746279737  f1= 0.007600310387666527  f2= 0.02890523887317815  f3= 0.00023956138824419637  f4= 0.10290579068313553 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1070  time= 38.06941819190979  loss= 0.1433291188723675  val_loss= 0.19571344287614556  f1= 0.01291211360754651  f2= 0.014761487008747624  f3= 0.002561608620143554  f4= 0.1130939096359298 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1080  time= 38.38563418388367  loss= 0.13663439117043885  val_loss= 0.08368405952682795  f1= 0.010400131443215247  f2= 0.036831304595652044  f3= 0.0023812383152581524  f4= 0.08702171681631339 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1090  time= 38.640918493270874  loss= 0.15441566665571807  val_loss= 0.12862801948361924  f1= 0.007085685869217144  f2= 0.027777524170249868  f3= 0.013171992192924519  f4= 0.1063804644233265 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1100  time= 38.98258590698242  loss= 0.06395653270578683  val_loss= 0.11687571114171194  f1= 0.007168782030084988  f2= 0.018012569302335236  f3= 0.0004493817098910485  f4= 0.03832579966347554 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1110  time= 39.40302538871765  loss= 0.1139498069988049  val_loss= 0.19547640086395035  f1= 0.013391055833834301  f2= 0.014464342534704548  f3= 0.0008532356312966656  f4= 0.08524117299896937 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1120  time= 39.621044874191284  loss= 0.11663106535355178  val_loss= 0.13027954730123026  f1= 0.011578385983846876  f2= 0.016360263343423834  f3= 0.000771383992113202  f4= 0.08792103203416786 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1130  time= 39.94550323486328  loss= 0.39496247833115156  val_loss= 0.08399888331688624  f1= 0.012187196853526516  f2= 0.043939181905220386  f3= 0.00023563709384645697  f4= 0.33860046247855813 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1140  time= 40.2054009437561  loss= 0.26210936628590414  val_loss= 0.10284167670306349  f1= 0.017922104226964438  f2= 0.030093490152069174  f3= 0.00012829803917673678  f4= 0.21396547386769374 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1150  time= 40.56849813461304  loss= 0.19186991192952238  val_loss= 0.15528450050249734  f1= 0.028205730207427806  f2= 0.02810468451912709  f3= 0.0004686573856834489  f4= 0.13509083981728406 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1160  time= 40.94083786010742  loss= 0.12528558391752484  val_loss= 0.12003230992361556  f1= 0.004454765115647079  f2= 0.024021871980826785  f3= 0.0006873112789248141  f4= 0.09612163554212615 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1170  time= 41.20071792602539  loss= 0.19243886625340678  val_loss= 0.09144816334362386  f1= 0.013043666248348641  f2= 0.019114741498896075  f3= 0.00021584180208977975  f4= 0.16006461670407227 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1180  time= 41.52670764923096  loss= 0.09749310887343023  val_loss= 0.11174724971059564  f1= 0.009196662219641737  f2= 0.01251643180000788  f3= 7.317804965266988e-05  f4= 0.07570683680412794 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1190  time= 41.81992220878601  loss= 0.14220919004189364  val_loss= 0.09678177760170793  f1= 0.011040921215590492  f2= 0.012485301118022984  f3= 4.5247926842418314e-05  f4= 0.11863771978143774 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1200  time= 42.09714698791504  loss= 0.14045916122658972  val_loss= 0.21395059166175026  f1= 0.011437511179529994  f2= 0.015162984742545293  f3= 0.002451406310313891  f4= 0.11140725899420055 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1210  time= 42.505587339401245  loss= 0.26196753921851285  val_loss= 0.5958000080662073  f1= 0.010634790893661908  f2= 0.05494376481434173  f3= 0.01844906879978417  f4= 0.177939914710725 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1220  time= 42.93528413772583  loss= 0.23200650326327596  val_loss= 0.3104788155388689  f1= 0.007674337971993589  f2= 0.040781136576674794  f3= 0.0028223551207905034  f4= 0.18072867359381709 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1230  time= 43.30115795135498  loss= 0.1862054542645221  val_loss= 0.31807413866494383  f1= 0.012160653454338166  f2= 0.03848151722566612  f3= 0.0023518179164362684  f4= 0.13321146566808154 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1240  time= 43.55060696601868  loss= 0.265402859381252  val_loss= 0.561250517018372  f1= 0.0093513330986491  f2= 0.029006907217987907  f3= 0.0010865057882539489  f4= 0.2259581132763611 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1250  time= 43.771528244018555  loss= 0.223222626903562  val_loss= 0.2189695274582084  f1= 0.004887132785211141  f2= 0.019352306986315907  f3= 4.972920393779943e-05  f4= 0.19893345792809716 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1260  time= 43.97744393348694  loss= 0.22756113359966226  val_loss= 0.3762094961341742  f1= 0.009117112755678903  f2= 0.016153092590036708  f3= 0.0008548030204055881  f4= 0.20143612523354107 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1270  time= 44.17542052268982  loss= 0.06991764585278555  val_loss= 0.34746054241822116  f1= 0.006073379945630578  f2= 0.014707479131807607  f3= 0.003279378960509139  f4= 0.045857407814838214 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1280  time= 44.37730884552002  loss= 0.13163497269711263  val_loss= 0.23538530607003064  f1= 0.01523887595604667  f2= 0.019197377909845173  f3= 0.00022542504921248567  f4= 0.09697329378200832 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1290  time= 44.57149338722229  loss= 0.10264065997542209  val_loss= 0.5109973616712289  f1= 0.008660376628757181  f2= 0.019101116094723893  f3= 0.00020878884727727146  f4= 0.07467037840466376 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1300  time= 44.78826117515564  loss= 0.08471712492948609  val_loss= 0.5027630442767013  f1= 0.005428036065591113  f2= 0.01879646260002551  f3= 0.0038437683194055983  f4= 0.056648857944463886 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1310  time= 44.98296642303467  loss= 0.05058980619218751  val_loss= 0.4902579510769441  f1= 0.008199767767412136  f2= 0.016816044128283275  f3= 0.0007115647934137188  f4= 0.02486242950307839 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1320  time= 45.20811915397644  loss= 0.29368385142995435  val_loss= 0.1809060786605034  f1= 0.009459018321861514  f2= 0.01445225841440366  f3= 0.0010430560779207292  f4= 0.26872951861576844 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1330  time= 45.447760581970215  loss= 1.3467265108446427  val_loss= 0.37098581580890766  f1= 0.02511314029241843  f2= 0.22124556278849714  f3= 0.0005511883096980259  f4= 1.099816619454029 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1340  time= 45.660964488983154  loss= 0.11115296437298028  val_loss= 0.3394382781223918  f1= 0.008078801391999095  f2= 0.01985805304661187  f3= 0.00146870603587606  f4= 0.08174740389849326 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1350  time= 45.89794683456421  loss= 0.05695099932059289  val_loss= 0.47969119258911797  f1= 0.0049577082504877155  f2= 0.018040302011300524  f3= 0.0003924247585083357  f4= 0.0335605643002963 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1360  time= 46.10052967071533  loss= 0.11711420501225356  val_loss= 0.6213017461475919  f1= 0.015442693389980123  f2= 0.013970397663585545  f3= 0.0021043514388872173  f4= 0.0855967625198007 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1370  time= 46.319462299346924  loss= 0.1002432082718373  val_loss= 0.3870914045975114  f1= 0.004287906032359023  f2= 0.02207811149784752  f3= 0.002126398364322123  f4= 0.07175079237730862 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1380  time= 46.51431083679199  loss= 0.049693318395981206  val_loss= 1.3340334484076897  f1= 0.00956935582750093  f2= 0.007377324829771943  f3= 0.00231931820461434  f4= 0.030427319534094 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1390  time= 46.7158522605896  loss= 0.24588758703589608  val_loss= 0.11189618908874968  f1= 0.010485999899568227  f2= 0.015085840379789772  f3= 0.0023297703019749725  f4= 0.21798597645456305 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1400  time= 46.921213150024414  loss= 0.0756632885240751  val_loss= 1.324431677093517  f1= 0.01005398202084523  f2= 0.01033580042339015  f3= 0.0014416596768854464  f4= 0.05383184640295428 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1410  time= 47.151360750198364  loss= 0.10932561243861251  val_loss= 0.6321345315237843  f1= 0.016080282530209183  f2= 0.020054227140926188  f3= 0.0035630113442064928  f4= 0.06962809142327066 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1420  time= 47.356520891189575  loss= 0.037324100388105415  val_loss= 0.25430511763626046  f1= 0.008162546322837996  f2= 0.010696599148161576  f3= 0.00039127069717405616  f4= 0.018073684219931784 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1430  time= 47.5735228061676  loss= 0.07152051088799245  val_loss= 0.7032301849737607  f1= 0.004737062568274411  f2= 0.01573322006562527  f3= 0.0005529364362721642  f4= 0.050497291817820596 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1440  time= 47.776769161224365  loss= 0.18190924831567845  val_loss= 0.2523786909541765  f1= 0.007885322657585942  f2= 0.00979532976546102  f3= 0.000886790771535853  f4= 0.16334180512109567 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1450  time= 48.00977540016174  loss= 0.1003469189271546  val_loss= 1.028815786017036  f1= 0.01353362252190469  f2= 0.013533041072935322  f3= 0.007560531544002116  f4= 0.06571972378831246 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1460  time= 48.22374939918518  loss= 0.09929223998750387  val_loss= 0.44026193973976724  f1= 0.01010942990607946  f2= 0.02207006623165028  f3= 0.0005552408880018416  f4= 0.06655750296177228 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1470  time= 48.423932790756226  loss= 0.059170612392530825  val_loss= 0.6140037026857519  f1= 0.008248782938811493  f2= 0.010066672953545864  f3= 0.0002821103690994284  f4= 0.040573046131074035 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1480  time= 48.65126371383667  loss= 0.057508291129364876  val_loss= 0.5835897245054209  f1= 0.005327162019779229  f2= 0.009365220374378966  f3= 0.0015236506074146519  f4= 0.041292258127792046 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1490  time= 48.88107228279114  loss= 0.12002223999234342  val_loss= 0.5656552319682467  f1= 0.008033537746863117  f2= 0.01693397725447374  f3= 0.000887360046038359  f4= 0.09416736494496823 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1500  time= 49.11814498901367  loss= 0.12065914475346899  val_loss= 0.3127610616169415  f1= 0.011206970171001543  f2= 0.014797609258487712  f3= 0.005858081107616806  f4= 0.08879648421636294 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1510  time= 49.310274600982666  loss= 0.0857856542001496  val_loss= 0.6195884268200388  f1= 0.0060702942921133166  f2= 0.01082923639983817  f3= 0.003822445392705318  f4= 0.06506367811549278 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1520  time= 49.529834508895874  loss= 0.12900586864560992  val_loss= 0.2749905315335399  f1= 0.007293615507722784  f2= 0.011421854678002039  f3= 0.00047461249061831064  f4= 0.10981578596926679 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1530  time= 49.71675491333008  loss= 0.06701355650857622  val_loss= 0.641965522815207  f1= 0.007051311835489769  f2= 0.00992131408557967  f3= 0.0020215336518030025  f4= 0.048019396935703805 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1540  time= 49.944636821746826  loss= 0.05729742380430968  val_loss= 0.46818819748397084  f1= 0.008212574163538437  f2= 0.006902445999508238  f3= 0.004274950385110388  f4= 0.03790745325615262 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1550  time= 50.1592800617218  loss= 0.06582928088125564  val_loss= 0.1577257010195769  f1= 0.00765184323434711  f2= 0.0054727136670646775  f3= 0.00016220529586237034  f4= 0.05254251868398147 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1560  time= 50.40311884880066  loss= 0.1915385970035887  val_loss= 0.22605047909473508  f1= 0.005015493895739857  f2= 0.015569823554944193  f3= 7.566858519423372e-05  f4= 0.17087761096771037 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1570  time= 50.62701654434204  loss= 0.02031643501024782  val_loss= 0.21619840941032642  f1= 0.0026066666908887964  f2= 0.005028347941701264  f3= 6.757598682710034e-05  f4= 0.01261384439083066 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1580  time= 50.86554026603699  loss= 0.09261119482957048  val_loss= 0.28640547715825604  f1= 0.008277538123219633  f2= 0.006324385183966397  f3= 0.0013041534256217145  f4= 0.07670511809676274 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1590  time= 51.10077953338623  loss= 0.11616128118063379  val_loss= 0.5967673916980825  f1= 0.023343185050381538  f2= 0.011147234076927451  f3= 0.00741961140163512  f4= 0.07425125065168968 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1600  time= 51.3001229763031  loss= 0.02963689490750486  val_loss= 0.461596660965186  f1= 0.004073066020946091  f2= 0.0041098626586886415  f3= 0.0016877714444792633  f4= 0.01976619478339086 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1610  time= 51.49668478965759  loss= 0.06622462174718302  val_loss= 0.5430405273896239  f1= 0.01066711430879907  f2= 0.010690360656116003  f3= 0.0026022888408048883  f4= 0.04226485794146306 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1620  time= 51.69686245918274  loss= 0.18291179265954602  val_loss= 0.8706483762842273  f1= 0.007614324576773339  f2= 0.011714832618496892  f3= 0.00038526390008200486  f4= 0.16319737156419376 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1630  time= 51.889832973480225  loss= 0.06826239872590961  val_loss= 0.429230680338807  f1= 0.00739067928518819  f2= 0.00599195496404099  f3= 0.0014897314868732873  f4= 0.053390032989807136 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1640  time= 52.122297048568726  loss= 0.06183273495537125  val_loss= 0.36415251813560023  f1= 0.0151303946217976  f2= 0.012432054561108912  f3= 0.00014095229137800055  f4= 0.03412933348108673 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1650  time= 52.336148738861084  loss= 0.05860716073373434  val_loss= 0.2065071088002351  f1= 0.006637520050442931  f2= 0.008889625758767459  f3= 0.00045169778054914993  f4= 0.04262831714397481 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1660  time= 52.540180683135986  loss= 0.18911080955384238  val_loss= 0.46661602567402766  f1= 0.00878932246992801  f2= 0.017480394899225326  f3= 0.0006702754152446267  f4= 0.16217081676944442 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1670  time= 52.76356863975525  loss= 0.09046859813635057  val_loss= 1.5716337657232915  f1= 0.011806291152028173  f2= 0.017552899935597815  f3= 0.003604118579930808  f4= 0.057505288468793776 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1680  time= 52.969361782073975  loss= 0.04615297095115283  val_loss= 0.2774661559027189  f1= 0.004877641935491881  f2= 0.008819418141060771  f3= 0.00013107432083133647  f4= 0.032324836553768846 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1690  time= 53.19271469116211  loss= 0.05357833411321374  val_loss= 0.9584096725991743  f1= 0.008947054493503935  f2= 0.006304203548870588  f3= 0.002253823341601562  f4= 0.036073252729237656 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1700  time= 53.393263816833496  loss= 0.10504540991590168  val_loss= 0.1486102505728485  f1= 0.004936619319828425  f2= 0.01087676022922618  f3= 0.004079155812868779  f4= 0.08515287455397828 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1710  time= 53.596763610839844  loss= 0.0504862837066166  val_loss= 0.2729272176989728  f1= 0.004845155424560006  f2= 0.004971369643243917  f3= 0.0016336520190395655  f4= 0.03903610661977313 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1720  time= 53.803577184677124  loss= 0.02465279910842576  val_loss= 0.4326610330935328  f1= 0.002802145617412669  f2= 0.005039219611615668  f3= 0.0012508138543653477  f4= 0.015560620025032076 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1730  time= 54.01479887962341  loss= 0.18752825083205307  val_loss= 1.1730450168311566  f1= 0.011013201470608547  f2= 0.00877223807811506  f3= 0.0027457326400661067  f4= 0.16499707864326338 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1740  time= 54.24849200248718  loss= 0.31660731446642926  val_loss= 1.2181534798627969  f1= 0.0061226141040081755  f2= 0.014136638677766634  f3= 0.0015534122567026647  f4= 0.2947946494279518 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1750  time= 54.48883509635925  loss= 0.06267889098184072  val_loss= 0.8336090662871444  f1= 0.004356409198372073  f2= 0.01293375222250284  f3= 0.000754478316974253  f4= 0.04463425124399156 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1760  time= 54.718146324157715  loss= 0.11450280753198126  val_loss= 0.5615818288858434  f1= 0.01612455804556409  f2= 0.012630051599276618  f3= 0.00684144829969825  f4= 0.07890674958744232 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1770  time= 54.93944549560547  loss= 0.06078124968391597  val_loss= 0.5484843459394355  f1= 0.005162854162736607  f2= 0.004881634727930513  f3= 0.0010479941794786297  f4= 0.049688766613770224 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1780  time= 55.18440771102905  loss= 0.20032306919487605  val_loss= 0.8699206136458147  f1= 0.012897161943613754  f2= 0.01835186230967283  f3= 0.0001238817057971183  f4= 0.16895016323579234 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1790  time= 55.39677596092224  loss= 0.12187480986024174  val_loss= 2.4079552574600362  f1= 0.006619707587973338  f2= 0.009710013402413372  f3= 0.00048028330658995093  f4= 0.10506480556326507 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1800  time= 55.6213014125824  loss= 0.06474890890239506  val_loss= 0.2541476267216946  f1= 0.0052858183212339226  f2= 0.004270777142173764  f3= 0.00031196337632039423  f4= 0.05488035006266698 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1810  time= 55.851258993148804  loss= 0.11430866494471713  val_loss= 0.7351187018043617  f1= 0.01677581271414576  f2= 0.004403066081670203  f3= 0.002040391957351076  f4= 0.09108939419155009 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1820  time= 56.092838525772095  loss= 0.23513495669909915  val_loss= 0.6929287475564898  f1= 0.011453788642202182  f2= 0.005490213083099257  f3= 0.0008897068625793349  f4= 0.21730124811121843 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1830  time= 56.340662240982056  loss= 0.15280309623118232  val_loss= 1.079343708573528  f1= 0.013866949216838042  f2= 0.008027482260001016  f3= 0.0021086564455526824  f4= 0.1288000083087906 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1840  time= 56.59125638008118  loss= 0.19543558761651295  val_loss= 1.5976895870923535  f1= 0.009077127544420941  f2= 0.005503643778781723  f3= 0.0077475207861823795  f4= 0.17310729550712792 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1850  time= 56.846314668655396  loss= 0.0674500547933941  val_loss= 0.7666878544889075  f1= 0.0050564091769201465  f2= 0.007960648256550287  f3= 3.0222696184094334e-05  f4= 0.05440277466373957 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1860  time= 57.074628829956055  loss= 0.054462755731725744  val_loss= 0.8146491677643506  f1= 0.006404781119007905  f2= 0.005846395660301529  f3= 0.0010329302788733413  f4= 0.04117864867354297 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1870  time= 57.326472997665405  loss= 0.18116477104333165  val_loss= 1.1489892115271279  f1= 0.011025841352597453  f2= 0.024447205103118475  f3= 0.0002442044994391361  f4= 0.14544752008817657 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1880  time= 57.56985950469971  loss= 0.10234093382488206  val_loss= 0.9541239625275063  f1= 0.005186848038833272  f2= 0.010638493789963238  f3= 0.001731202805594233  f4= 0.0847843891904913 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1890  time= 57.842573165893555  loss= 0.19880741407574098  val_loss= 0.7844315862540734  f1= 0.0061624518803568215  f2= 0.011160891418640665  f3= 0.0005125873319965912  f4= 0.1809714834447469 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1900  time= 58.093440532684326  loss= 0.11342915343731146  val_loss= 1.4062409108864062  f1= 0.008528628571979143  f2= 0.013739356825472586  f3= 0.0007611096038073451  f4= 0.09040005843605237 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1910  time= 58.38900065422058  loss= 0.1502653948602106  val_loss= 2.9799250390995633  f1= 0.005505540889779153  f2= 0.008618182486121532  f3= 0.00010580229510620213  f4= 0.1360358691892037 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1920  time= 58.65868544578552  loss= 0.41018200358281426  val_loss= 1.6741057573097398  f1= 0.012810551441443193  f2= 0.02920789571684716  f3= 0.0013819143885588385  f4= 0.3667816420359651 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1930  time= 58.91872692108154  loss= 0.044478650927711604  val_loss= 2.2146663624425855  f1= 0.005405120227236749  f2= 0.010212133282411886  f3= 3.828495583125744e-05  f4= 0.028823112462231706 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1940  time= 59.18860578536987  loss= 0.040534425245310333  val_loss= 3.571178714190388  f1= 0.004974766791204875  f2= 0.00590556907137961  f3= 0.0018881454795496866  f4= 0.027765943903176164 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1950  time= 59.42842173576355  loss= 0.05532233065659675  val_loss= 3.063535037073829  f1= 0.004621326783724158  f2= 0.00809818387552424  f3= 0.00023977483850289908  f4= 0.04236304515884546 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1960  time= 59.64009165763855  loss= 0.2390671166287501  val_loss= 3.616946007126985  f1= 0.008688362549325224  f2= 0.06763510782296751  f3= 0.0008987030621232615  f4= 0.16184494319433404 num_batches= 6 percent lr= 0.01\n"," \n","epoch= 1970  time= 59.880502462387085  loss= 0.06520630303447779  val_loss= 2.47889289496242  f1= 0.0032872657859046215  f2= 0.009167075182060367  f3= 0.0006602965619370884  f4= 0.05209166550457572 num_batches= 6 percent lr= 0.01\n","EarlyStopping counter: 996 out of 1000\n","EarlyStopping counter: 997 out of 1000\n","EarlyStopping counter: 998 out of 1000\n","EarlyStopping counter: 999 out of 1000\n","EarlyStopping counter: 1000 out of 1000\n","Early Stopping\n","[[ 0.39050803  1.72151493  0.82210701 ...  0.9639992   1.11697794\n","   3.58832241]\n"," [ 2.22620934  2.78676216 -0.07664073 ...  3.55062071  1.49426694\n","  -1.69939694]\n"," [ 2.15199138 -3.33468183  3.79819538 ... -0.73192706 -3.28993612\n","   0.40463135]\n"," [-0.36537019 -1.9769609   3.13690668 ...  0.79859667 -2.17563121\n","  -0.66574602]] [[ 2.15198969 -3.33468427  3.79812235 ... -0.73192802 -3.28994472\n","   0.40462848]\n"," [-0.36537983 -1.97696487  3.13691349 ...  0.7985931  -2.17564272\n","  -0.66574466]\n"," [-0.03383051 -0.04898445 -1.45992689 ... -0.01935551 -0.17204925\n","  -0.05733018]\n"," [-0.19280639 -0.07930016  0.13579164 ... -0.07129411 -0.23018061\n","   0.02715143]] [ 1.93984244  7.20895251 10.92210068 ...  0.31493559  7.24250561\n","  0.05160781]\n","training sepNet\n"," \n","epoch= 0  time= 0.033774614334106445  loss= 13.440793318178393  val_loss= 15.732554037267642  f1= 5.329374722409276  f2= 7.707628508071896  f3= 0.4036529373748912  f4= 0.00013715032232734632 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 10  time= 0.4134559631347656  loss= 18.128237771563434  val_loss= 89.52377337746208  f1= 0.9554940591613874  f2= 7.70422303273776  f3= 0.06479003923418135  f4= 9.403730640430107 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 20  time= 0.7846150398254395  loss= 18.43341274518875  val_loss= 43.13057780868232  f1= 1.0121111235019886  f2= 7.705427782377588  f3= 0.06693079831549399  f4= 9.648943040993682 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 30  time= 1.1709129810333252  loss= 17.68932722951714  val_loss= 46.19619171179039  f1= 0.6208953466744871  f2= 7.6884046555142875  f3= 0.18813568254832497  f4= 9.191891544780043 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 40  time= 1.496816873550415  loss= 17.81587017828053  val_loss= 51.41616294884976  f1= 0.4657189985046208  f2= 7.685822123890637  f3= 0.09441800486496973  f4= 9.5699110510203 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 50  time= 1.8640074729919434  loss= 13.773938535718491  val_loss= 50.01086865633167  f1= 0.5672618734249142  f2= 7.681996968502361  f3= 0.3414837120826997  f4= 5.183195981708519 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 60  time= 2.2500646114349365  loss= 14.526921261746159  val_loss= 42.77324484701735  f1= 0.37991808429264345  f2= 7.633880220875339  f3= 0.004703770578790681  f4= 6.5084191859993865 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 70  time= 2.620201587677002  loss= 11.966974517154638  val_loss= 49.5972858874873  f1= 0.4571968145257785  f2= 7.606184670085069  f3= 0.04478640499224502  f4= 3.8588066275515445 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 80  time= 2.991347551345825  loss= 14.477634422964186  val_loss= 40.256975109461415  f1= 0.1387128623979174  f2= 7.467263153816993  f3= 0.005884399917411105  f4= 6.865774006831864 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 90  time= 3.406353712081909  loss= 12.129577514837393  val_loss= 35.64258297196309  f1= 0.2864500531728124  f2= 7.317617638111225  f3= 0.008244501730507368  f4= 4.517265321822849 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 100  time= 3.8032050132751465  loss= 14.281103949637883  val_loss= 39.54581620461524  f1= 0.14270319244571975  f2= 7.299305156547859  f3= 0.13454616070832978  f4= 6.704549439935974 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 110  time= 4.198010206222534  loss= 12.610919957262334  val_loss= 41.69157411457982  f1= 0.16341636758594977  f2= 7.139312246155603  f3= 0.0967469636410377  f4= 5.211444379879743 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 120  time= 4.552523136138916  loss= 14.88259348433212  val_loss= 37.2373351866469  f1= 0.10246434398913097  f2= 7.0534793048638065  f3= 0.14671564806786594  f4= 7.5799341874113155 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 130  time= 4.93852686882019  loss= 11.659364462557479  val_loss= 38.86758608061709  f1= 0.16769867509109632  f2= 6.944599505315116  f3= 0.025504488688494135  f4= 4.521561793462771 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 140  time= 5.302973031997681  loss= 12.149687646565608  val_loss= 36.79979450290666  f1= 0.1445588176263684  f2= 6.857636541469005  f3= 0.08425204089205088  f4= 5.063240246578182 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 150  time= 5.683912515640259  loss= 12.52456904802881  val_loss= 33.05856979073244  f1= 0.12168720284055527  f2= 6.848740945525438  f3= 0.020125082406205332  f4= 5.534015817256612 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 160  time= 6.092258453369141  loss= 16.510439748127197  val_loss= 27.945840062931488  f1= 0.05168112590425706  f2= 6.587697128361214  f3= 0.006055714801322657  f4= 9.865005779060402 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 170  time= 6.499377489089966  loss= 23.473058338757795  val_loss= 31.377093172903987  f1= 0.06311972161112549  f2= 7.575198085263447  f3= 0.09890223449090436  f4= 15.73583829739232 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 180  time= 6.888876438140869  loss= 12.123278067780825  val_loss= 27.24162017408148  f1= 0.0863542581091716  f2= 6.251330909632665  f3= 0.005332137276319396  f4= 5.7802607627626665 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 190  time= 7.291295528411865  loss= 10.073337036710816  val_loss= 27.994313992141436  f1= 0.09241553777410183  f2= 5.87995134425637  f3= 0.04939461956147982  f4= 4.0515755351188645 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 200  time= 7.7327752113342285  loss= 9.303985417803972  val_loss= 24.27845563462338  f1= 0.07506496489101067  f2= 5.533013904269426  f3= 0.01585447567516776  f4= 3.6800520729683655 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 210  time= 8.152499914169312  loss= 8.569829072329307  val_loss= 16.843900639314185  f1= 0.06828529713520812  f2= 5.10473344704476  f3= 0.019068061326880183  f4= 3.3777422668224597 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 220  time= 8.507187366485596  loss= 18.761021599459884  val_loss= 15.99903739396022  f1= 0.032053822230306304  f2= 11.771477467174789  f3= 0.01200478816780325  f4= 6.945485521886987 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 230  time= 8.869826316833496  loss= 8.471138643163894  val_loss= 13.613479610099475  f1= 0.04130697247552476  f2= 4.783067097270544  f3= 0.002656996820443065  f4= 3.644107576597381 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 240  time= 9.246967077255249  loss= 14.41275444345558  val_loss= 13.819822111693187  f1= 0.04856082737303503  f2= 10.677145938076569  f3= 0.010601759565923461  f4= 3.67644591844005 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 250  time= 9.636247158050537  loss= 5.652931039836828  val_loss= 10.812903559563518  f1= 0.0420697802419463  f2= 3.932354332447344  f3= 0.0007717310592429852  f4= 1.6777351960882947 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 260  time= 10.062331199645996  loss= 8.802498944463151  val_loss= 12.487383508958011  f1= 0.03138086317584141  f2= 4.138103514905184  f3= 0.004082456432598715  f4= 4.628932109949527 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 270  time= 10.474826335906982  loss= 4.766423800693319  val_loss= 11.451028531762391  f1= 0.07765298000614841  f2= 3.802983864586609  f3= 0.006343861792570478  f4= 0.8794430943079912 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 280  time= 10.87851071357727  loss= 3.9685339974385747  val_loss= 5.333654565853212  f1= 0.03293289640569816  f2= 3.1941289828945107  f3= 0.0065821440947939625  f4= 0.7348899740435718 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 290  time= 11.274696588516235  loss= 3.264613055400555  val_loss= 3.1071966670690276  f1= 0.037361247170998695  f2= 2.707866432955202  f3= 0.0074625403762551374  f4= 0.5119228348980999 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 300  time= 11.646461009979248  loss= 4.517790365969114  val_loss= 5.7209732089671075  f1= 0.030231635347362656  f2= 3.0198438780967116  f3= 0.010514267163591414  f4= 1.4572005853614496 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 310  time= 12.02341079711914  loss= 3.323419607722357  val_loss= 4.874611623541359  f1= 0.024436181979746653  f2= 2.8555767855024783  f3= 0.008660696110540699  f4= 0.4347459441295913 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 320  time= 12.450219869613647  loss= 2.32884539459022  val_loss= 2.3576487413011655  f1= 0.022494425544842128  f2= 2.050670728282209  f3= 0.0013000952690771222  f4= 0.25438014549409155 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 330  time= 12.878745317459106  loss= 2.374643607744094  val_loss= 1.6457511345380682  f1= 0.01895224577891919  f2= 2.018172039631374  f3= 0.013982594638597218  f4= 0.3235367276952031 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 340  time= 13.30892014503479  loss= 1.5816225360733878  val_loss= 5.128373758168426  f1= 0.023429806857861436  f2= 1.3478500797610664  f3= 0.034185314015438006  f4= 0.17615733543902243 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 350  time= 13.706742286682129  loss= 1.5277006231754473  val_loss= 2.9050420451166525  f1= 0.01317378447994262  f2= 1.266577256731892  f3= 0.0009514232542616151  f4= 0.24699815870935135 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 360  time= 14.103374481201172  loss= 1.2270959118871738  val_loss= 3.3712808542487114  f1= 0.01599010961012016  f2= 1.0876453048459906  f3= 0.007569596543525718  f4= 0.11589090088753731 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 370  time= 14.540162324905396  loss= 1.1009103968901612  val_loss= 7.579459328415357  f1= 0.01255862579503865  f2= 0.8431062540181858  f3= 0.007869055400757389  f4= 0.23737646167617973 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 380  time= 14.967344284057617  loss= 0.922199690776039  val_loss= 8.569834003169092  f1= 0.012008073555617994  f2= 0.7699888580288173  f3= 0.007488501921404171  f4= 0.13271425727019942 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 390  time= 15.39792776107788  loss= 1.0317734567125263  val_loss= 16.83054530864048  f1= 0.0137197618732047  f2= 0.6946948698012767  f3= 0.0005098440353103216  f4= 0.3228489810027345 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 400  time= 15.844339609146118  loss= 0.5181741900562152  val_loss= 14.83294747438816  f1= 0.009734559185219456  f2= 0.4061907944132101  f3= 0.004021930802596691  f4= 0.09822690565518896 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 410  time= 16.315826892852783  loss= 0.32649669960246597  val_loss= 17.914362835744722  f1= 0.008805650179851897  f2= 0.21924333571309954  f3= 0.000781004609060375  f4= 0.09766670910045425 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 420  time= 16.707409143447876  loss= 0.5272780926797965  val_loss= 9.785747636369107  f1= 0.024991808123771393  f2= 0.140684245623809  f3= 0.03403408990216531  f4= 0.3275679490300509 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 430  time= 17.11832571029663  loss= 0.22912039449061852  val_loss= 21.086179576428684  f1= 0.013635541006358862  f2= 0.08630787789071415  f3= 0.004457014285534016  f4= 0.12471996130801144 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 440  time= 17.524934768676758  loss= 0.2662459328398933  val_loss= 13.71901868051011  f1= 0.00881292646336202  f2= 0.07379591864581335  f3= 0.0018489592310354985  f4= 0.18178812849968246 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 450  time= 17.91918134689331  loss= 0.19525298605176716  val_loss= 26.605308374850893  f1= 0.011424311179534125  f2= 0.05065805413633331  f3= 0.000704494892612008  f4= 0.13246612584328774 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 460  time= 18.312095403671265  loss= 0.13618774546402798  val_loss= 53.787498049746304  f1= 0.008485774641574172  f2= 0.06242054290120059  f3= 0.021541697661416243  f4= 0.04373973025983697 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 470  time= 18.7173855304718  loss= 0.2877119918160525  val_loss= 17.034171663597416  f1= 0.011590363555114476  f2= 0.03696194244780273  f3= 0.0021482992671023513  f4= 0.23701138654603285 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 480  time= 19.09306311607361  loss= 0.6082145246225414  val_loss= 14.377489348491908  f1= 0.026482106857134396  f2= 0.02571651667365264  f3= 0.02747696259895549  f4= 0.5285389384927992 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 490  time= 19.50538921356201  loss= 0.08270941023408813  val_loss= 22.984010148366533  f1= 0.007191971475372915  f2= 0.013439550011111561  f3= 0.00021560484275745545  f4= 0.0618622839048462 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 500  time= 19.908634185791016  loss= 0.1993976898390814  val_loss= 20.470644717387774  f1= 0.010216098145237705  f2= 0.024947090146943247  f3= 0.004030602169415119  f4= 0.16020389937748528 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 510  time= 20.298110008239746  loss= 0.23562162059830496  val_loss= 20.521242785962098  f1= 0.005484090959362542  f2= 0.04432711200893561  f3= 0.008054006252767738  f4= 0.17775641137723905 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 520  time= 20.755006074905396  loss= 0.055515395870707855  val_loss= 19.126034403346313  f1= 0.008542628899356558  f2= 0.0185729948159879  f3= 0.0007447215284046685  f4= 0.02765505062695873 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 530  time= 21.188785552978516  loss= 0.06675386980344682  val_loss= 15.792256018013173  f1= 0.004316591898273418  f2= 0.004911780322636047  f3= 0.000455329086781731  f4= 0.057070168495755635 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 540  time= 21.60357904434204  loss= 0.10090365747494459  val_loss= 17.825091693978106  f1= 0.008401542980051151  f2= 0.013773466737198986  f3= 0.010554980172448306  f4= 0.06817366758524615 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 550  time= 22.018152475357056  loss= 0.12667952024247578  val_loss= 16.4079276822456  f1= 0.012734040603217431  f2= 0.007455054036655835  f3= 0.001479813967618582  f4= 0.10501061163498396 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 560  time= 22.446287870407104  loss= 0.0864897486117182  val_loss= 12.906587944629887  f1= 0.004379830637654145  f2= 0.008555143512493696  f3= 0.003560567723580969  f4= 0.06999420673798938 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 570  time= 22.851017475128174  loss= 0.18507106537001178  val_loss= 8.923396899279672  f1= 0.01639146196073856  f2= 0.004084653928238205  f3= 0.002249527045653396  f4= 0.1623454224353816 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 580  time= 23.237395524978638  loss= 0.21633623435014943  val_loss= 21.655082646264013  f1= 0.013510741924554885  f2= 0.01707325230333492  f3= 0.004175683659779836  f4= 0.18157655646247975 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 590  time= 23.64005208015442  loss= 0.19889890500109977  val_loss= 14.471217549339595  f1= 0.015944660093429673  f2= 0.027912118483091046  f3= 0.00030474598751367086  f4= 0.1547373804370654 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 600  time= 24.0653977394104  loss= 0.2355715781253402  val_loss= 16.8419458382553  f1= 0.006168243841507769  f2= 0.024088729682917358  f3= 0.00019479387728367923  f4= 0.2051198107236314 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 610  time= 24.49454641342163  loss= 0.040393093611321276  val_loss= 23.96916128148337  f1= 0.003872037643278211  f2= 0.008879336413086818  f3= 0.0005322458484760703  f4= 0.027109473706480182 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 620  time= 24.923453330993652  loss= 0.2192781837728437  val_loss= 20.519162487966238  f1= 0.02223265505624933  f2= 0.034372367676804794  f3= 0.0024015367155348845  f4= 0.1602716243242547 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 630  time= 25.36845588684082  loss= 0.1383186196990821  val_loss= 22.270526087870245  f1= 0.010333014465885023  f2= 0.017965542824862336  f3= 0.007248153901073444  f4= 0.10277190850726133 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 640  time= 26.02558469772339  loss= 0.03649941163553915  val_loss= 18.159914083431943  f1= 0.008791062589767416  f2= 0.00521999099424347  f3= 0.0006060840390871221  f4= 0.021882274012441146 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 650  time= 26.680440187454224  loss= 0.09435366194901186  val_loss= 26.06652049274756  f1= 0.005433454641043556  f2= 0.015154481099566765  f3= 0.004441981760997074  f4= 0.06932374444740448 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 660  time= 27.309619665145874  loss= 0.11352299277530044  val_loss= 13.264402903684388  f1= 0.005068448962574246  f2= 0.00699881923604006  f3= 0.0005509253811501598  f4= 0.10090479919553598 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 670  time= 27.965795278549194  loss= 0.15237220948739502  val_loss= 18.709839890626597  f1= 0.005923895009786156  f2= 0.02566563220915689  f3= 0.0022705977568511343  f4= 0.11851208451160088 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 680  time= 28.578455209732056  loss= 0.20254049296825835  val_loss= 17.692063365021784  f1= 0.01562885562608403  f2= 0.005539477160343332  f3= 0.005120858739098249  f4= 0.17625130144273274 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 690  time= 29.233023643493652  loss= 0.07717076219065142  val_loss= 30.245556186722123  f1= 0.00911546550129812  f2= 0.005752079258173801  f3= 0.00044491449421649686  f4= 0.06185830293696298 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 700  time= 29.896167755126953  loss= 0.1673058369838846  val_loss= 10.174681641597568  f1= 0.027210405788835743  f2= 0.018980533362414988  f3= 0.01106116925658339  f4= 0.11005372857605045 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 710  time= 30.51416039466858  loss= 0.16404648601282443  val_loss= 20.35334603194388  f1= 0.020086531717308453  f2= 0.024496604299295798  f3= 0.005916071079381898  f4= 0.11354727891683829 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 720  time= 30.933921575546265  loss= 0.22177652289871033  val_loss= 28.230347089144935  f1= 0.009364046024625356  f2= 0.02155493207020378  f3= 0.001489326315636103  f4= 0.1893682184882451 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 730  time= 31.333220720291138  loss= 0.06657051377726464  val_loss= 10.888221430766896  f1= 0.007217525056618065  f2= 0.008550525377246517  f3= 0.004388030725993266  f4= 0.0464144326174068 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 740  time= 31.71263313293457  loss= 0.07323953910830508  val_loss= 13.326658747310013  f1= 0.007645612239411012  f2= 0.00576160973039741  f3= 0.0009019307784077882  f4= 0.05893038636008886 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 750  time= 32.15315556526184  loss= 0.08562686016903358  val_loss= 15.814885661345388  f1= 0.012453142172897497  f2= 0.008974397513972028  f3= 0.0006235671378725084  f4= 0.06357575334429157 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 760  time= 32.57706618309021  loss= 0.057172707359531005  val_loss= 16.212004532444983  f1= 0.005087181566002842  f2= 0.00982790917076568  f3= 0.006243110893874618  f4= 0.03601450572888786 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 770  time= 33.01811933517456  loss= 0.16957676440683073  val_loss= 18.037670339445878  f1= 0.010363303457683876  f2= 0.02948898105144049  f3= 0.004434627228522311  f4= 0.12528985266918408 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 780  time= 33.42251634597778  loss= 0.13319381893747237  val_loss= 18.592557218359183  f1= 0.010024165627131902  f2= 0.027968633924027844  f3= 0.0002316152378857726  f4= 0.09496940414842683 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 790  time= 33.809635639190674  loss= 0.32190046335514577  val_loss= 19.817410911265473  f1= 0.005435170348814015  f2= 0.020588061788624903  f3= 0.000157063766142613  f4= 0.29572016745156426 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 800  time= 34.21801948547363  loss= 0.10703429031971011  val_loss= 18.695530827593583  f1= 0.004366118831610715  f2= 0.009595871758391883  f3= 0.00387102312289148  f4= 0.08920127660681602 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 810  time= 34.59911489486694  loss= 0.1424427504836214  val_loss= 28.0276589883347  f1= 0.010881112649293534  f2= 0.014875522865097413  f3= 0.0009018371159441294  f4= 0.1157842778532863 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 820  time= 35.013941526412964  loss= 0.10508704888939904  val_loss= 22.482328627982156  f1= 0.004270163766652444  f2= 0.013861617029979425  f3= 0.005143805632190798  f4= 0.08181146246057637 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 830  time= 35.44246792793274  loss= 0.10932457226575294  val_loss= 31.747316995634264  f1= 0.008321243728331486  f2= 0.009832785202721532  f3= 0.0020614198297920416  f4= 0.08910912350490788 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 840  time= 35.8617045879364  loss= 0.15571451552513763  val_loss= 18.11411579849036  f1= 0.004464416607349332  f2= 0.01673623613733585  f3= 0.0023659987982937477  f4= 0.1321478639821587 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 850  time= 36.2507483959198  loss= 0.1308197296321458  val_loss= 24.096187036412978  f1= 0.0036975504638430143  f2= 0.015817944062165752  f3= 0.0025703116599598654  f4= 0.10873392344617717 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 860  time= 36.64364719390869  loss= 0.1339300802735252  val_loss= 28.30629271670571  f1= 0.01042821536833027  f2= 0.03752587992130246  f3= 0.003747629981476199  f4= 0.0822283550024163 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 870  time= 37.05494785308838  loss= 0.03544813326764294  val_loss= 30.358070290182873  f1= 0.006769205436183363  f2= 0.0051903460518266465  f3= 0.0034627447532014524  f4= 0.02002583702643148 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 880  time= 37.455317974090576  loss= 0.1679188628180057  val_loss= 24.21530651618816  f1= 0.0047175566777147996  f2= 0.022681331013553796  f3= 0.0016257757034455426  f4= 0.1388941994232916 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 890  time= 37.84302258491516  loss= 0.03235821599440237  val_loss= 23.0121483987937  f1= 0.0034972788478058402  f2= 0.005230625615416725  f3= 0.0016502992221729166  f4= 0.02198001230900689 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 900  time= 38.28520226478577  loss= 0.17186986355331557  val_loss= 28.31722745315679  f1= 0.004778222902166536  f2= 0.017810023997053444  f3= 0.003009096748482251  f4= 0.14627251990561335 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 910  time= 38.7309935092926  loss= 0.08483910817491006  val_loss= 10.10931589264578  f1= 0.005377797721700691  f2= 0.014570808693987832  f3= 0.0008252488322150869  f4= 0.06406525292700645 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 920  time= 39.18039608001709  loss= 0.25530117076077424  val_loss= 17.875815111027222  f1= 0.0040637990040749685  f2= 0.020795509372381914  f3= 0.011253617546875849  f4= 0.21918824483744156 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 930  time= 39.60363221168518  loss= 0.035990847098840095  val_loss= 26.062087200684665  f1= 0.0035816089694853024  f2= 0.004025696016778542  f3= 0.00034161985835379184  f4= 0.028041922254222456 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 940  time= 40.0257134437561  loss= 0.15823422060458245  val_loss= 20.821906074268767  f1= 0.015167784990441854  f2= 0.007635623388218134  f3= 0.003690419773568247  f4= 0.13174039245235422 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 950  time= 40.466376543045044  loss= 0.10082108635554074  val_loss= 25.749224303976387  f1= 0.00708851471205483  f2= 0.012431040421641474  f3= 0.0005729122413338855  f4= 0.08072861898051055 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 960  time= 40.89426255226135  loss= 0.12840816877644456  val_loss= 13.937555693200029  f1= 0.007923486368605542  f2= 0.01852127493941701  f3= 0.00040970539230435427  f4= 0.10155370207611766 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 970  time= 41.32722067832947  loss= 0.08192162357324394  val_loss= 23.052088361170846  f1= 0.006232883158299646  f2= 0.017853173234321316  f3= 0.0007346426286627227  f4= 0.05710092455196023 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 980  time= 41.74159383773804  loss= 0.07306160458666865  val_loss= 28.841612523845193  f1= 0.004393110425902107  f2= 0.005458124997677253  f3= 0.00030173845596908385  f4= 0.06290863070712019 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 990  time= 42.14956188201904  loss= 0.054641997969136616  val_loss= 24.91626419046169  f1= 0.004776875594155944  f2= 0.005697406783227656  f3= 0.0009895700715678506  f4= 0.04317814552018516 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1000  time= 42.5305814743042  loss= 0.040937405992173485  val_loss= 29.96315706428043  f1= 0.0056458156755908925  f2= 0.008512416767992706  f3= 0.005477582132559222  f4= 0.021301591416030666 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1010  time= 42.932127714157104  loss= 0.1843101216647505  val_loss= 12.07852010024353  f1= 0.005982759607820255  f2= 0.02155007347878757  f3= 0.0004800329375795126  f4= 0.1562972556405632 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1020  time= 43.353769063949585  loss= 0.24489557767452724  val_loss= 23.953307895960492  f1= 0.005720721286731183  f2= 0.051964022403243944  f3= 0.0006346614831100862  f4= 0.18657617250144204 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1030  time= 43.75240421295166  loss= 0.05998648816388285  val_loss= 18.48714295099547  f1= 0.005547643441889808  f2= 0.004842277930360592  f3= 0.00012657405506129012  f4= 0.04946999273657116 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1040  time= 44.16201949119568  loss= 0.20871419379323647  val_loss= 29.336328377673265  f1= 0.011639214417032808  f2= 0.03278964682292072  f3= 0.008030207838179837  f4= 0.1562551247151031 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1050  time= 44.556190490722656  loss= 0.07492856032426211  val_loss= 20.825942154058225  f1= 0.007986193733872594  f2= 0.005432588735121914  f3= 0.0008298297553867345  f4= 0.060679948099880865 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1060  time= 44.96081209182739  loss= 0.08992928485133671  val_loss= 27.96838974415612  f1= 0.009288486528395602  f2= 0.00955111438393261  f3= 0.005153951437058555  f4= 0.06593573250194998 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1070  time= 45.35287022590637  loss= 0.3580125073942856  val_loss= 23.52853011782699  f1= 0.011953994551153199  f2= 0.04679321819582157  f3= 0.03286513626661103  f4= 0.26640015838069986 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1080  time= 45.75046515464783  loss= 0.07643906974054118  val_loss= 28.658175914358594  f1= 0.016224843224680548  f2= 0.005514055730800481  f3= 0.0018281414536729236  f4= 0.05287202933138724 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1090  time= 46.17504644393921  loss= 0.03282875848961485  val_loss= 29.010151448620356  f1= 0.005816892813632534  f2= 0.00498375578392011  f3= 0.0021910615379267134  f4= 0.01983704835413549 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1100  time= 46.60720777511597  loss= 0.030096474916748828  val_loss= 30.38838870301944  f1= 0.009760820715493213  f2= 0.0034424296984611915  f3= 0.004020216908130964  f4= 0.01287300759466346 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1110  time= 47.028456687927246  loss= 0.11066521657633058  val_loss= 29.0568785340985  f1= 0.014420987625356606  f2= 0.027013790779461356  f3= 0.004931464614271976  f4= 0.06429897355724065 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1120  time= 47.459954500198364  loss= 0.12415149327488313  val_loss= 25.46068747361074  f1= 0.006131057855824994  f2= 0.018810689071767573  f3= 0.0017317574573131754  f4= 0.09747798888997737 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1130  time= 47.882055044174194  loss= 0.04980167874477955  val_loss= 19.80901553958279  f1= 0.010960707065806038  f2= 0.0051778934505674525  f3= 0.003588446731713806  f4= 0.03007463149669226 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1140  time= 48.3328332901001  loss= 0.1396621188198272  val_loss= 21.940007129525426  f1= 0.00788609344394845  f2= 0.025180165929601418  f3= 0.003077180180101369  f4= 0.10351867926617596 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1150  time= 48.756678342819214  loss= 0.07276421081132524  val_loss= 33.11123007901221  f1= 0.007603203024377531  f2= 0.007025220641728937  f3= 0.00033139439237200467  f4= 0.057804392752846774 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1160  time= 49.15973711013794  loss= 0.08267730437038148  val_loss= 15.148648758703121  f1= 0.00944511676112193  f2= 0.006485016646677698  f3= 0.0011557505342902033  f4= 0.06559142042829165 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1170  time= 49.58599495887756  loss= 0.08638530253141108  val_loss= 22.299805318746433  f1= 0.011259406221195877  f2= 0.006127797503987745  f3= 0.00042666543460791954  f4= 0.06857143337161954 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1180  time= 50.03210425376892  loss= 0.07577104343022319  val_loss= 16.382620596725594  f1= 0.003286199926581947  f2= 0.012596363301936645  f3= 0.0036279395547011255  f4= 0.05626054064700346 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1190  time= 50.427043437957764  loss= 0.07558797342400744  val_loss= 25.17084270313379  f1= 0.005742867768708205  f2= 0.005953978852999858  f3= 0.0033286102812740137  f4= 0.06056251652102535 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1200  time= 50.790780544281006  loss= 0.09180512579269866  val_loss= 23.918139395750018  f1= 0.0073072825193422185  f2= 0.011184812458174502  f3= 0.0011700540782939343  f4= 0.072142976736888 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1210  time= 51.17036247253418  loss= 0.3277131581112885  val_loss= 15.140411665555751  f1= 0.008135613410424026  f2= 0.03472686677083332  f3= 0.02087797431993956  f4= 0.26397270361009156 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1220  time= 51.615304946899414  loss= 0.21450560604594202  val_loss= 26.667301348691925  f1= 0.004884957305285037  f2= 0.02176952970326011  f3= 0.0017634067388645198  f4= 0.18608771229853235 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1230  time= 52.04600191116333  loss= 0.032121570134780945  val_loss= 24.854813326306374  f1= 0.002353107876286633  f2= 0.008047466883436695  f3= 0.001035886564102788  f4= 0.02068510881095483 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1240  time= 52.473520278930664  loss= 0.07479064331371822  val_loss= 22.591721793665567  f1= 0.004866285008398132  f2= 0.014209934805397192  f3= 0.004587288762285078  f4= 0.0511271347376378 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1250  time= 52.87801265716553  loss= 0.09611042890787731  val_loss= 19.21288763977495  f1= 0.009902814066027662  f2= 0.005438298860530399  f3= 0.02563217875250015  f4= 0.05513713722881909 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1260  time= 53.30857801437378  loss= 0.4864412427125688  val_loss= 33.9631420515022  f1= 0.014659640127669868  f2= 0.029884005084007414  f3= 0.008920640266791431  f4= 0.43297695723410007 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1270  time= 53.74796938896179  loss= 0.21839009528429143  val_loss= 33.28731639182086  f1= 0.012869482574091989  f2= 0.060202110216120666  f3= 0.0015183322928290642  f4= 0.14380017020124972 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1280  time= 54.176917552948  loss= 0.024298562469593075  val_loss= 17.718897632885067  f1= 0.0037791625447237305  f2= 0.003872649895973419  f3= 0.0010653807128921278  f4= 0.015581369316003799 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1290  time= 54.57083010673523  loss= 0.23889308961246788  val_loss= 29.160245040649578  f1= 0.011147863928453818  f2= 0.022420433879937206  f3= 0.004997116030418961  f4= 0.20032767577365787 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1300  time= 55.471413135528564  loss= 0.16376154409863738  val_loss= 27.953818700637726  f1= 0.0077869837171181166  f2= 0.0031150004185854113  f3= 0.006017547169970432  f4= 0.1468420127929634 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1310  time= 55.92128086090088  loss= 0.16108500793418304  val_loss= 19.844870992197507  f1= 0.005645021401033504  f2= 0.03323101028567031  f3= 0.00020520025384515567  f4= 0.1220037759936341 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1320  time= 56.35543131828308  loss= 0.09836465525219225  val_loss= 36.03788251541419  f1= 0.003665682334945277  f2= 0.006354246978298612  f3= 0.0024476555278348947  f4= 0.08589707041111345 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 1330  time= 56.74351119995117  loss= 0.04568756249691962  val_loss= 32.08625772793212  f1= 0.004757988272255352  f2= 0.002868544722989161  f3= 0.001040873981117247  f4= 0.03702015552055785 num_batches= 7 percent lr= 0.01\n","EarlyStopping counter: 996 out of 1000\n","EarlyStopping counter: 997 out of 1000\n","EarlyStopping counter: 998 out of 1000\n","EarlyStopping counter: 999 out of 1000\n","EarlyStopping counter: 1000 out of 1000\n","Early Stopping\n","training PINN Net\n"," \n","epoch= 0  time= 0.020462989807128906  loss= 14.822677196112485  val_loss= 15.321905330704222  f1= 5.323999404040625  f2= 9.30146592934295  f3= 0.17843120352575986  f4= 0.01878065920315197 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 10  time= 0.26120734214782715  loss= 15.453428183970926  val_loss= 47.928504445675365  f1= 1.010556820519065  f2= 7.699065801528673  f3= 0.135700119658074  f4= 6.608105442265115 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 20  time= 0.5180857181549072  loss= 14.735687008995246  val_loss= 46.11834806542863  f1= 0.9667681532218662  f2= 7.722740006243963  f3= 0.010583597040416335  f4= 6.035595252488998 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 30  time= 0.7686243057250977  loss= 11.165605116665136  val_loss= 48.333203089377236  f1= 0.9047405407561556  f2= 7.717407215964842  f3= 0.02823829208793553  f4= 2.5152190678562 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 40  time= 1.0245938301086426  loss= 9.430466334208942  val_loss= 42.69426413720042  f1= 0.7034373704985638  f2= 7.743906313627484  f3= 0.09549345969681211  f4= 0.8876291903860823 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 50  time= 1.2855000495910645  loss= 10.568840329091893  val_loss= 38.810525438475864  f1= 0.23951142149749557  f2= 7.7215954745132125  f3= 0.004556873290668089  f4= 2.603176559790516 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 60  time= 1.532069206237793  loss= 9.748312765069807  val_loss= 38.59012549844263  f1= 0.22421894637704726  f2= 7.680320421438758  f3= 0.0022875206389221573  f4= 1.8414858766150803 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 70  time= 1.8189499378204346  loss= 9.46333352842482  val_loss= 39.288167550060066  f1= 0.21441167259072844  f2= 7.636253384240517  f3= 0.004128374844709468  f4= 1.6085400967488659 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 80  time= 2.0608327388763428  loss= 9.142098422306702  val_loss= 43.78760439312489  f1= 0.15099319329779173  f2= 7.557494232856223  f3= 0.0003000387748436293  f4= 1.4333109573778438 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 90  time= 2.2981221675872803  loss= 9.3660602214413  val_loss= 42.050142366403364  f1= 0.12010137920594652  f2= 7.835017314556245  f3= 0.0166111614947782  f4= 1.3943303661843294 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 100  time= 2.538503646850586  loss= 8.797560398485972  val_loss= 40.58164884113636  f1= 0.10122914663452688  f2= 7.425940760218455  f3= 0.00038882905696189455  f4= 1.2700016625760264 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 110  time= 2.794490098953247  loss= 8.565823879419385  val_loss= 44.22513687183829  f1= 0.07448998068931091  f2= 7.3139190952664705  f3= 0.001074171468892401  f4= 1.1763406319947118 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 120  time= 3.021218776702881  loss= 8.730587711879577  val_loss= 38.769269031225704  f1= 0.07923531306804564  f2= 7.282596452122104  f3= 0.000561239703303185  f4= 1.3681947069861256 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 130  time= 3.248832941055298  loss= 7.857751788171968  val_loss= 46.00801347181612  f1= 0.11054724253393092  f2= 7.038265962048792  f3= 0.010499169869449992  f4= 0.6984394137197959 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 140  time= 3.518914222717285  loss= 7.532341716971523  val_loss= 48.32608073243033  f1= 0.08280793671958543  f2= 6.851112976282012  f3= 0.008472689581015928  f4= 0.589948114388909 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 150  time= 3.7774808406829834  loss= 7.351183727437106  val_loss= 54.82846964449292  f1= 0.06323208164484141  f2= 6.819690287301567  f3= 0.00218218591947269  f4= 0.4660791725712257 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 160  time= 4.039360523223877  loss= 7.512383509986597  val_loss= 42.26152329559591  f1= 0.06988293260529997  f2= 6.778504080381093  f3= 0.001583237942936112  f4= 0.6624132590572666 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 170  time= 4.299867153167725  loss= 16.367165159464506  val_loss= 41.92471753954774  f1= 0.061079247088421926  f2= 15.137151527028536  f3= 0.023370034581662168  f4= 1.1455643507658853 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 180  time= 4.579844236373901  loss= 6.888533055906104  val_loss= 46.143524675114584  f1= 0.06797657037494807  f2= 6.409349902088858  f3= 0.012497892917329697  f4= 0.3987086905249683 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 190  time= 4.868430852890015  loss= 6.491667931805982  val_loss= 44.75654291479118  f1= 0.05397280077893355  f2= 6.081970542635696  f3= 0.004710610810670443  f4= 0.3510139775806812 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 200  time= 5.128733396530151  loss= 6.308755721662253  val_loss= 44.86955032955956  f1= 0.04807511089337461  f2= 5.866543544782762  f3= 0.0009836050858830184  f4= 0.39315346090023484 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 210  time= 5.3792877197265625  loss= 15.48479994481616  val_loss= 42.15540052343563  f1= 0.05041289726895163  f2= 13.883940304563936  f3= 0.004776704336027128  f4= 1.5456700386472448 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 220  time= 5.630412578582764  loss= 7.911364599079156  val_loss= 47.715027704513346  f1= 0.04029339049518  f2= 5.807889854746315  f3= 0.011790838986436078  f4= 2.051390514851225 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 230  time= 5.88500189781189  loss= 5.648382496656225  val_loss= 50.92174515250725  f1= 0.03618449604818343  f2= 5.249237050298694  f3= 0.013096941590188775  f4= 0.3498640087191592 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 240  time= 6.130697965621948  loss= 7.690928426006198  val_loss= 47.62837991317677  f1= 0.05251719177037927  f2= 5.235614570453739  f3= 0.018113847065726017  f4= 2.384682816716353 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 250  time= 6.407916307449341  loss= 6.360368382394304  val_loss= 44.107966673717975  f1= 0.07982685624541355  f2= 5.559154959571127  f3= 0.014000013804446996  f4= 0.7073865527733162 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 260  time= 6.65649151802063  loss= 9.206133213992048  val_loss= 50.941720663418  f1= 0.04498407781943462  f2= 5.142815585966007  f3= 0.0014165421036710121  f4= 4.0169170081029355 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 270  time= 7.066919565200806  loss= 16.60490228377736  val_loss= 43.66999797252288  f1= 0.04393648704729599  f2= 12.921677883186007  f3= 0.01193462959883053  f4= 3.6273532839452267 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 280  time= 7.74757981300354  loss= 6.4381024539015135  val_loss= 45.51076840198411  f1= 0.030618563578397966  f2= 5.450176869355396  f3= 0.00716110838329026  f4= 0.9501459125844275 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 290  time= 8.049531698226929  loss= 7.435397330219611  val_loss= 44.73035050332868  f1= 0.03397554096829799  f2= 5.259130681396586  f3= 0.022180292785252582  f4= 2.120110815069476 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 300  time= 8.794914484024048  loss= 5.525783857280944  val_loss= 44.082241647067484  f1= 0.03167922815448652  f2= 5.16390931047234  f3= 0.005627277772552805  f4= 0.3245680408815632 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 310  time= 9.038259029388428  loss= 5.371487262914182  val_loss= 48.1084465182326  f1= 0.03165542454104768  f2= 5.08427844981105  f3= 0.011310735833949436  f4= 0.2442426527281362 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 320  time= 9.261456966400146  loss= 5.5707420289726395  val_loss= 43.57178296965609  f1= 0.03085998894344531  f2= 4.9984165242676655  f3= 0.0030243067956517303  f4= 0.5384412089658795 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 330  time= 9.506094932556152  loss= 5.440235416134703  val_loss= 47.52978636443188  f1= 0.02981617092416964  f2= 4.766619914792921  f3= 0.0012728065085479878  f4= 0.6425265239090642 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 340  time= 9.762125730514526  loss= 5.34471723589661  val_loss= 50.40516673651933  f1= 0.060036642627782556  f2= 5.057860238770935  f3= 0.0018129985335439615  f4= 0.22500735596434748 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 350  time= 10.015297412872314  loss= 5.104688160745795  val_loss= 50.44109418084152  f1= 0.032424195780231  f2= 4.698895291711266  f3= 0.007740388726256357  f4= 0.36562828452804114 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 360  time= 10.261279821395874  loss= 5.062596085978553  val_loss= 44.29351016063627  f1= 0.031203546192306624  f2= 4.79710638600635  f3= 0.0012876414357349021  f4= 0.23299851234416152 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 370  time= 10.510937213897705  loss= 6.082335129661316  val_loss= 45.73527495577762  f1= 0.026246894489847024  f2= 4.664431926369038  f3= 0.0039514845348863565  f4= 1.3877048242675447 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 380  time= 10.776015043258667  loss= 6.7256099654297845  val_loss= 49.708625338721305  f1= 0.02769638991489243  f2= 4.575420301595512  f3= 0.049722176664260584  f4= 2.072771097255119 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 390  time= 11.016158103942871  loss= 5.738049750314117  val_loss= 45.94518535609895  f1= 0.04664666745092404  f2= 4.635193256327845  f3= 0.00316572137498473  f4= 1.0530441051603634 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 400  time= 11.242465019226074  loss= 5.667649326582711  val_loss= 51.388873375903074  f1= 0.026828034497840405  f2= 4.635563350011409  f3= 0.03646389181450945  f4= 0.9687940502589535 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 410  time= 11.484310150146484  loss= 4.5260400466602  val_loss= 55.824410302096055  f1= 0.03205862112464202  f2= 4.235692306662942  f3= 0.0038871336232790036  f4= 0.2544019852493356 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 420  time= 11.748411893844604  loss= 4.800077481573483  val_loss= 54.19831141315468  f1= 0.028009407348608732  f2= 4.580036350913212  f3= 0.007292034409100743  f4= 0.18473968890256015 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 430  time= 12.008606910705566  loss= 5.183934016196891  val_loss= 51.4238824176822  f1= 0.01918966972187185  f2= 4.659031557668278  f3= 0.0021316932199060288  f4= 0.5035810955868351 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 440  time= 12.255414009094238  loss= 5.36484293822089  val_loss= 52.31624303839041  f1= 0.027853184271791138  f2= 4.642494111499427  f3= 0.008141262586213156  f4= 0.6863543798634576 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 450  time= 12.503401517868042  loss= 5.902712540920348  val_loss= 63.29297026197399  f1= 0.02411539489247747  f2= 5.096730998581518  f3= 0.007473549169610428  f4= 0.7743925982767423 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 460  time= 12.769944906234741  loss= 5.450484152111819  val_loss= 49.56733208502012  f1= 0.03867862274338625  f2= 4.2520334642678375  f3= 0.003697870859071477  f4= 1.1560741942415238 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 470  time= 13.043663740158081  loss= 4.811021770738399  val_loss= 51.501691798869594  f1= 0.02088016287124474  f2= 4.5171230273168295  f3= 0.007418628583490861  f4= 0.26559995196683367 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 480  time= 13.300316572189331  loss= 4.611260786399865  val_loss= 51.32789071003002  f1= 0.01999656618546231  f2= 4.390996109592107  f3= 0.006545082375822635  f4= 0.19372302824647222 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 490  time= 13.561711549758911  loss= 5.874158526993218  val_loss= 49.38275189059825  f1= 0.01959717359278135  f2= 4.2809258135703185  f3= 0.018746050848071797  f4= 1.5548894889820468 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 500  time= 13.8208327293396  loss= 5.228730348106415  val_loss= 50.618278860748276  f1= 0.025124651170435264  f2= 4.39793153952417  f3= 0.00905727424477408  f4= 0.7966168831670354 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 510  time= 14.063393831253052  loss= 4.782611858008642  val_loss= 52.53189598846398  f1= 0.030497798009476107  f2= 4.332306282527417  f3= 0.007799195567163166  f4= 0.4120085819045851 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 520  time= 14.30438232421875  loss= 5.363939603086058  val_loss= 62.65093537827078  f1= 0.02447746416103166  f2= 3.8485140500234656  f3= 0.03082540947451169  f4= 1.4601226794270494 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 530  time= 14.547728061676025  loss= 5.029624034800618  val_loss= 52.35701617763867  f1= 0.02055638256215852  f2= 4.043259502646569  f3= 0.01798058476358691  f4= 0.9478275648283045 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 540  time= 14.793061971664429  loss= 4.73674870651922  val_loss= 63.40510948600041  f1= 0.035542614978158346  f2= 4.015867011113757  f3= 0.005232945452041858  f4= 0.6801061349752631 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 550  time= 15.068742513656616  loss= 6.786369450683209  val_loss= 56.13880457769503  f1= 0.025907862969559582  f2= 3.7589890623519624  f3= 0.04686065253395869  f4= 2.954611872827726 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 560  time= 15.320780515670776  loss= 4.662562947065223  val_loss= 58.519768083648295  f1= 0.018746316210853736  f2= 4.022724607134986  f3= 0.0021468329199496525  f4= 0.6189451907994332 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 570  time= 15.571923017501831  loss= 4.752920620149912  val_loss= 59.287962802010604  f1= 0.031806299746336524  f2= 3.806646688578435  f3= 0.0034220911833899048  f4= 0.9110455406417518 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 580  time= 15.833548307418823  loss= 4.018802153564587  val_loss= 61.2169796632758  f1= 0.019008179708759208  f2= 3.752349096239135  f3= 0.00966249232714358  f4= 0.23778238528954973 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 590  time= 16.081941843032837  loss= 4.377967298404678  val_loss= 59.964322160141386  f1= 0.0192091141986975  f2= 4.041383340508007  f3= 0.02737266848795449  f4= 0.2900021752100181 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 600  time= 16.324686527252197  loss= 4.640716818660387  val_loss= 50.236592499050715  f1= 0.03581617204079808  f2= 3.6043612081011567  f3= 0.00909734501022281  f4= 0.9914420935082101 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 610  time= 16.558522701263428  loss= 4.794726167273653  val_loss= 46.082273995196395  f1= 0.06354328840310142  f2= 3.833104988452962  f3= 0.0022022189043556174  f4= 0.8958756715132347 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 620  time= 16.79499101638794  loss= 4.540522494173925  val_loss= 62.32544467701007  f1= 0.01818487979817959  f2= 4.176776805452008  f3= 0.0011891919975024082  f4= 0.344371616926235 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 630  time= 17.040645122528076  loss= 4.904961265236567  val_loss= 54.48855640749737  f1= 0.01380465646954101  f2= 3.731623020759653  f3= 0.00201490948504852  f4= 1.1575186785223248 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 640  time= 17.279401540756226  loss= 4.544732980314189  val_loss= 53.874499947639144  f1= 0.022718047541465676  f2= 3.7879694004241853  f3= 0.006221147955523253  f4= 0.7278243843930136 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 650  time= 17.57882833480835  loss= 5.225622442258076  val_loss= 49.86986031384136  f1= 0.016434109129370756  f2= 4.065057018209729  f3= 0.003019208939133307  f4= 1.1411121059798428 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 660  time= 18.208221435546875  loss= 4.134058669093371  val_loss= 57.47481899497065  f1= 0.023282302492512157  f2= 3.7772985277445055  f3= 0.022595837005763484  f4= 0.31088200185058973 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 670  time= 18.46614718437195  loss= 4.945235895736969  val_loss= 59.578313100848675  f1= 0.021368645684387254  f2= 3.756762003581415  f3= 0.002908945405984744  f4= 1.1641963010651823 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 680  time= 18.7328622341156  loss= 3.813179750159134  val_loss= 64.08183579081178  f1= 0.07973947662107553  f2= 3.5128675855350378  f3= 9.423905684017534e-05  f4= 0.22047844894618182 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 690  time= 18.984567642211914  loss= 4.59993666982916  val_loss= 56.80611465976056  f1= 0.016155439852448373  f2= 4.067831955634595  f3= 0.005115123613326977  f4= 0.5108341507287888 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 700  time= 19.25831937789917  loss= 4.689440085551964  val_loss= 61.60933455630649  f1= 0.01690813845391156  f2= 3.4844521880103874  f3= 0.00510054834269643  f4= 1.1829792107449684 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 710  time= 19.497610807418823  loss= 4.249230819267998  val_loss= 62.53299672354178  f1= 0.013455692716675753  f2= 3.524896591470698  f3= 0.008871419652772185  f4= 0.7020071154278532 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 720  time= 19.734174728393555  loss= 4.002732827810838  val_loss= 57.50202611453943  f1= 0.021708178977018162  f2= 3.51447430164512  f3= 0.0007719524572802793  f4= 0.46577839473141874 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 730  time= 20.0063157081604  loss= 4.133277257122945  val_loss= 60.549176575201116  f1= 0.015425148542885094  f2= 3.485022295008665  f3= 0.00885694043502007  f4= 0.623972873136376 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 740  time= 20.262601137161255  loss= 4.828974756379307  val_loss= 62.78181242216313  f1= 0.014828982382435811  f2= 2.945668719504695  f3= 0.001302542744888055  f4= 1.8671745117472887 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 750  time= 20.522335290908813  loss= 5.6171105023637  val_loss= 59.16458423991166  f1= 0.01473906584940378  f2= 3.7732972989775773  f3= 0.0010261334880104314  f4= 1.8280480040487075 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 760  time= 20.78483486175537  loss= 3.537711296921766  val_loss= 62.313169520013766  f1= 0.04105427224491572  f2= 3.02746157089145  f3= 0.006970586297513138  f4= 0.4622248674878879 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 770  time= 21.027135610580444  loss= 4.003132600431298  val_loss= 57.04197807331096  f1= 0.04271474828709277  f2= 3.410019540760835  f3= 0.009158878882623196  f4= 0.541239432500747 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 780  time= 21.274086475372314  loss= 5.612937046123248  val_loss= 64.73903935005117  f1= 0.03943103782021296  f2= 3.106763639666472  f3= 0.009908828896039135  f4= 2.4568335397405243 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 790  time= 21.51397728919983  loss= 5.020263131879825  val_loss= 72.72188827843742  f1= 0.020389609363649305  f2= 3.200461927229355  f3= 0.012651925532561321  f4= 1.7867596697542605 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 800  time= 21.766279220581055  loss= 5.536621403713946  val_loss= 55.65088474836435  f1= 0.02966489968990917  f2= 3.048489572846346  f3= 0.00015231291714788765  f4= 2.4583146182605438 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 810  time= 22.016168117523193  loss= 3.316420976736009  val_loss= 70.24252651115161  f1= 0.01154988563261859  f2= 3.0349397734656973  f3= 0.004030207258873506  f4= 0.2659011103788193 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 820  time= 22.29171323776245  loss= 2.914825873381189  val_loss= 62.12001717877097  f1= 0.012902337951504472  f2= 2.718640836415949  f3= 0.0012845174076330128  f4= 0.1819981816061011 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 830  time= 22.5337233543396  loss= 5.227568993690666  val_loss= 59.23600930869972  f1= 0.020996088693720518  f2= 2.098809412629416  f3= 0.0015482386826477079  f4= 3.106215253684882 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 840  time= 22.791385650634766  loss= 3.1323331398436594  val_loss= 60.30390866495649  f1= 0.0239676628925461  f2= 2.554988042559599  f3= 0.00247374224955613  f4= 0.5509036921419581 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 850  time= 23.033171892166138  loss= 3.32501738091331  val_loss= 74.15360582165323  f1= 0.06774349862678913  f2= 2.1787592432840275  f3= 0.002351961385931258  f4= 1.076162677616562 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 860  time= 23.29864501953125  loss= 3.678688768143739  val_loss= 62.249048136058164  f1= 0.015417798797879235  f2= 2.3181557838147984  f3= 0.0075059049704207315  f4= 1.3376092805606405 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 870  time= 23.5485360622406  loss= 8.510890524597048  val_loss= 74.09211054275046  f1= 0.03631071600609551  f2= 1.8227167424730353  f3= 0.0034573390621274527  f4= 6.648405727055789 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 880  time= 23.78917956352234  loss= 4.003967449609883  val_loss= 84.21895891514984  f1= 0.017039733835183788  f2= 1.7286390034727763  f3= 0.001021609252220635  f4= 2.257267103049702 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 890  time= 24.048673152923584  loss= 2.3484444206165103  val_loss= 72.18145570645345  f1= 0.02457802335945725  f2= 1.7391192603659629  f3= 0.0029626617669170742  f4= 0.5817844751241734 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 900  time= 24.30357050895691  loss= 2.1718968873066418  val_loss= 75.49926924290853  f1= 0.02662949523049165  f2= 1.4857892903267835  f3= 0.00400366710638566  f4= 0.655474434642981 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 910  time= 24.549222230911255  loss= 1.7996619822679993  val_loss= 70.91319355965423  f1= 0.02168161528734113  f2= 1.6316760979543368  f3= 0.005547853076996457  f4= 0.1407564159493249 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 920  time= 24.8195219039917  loss= 31.733943334784183  val_loss= 51.13451829306509  f1= 0.03226609836934442  f2= 4.119039112232745  f3= 0.0023729370791942916  f4= 27.580265187102896 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 930  time= 25.07379126548767  loss= 12.384374354697092  val_loss= 78.34808913816366  f1= 0.02783800368481909  f2= 4.528734703126237  f3= 0.0014216595644077237  f4= 7.826379988321628 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 940  time= 25.343602657318115  loss= 3.9253963668312197  val_loss= 60.19148210091011  f1= 0.01599602889951961  f2= 1.3148974558052196  f3= 0.005961938284026448  f4= 2.5885409438424545 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 950  time= 25.589606046676636  loss= 2.9372687071912127  val_loss= 64.01288774614785  f1= 0.011844888207075927  f2= 1.3713563522879748  f3= 0.0034728394074887946  f4= 1.5505946272886733 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 960  time= 25.84759283065796  loss= 2.0033516092817196  val_loss= 70.2226948774398  f1= 0.016508676597349677  f2= 1.8196876088664884  f3= 0.0017572890083654934  f4= 0.16539803480951565 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 970  time= 26.101112127304077  loss= 7.111908083974419  val_loss= 54.381678608939396  f1= 0.00969976974455681  f2= 4.229552097748621  f3= 0.0008948140140206207  f4= 2.871761402467221 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 980  time= 26.37299156188965  loss= 2.769972006611148  val_loss= 70.53494282678656  f1= 0.01155258596335638  f2= 1.0270337412488106  f3= 0.021089621907428698  f4= 1.710296057491552 num_batches= 7 percent lr= 0.01\n"," \n","epoch= 990  time= 26.63787055015564  loss= 8.682148066014724  val_loss= 89.16185082083148  f1= 0.010784567099106654  f2= 1.1169760916516798  f3= 0.00019860867602189267  f4= 7.5541887985879175 num_batches= 7 percent lr= 0.01\n","EarlyStopping counter: 996 out of 1000\n","EarlyStopping counter: 997 out of 1000\n","EarlyStopping counter: 998 out of 1000\n"," \n","epoch= 1000  time= 26.913933515548706  loss= 2.4299401189030485  val_loss= 50.415208822568665  f1= 0.024484983534598572  f2= 1.3803812391177956  f3= 0.0017320261685092775  f4= 1.0233418700821448 num_batches= 7 percent lr= 0.01\n","EarlyStopping counter: 999 out of 1000\n","EarlyStopping counter: 1000 out of 1000\n","Early Stopping\n","[[ 0.39050803  1.72151493  0.82210701 ...  3.55062071  1.49426694\n","  -1.69939694]\n"," [ 2.15199138 -3.33468183  3.79819538 ...  0.79859667 -2.17563121\n","  -0.66574602]\n"," [-1.453646    1.14283158 -0.55172262 ... -2.6268154   3.34729822\n","  -1.16236254]\n"," [-2.26400987  3.29655388  3.7608362  ...  2.49491487  0.70991577\n","   0.52851388]] [[-1.45364787  1.14282995 -0.55172332 ... -2.62681909  3.34729416\n","  -1.16234857]\n"," [-2.26402015  3.29655704  3.76083296 ...  2.49491404  0.70992169\n","   0.52851935]\n"," [-0.03732466 -0.0325755  -0.01400542 ... -0.07366528 -0.08127372\n","   0.27950089]\n"," [-0.20571429  0.06309556 -0.06471144 ... -0.01657181  0.11831819\n","   0.10948758]] [3.16219472 5.82020003 6.96681916 ... 6.28760323 5.47531272 0.26730623]\n","training sepNet\n"," \n","epoch= 0  time= 0.049082279205322266  loss= 19.94058228995331  val_loss= 86.94765725518886  f1= 5.257945353296756  f2= 14.44409793584874  f3= 0.23839584031090338  f4= 0.0001431604969133371 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 10  time= 0.705683708190918  loss= 48.74403498866762  val_loss= 155.49951774676623  f1= 0.8682382161931645  f2= 17.30543671389777  f3= 0.456374254736151  f4= 30.113985803840535 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 20  time= 1.350400447845459  loss= 46.23859996544814  val_loss= 145.996085671291  f1= 0.6373146868230701  f2= 17.302445932764662  f3= 0.020649316775977502  f4= 28.27819002908444 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 30  time= 1.9659597873687744  loss= 31.16367095310104  val_loss= 138.01019836194055  f1= 0.7296934661215734  f2= 14.673972515517356  f3= 0.021012448907801056  f4= 15.738992522554305 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 40  time= 2.9270806312561035  loss= 33.04488392977181  val_loss= 116.33318354293966  f1= 0.41910390018579563  f2= 14.321711935119797  f3= 0.08642638668653112  f4= 18.21764170777968 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 50  time= 3.7306978702545166  loss= 24.88762165300669  val_loss= 128.49896084213017  f1= 0.548957759637019  f2= 14.334433018710586  f3= 0.05922455265410509  f4= 9.945006322004982 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 60  time= 4.3161163330078125  loss= 24.763735571403878  val_loss= 139.0570586452044  f1= 0.4771150146151478  f2= 14.136777777168268  f3= 0.005453498772379041  f4= 10.144389280848092 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 70  time= 4.890448570251465  loss= 31.155061537435245  val_loss= 142.41548552602146  f1= 0.30507952731589744  f2= 13.81783185446586  f3= 0.09752541754658849  f4= 16.9346247381069 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 80  time= 5.454196214675903  loss= 32.96221801655619  val_loss= 138.77963685080093  f1= 0.22389121956736202  f2= 13.546379713377918  f3= 0.06447313179186666  f4= 19.12747395181905 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 90  time= 6.083914756774902  loss= 36.53460907245541  val_loss= 133.21436753487197  f1= 0.33667240971051177  f2= 13.350408453028782  f3= 0.24128585721742515  f4= 22.60624235249869 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 100  time= 6.743492841720581  loss= 26.803258459101873  val_loss= 145.01736642976707  f1= 0.30447817944194505  f2= 13.134293331803418  f3= 0.208640063073935  f4= 13.155846884782575 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 110  time= 7.372485160827637  loss= 40.22764209049232  val_loss= 157.76744316238194  f1= 0.15147836746126508  f2= 12.856700620160597  f3= 0.014384057446441868  f4= 27.20507904542401 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 120  time= 8.049705743789673  loss= 35.50841108792047  val_loss= 154.12661809393467  f1= 0.14984507174085102  f2= 12.691912692456537  f3= 0.024144822820589502  f4= 22.64250850090249 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 130  time= 8.686493158340454  loss= 29.44119318266924  val_loss= 155.4637138934107  f1= 0.2448219996827365  f2= 12.689292534658641  f3= 0.06573520172731796  f4= 16.441343446600538 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 140  time= 9.36060905456543  loss= 36.899719001296305  val_loss= 170.20911912253737  f1= 0.10093849902956163  f2= 12.121663005066058  f3= 0.04279990500347454  f4= 24.6343175921972 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 150  time= 10.000967979431152  loss= 33.509262478749285  val_loss= 167.89050322010195  f1= 0.1383219610797233  f2= 11.607261477822824  f3= 0.005853915723902495  f4= 21.757825124122835 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 160  time= 10.659555673599243  loss= 36.447506930483364  val_loss= 151.83632615793647  f1= 0.14022159209424306  f2= 11.386808501852432  f3= 0.0548458861622662  f4= 24.86563095037442 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 170  time= 11.380375385284424  loss= 54.693916530855056  val_loss= 173.35795929918982  f1= 0.04227665632575017  f2= 11.921529237222417  f3= 0.005479437337229025  f4= 42.72463119996965 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 180  time= 12.058569192886353  loss= 39.322452199474895  val_loss= 170.2349599764723  f1= 0.08125071096937844  f2= 11.077228603982645  f3= 0.02676467873192307  f4= 28.13720820579095 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 190  time= 12.73360538482666  loss= 50.415702246960635  val_loss= 167.004320274374  f1= 0.08338523559700631  f2= 13.27589085227343  f3= 0.04771621788115229  f4= 37.00870994120905 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 200  time= 13.4268639087677  loss= 33.38503294515254  val_loss= 158.96435530968654  f1= 0.10387317978335998  f2= 10.314562970325472  f3= 0.041303703070056555  f4= 22.92529309197365 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 210  time= 14.090368032455444  loss= 28.996582500741184  val_loss= 157.79259717343794  f1= 0.09385236012782101  f2= 9.768936141703712  f3= 0.03519728067176691  f4= 19.09859671823788 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 220  time= 14.756663084030151  loss= 32.243337262696784  val_loss= 148.89919291508113  f1= 0.054743021692841634  f2= 9.33696955253126  f3= 0.00424746750131225  f4= 22.84737722097137 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 230  time= 15.525778770446777  loss= 26.98188721942248  val_loss= 132.98727732719706  f1= 0.07999468281915662  f2= 8.505653975121604  f3= 0.03206474084779726  f4= 18.364173820633916 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 240  time= 16.249574184417725  loss= 23.804560827909583  val_loss= 133.70270224267313  f1= 0.08464904321528975  f2= 8.068084736781232  f3= 0.06265438581082478  f4= 15.58917266210224 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 250  time= 16.950271368026733  loss= 24.08611951985284  val_loss= 125.33986074511081  f1= 0.06912159758294337  f2= 7.5025333387817374  f3= 0.054564349278094885  f4= 16.45990023421006 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 260  time= 17.69877290725708  loss= 26.4850107151435  val_loss= 119.98329233587945  f1= 0.034061475786089104  f2= 7.329172569410539  f3= 0.010653759019267463  f4= 19.111122910927595 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 270  time= 18.411128282546997  loss= 21.828430110552898  val_loss= 107.75902884087418  f1= 0.04427309101341736  f2= 6.593539213897099  f3= 0.023565452616388392  f4= 15.16705235302599 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 280  time= 19.11776065826416  loss= 25.992555674886304  val_loss= 107.99320925048437  f1= 0.027569572744827405  f2= 6.907598641701895  f3= 0.009448576730098553  f4= 19.047938883709488 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 290  time= 19.825778484344482  loss= 27.602952253960527  val_loss= 117.53654905528994  f1= 0.04561176463160342  f2= 6.823398993914127  f3= 0.011237256540120553  f4= 20.72270423887468 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 300  time= 20.53296208381653  loss= 27.430207495639692  val_loss= 102.4661321073058  f1= 0.03641518903979806  f2= 7.146893260480137  f3= 0.0975550885223755  f4= 20.149343957597385 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 310  time= 21.250442028045654  loss= 30.43527258481597  val_loss= 103.149575230192  f1= 0.016317705473261224  f2= 6.96668363633863  f3= 0.02802458297993904  f4= 23.42424666002414 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 320  time= 22.05553936958313  loss= 24.79089596709413  val_loss= 93.97627353172165  f1= 0.033929909260754974  f2= 6.693029273564248  f3= 0.02853000912961656  f4= 18.035406775139514 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 330  time= 22.81158137321472  loss= 21.17281432259421  val_loss= 113.36782802209608  f1= 0.03783514184067453  f2= 6.306856277405948  f3= 0.09979010613799838  f4= 14.728332797209582 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 340  time= 23.562340259552002  loss= 25.58582916292034  val_loss= 97.83866339576919  f1= 0.01853946139401924  f2= 6.304820096120758  f3= 0.019722386040633373  f4= 19.242747219364933 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 350  time= 24.28056764602661  loss= 26.2597505735232  val_loss= 107.79398372583391  f1= 0.03341528471722882  f2= 6.569766296078192  f3= 0.028682814651829733  f4= 19.62788617807594 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 360  time= 24.971878051757812  loss= 22.17996626985608  val_loss= 104.03743650096511  f1= 0.025278329843180427  f2= 6.177107867787371  f3= 0.01732256869353447  f4= 15.960257503531997 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 370  time= 25.738511562347412  loss= 25.114516762341296  val_loss= 99.51527378971569  f1= 0.027346126382005183  f2= 6.331041323840074  f3= 0.005392940854153976  f4= 18.75073637126506 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 380  time= 26.48515295982361  loss= 28.655118505544788  val_loss= 90.50942569544374  f1= 0.022428744974291502  f2= 6.473659222671923  f3= 0.005677996690420474  f4= 22.15335254120815 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 390  time= 27.198601007461548  loss= 26.869157638607128  val_loss= 92.66623649414146  f1= 0.016305045574195175  f2= 6.334181196364437  f3= 0.01606233806830164  f4= 20.502609058600196 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 400  time= 27.975404977798462  loss= 27.953998987528205  val_loss= 109.43149492128578  f1= 0.029573669010809402  f2= 6.3001192255528435  f3= 0.024822780339682347  f4= 21.599483312624876 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 410  time= 28.731221675872803  loss= 34.21535106500626  val_loss= 108.45882527554438  f1= 0.013949373310445958  f2= 7.688954164380158  f3= 0.002201669623060647  f4= 26.510245857692595 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 420  time= 29.495238304138184  loss= 26.569957125174206  val_loss= 102.70804609245106  f1= 0.045372075836938364  f2= 6.412474416950823  f3= 0.08739638780047064  f4= 20.024714244585976 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 430  time= 30.226043939590454  loss= 26.416000262248645  val_loss= 109.24473292892408  f1= 0.02131329957613185  f2= 5.891579016654918  f3= 0.0012996009742688402  f4= 20.501808345043326 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 440  time= 30.889111042022705  loss= 29.646712365929375  val_loss= 101.73702928929977  f1= 0.016934840081679666  f2= 6.277093298242221  f3= 0.0016519445148336608  f4= 23.351032283090642 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 450  time= 31.56164002418518  loss= 21.070892224865535  val_loss= 117.04363334190727  f1= 0.02142259041426645  f2= 5.8011136460476  f3= 0.059963190597883735  f4= 15.188392797805788 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 460  time= 32.221829891204834  loss= 23.459658193588698  val_loss= 109.50958824201275  f1= 0.015314557157077784  f2= 5.805564785076688  f3= 0.003606618600720261  f4= 17.63517223275422 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 470  time= 32.91738033294678  loss= 25.032493595819684  val_loss= 97.89949014204937  f1= 0.031064161910191516  f2= 6.1650795942592485  f3= 0.17456665487229378  f4= 18.661783184777953 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 480  time= 33.659651041030884  loss= 24.807757316484807  val_loss= 92.49608366541304  f1= 0.030322178526239477  f2= 6.416984267491699  f3= 0.004812878687103348  f4= 18.355637991779762 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 490  time= 34.42490100860596  loss= 21.528675929406266  val_loss= 111.80341297506529  f1= 0.026752440429297135  f2= 5.587739081184294  f3= 0.0053373433667027285  f4= 15.908847064425977 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 500  time= 35.11670684814453  loss= 27.259720716990334  val_loss= 114.51203748347395  f1= 0.014091047961526965  f2= 5.643025442376301  f3= 0.011371239974295717  f4= 21.591232986678207 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 510  time= 35.93289566040039  loss= 21.699814233078296  val_loss= 107.58137362645631  f1= 0.01660304758479473  f2= 5.572141006931441  f3= 0.016345615164663162  f4= 16.09472456339739 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 520  time= 36.701653718948364  loss= 21.28594033509677  val_loss= 104.05440628136098  f1= 0.02149155278830193  f2= 5.8088851664501515  f3= 0.016350923301120538  f4= 15.439212692557202 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 530  time= 37.45833969116211  loss= 28.019531590033914  val_loss= 100.11197747249071  f1= 0.012925280126560499  f2= 6.1747899871981895  f3= 0.0028046543470493677  f4= 21.829011668362114 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 540  time= 38.20992946624756  loss= 25.556176016098775  val_loss= 103.3290800908565  f1= 0.011811881758174539  f2= 5.982177103270743  f3= 0.004374639746181482  f4= 19.557812391323672 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 550  time= 38.95000123977661  loss= 27.073133245257296  val_loss= 110.07733764168803  f1= 0.010747835536639227  f2= 5.917161872991808  f3= 0.014392278317626279  f4= 21.13083125841122 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 560  time= 39.73152542114258  loss= 24.62549013031657  val_loss= 99.83846218188923  f1= 0.022970447191431854  f2= 5.821526226139586  f3= 0.005417245687018133  f4= 18.77557621129854 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 570  time= 40.47507619857788  loss= 35.772997959260906  val_loss= 95.0016021276481  f1= 0.07401125797678541  f2= 6.79899780823486  f3= 0.02973223016553726  f4= 28.870256662883722 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 580  time= 41.16742515563965  loss= 25.195353827227965  val_loss= 100.72630742311422  f1= 0.02046881387943428  f2= 5.952939003496539  f3= 0.01969436553435248  f4= 19.202251644317645 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 590  time= 41.861225843429565  loss= 26.991824434313017  val_loss= 108.03361873727793  f1= 0.012642699951527671  f2= 5.869161892314205  f3= 0.004593162250288387  f4= 21.105426679797 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 600  time= 42.52164912223816  loss= 23.84907760492176  val_loss= 95.88209590020303  f1= 0.03395518395521558  f2= 5.885584096279876  f3= 0.015757886548575094  f4= 17.913780438138097 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 610  time= 43.213993072509766  loss= 20.21561206920667  val_loss= 103.74077753228534  f1= 0.023636211756491818  f2= 5.577716341160526  f3= 0.03470595828495544  f4= 14.579553558004692 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 620  time= 43.89375638961792  loss= 27.515280431141427  val_loss= 99.93299718373198  f1= 0.015693371718965368  f2= 5.76054420957343  f3= 0.015374158969834354  f4= 21.723668690879187 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 630  time= 44.54722332954407  loss= 25.407520254485572  val_loss= 96.49105936603307  f1= 0.011278104117453284  f2= 5.508230732545882  f3= 0.00965451125225305  f4= 19.87835690656999 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 640  time= 45.2397198677063  loss= 24.886326653888112  val_loss= 100.63610598668039  f1= 0.013356076536520838  f2= 5.534457473609338  f3= 0.003001406087435337  f4= 19.33551169765482 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 650  time= 45.89658999443054  loss= 23.132629880149903  val_loss= 94.24133776573952  f1= 0.017512437090327272  f2= 5.7042115464302565  f3= 0.018579906018833692  f4= 17.39232599061048 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 660  time= 46.5968816280365  loss= 24.409263729536455  val_loss= 101.34304653552589  f1= 0.010499448700659608  f2= 5.54225134801062  f3= 0.001818935202708615  f4= 18.854693997622466 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 670  time= 47.333094358444214  loss= 23.631454582664784  val_loss= 94.41387061347416  f1= 0.011669046806724592  f2= 5.503984947744731  f3= 0.0015086611922280892  f4= 18.114291926921098 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 680  time= 48.063445806503296  loss= 24.527252659941702  val_loss= 90.81626245834182  f1= 0.011818225769456429  f2= 5.788851505472953  f3= 0.006454295042876957  f4= 18.720128633656415 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 690  time= 48.778950691223145  loss= 23.59043514966424  val_loss= 97.73365128649117  f1= 0.01648625814364862  f2= 5.455696144623388  f3= 0.010002527763107524  f4= 18.108250219134092 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 700  time= 49.45148515701294  loss= 19.787461336107523  val_loss= 103.10101571973598  f1= 0.017205727311221462  f2= 5.388806352450069  f3= 0.027295547195460436  f4= 14.354153709150761 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 710  time= 50.15210819244385  loss= 23.04431578943255  val_loss= 98.4451572635121  f1= 0.023957643001055053  f2= 5.621976931345036  f3= 0.06223696238576504  f4= 17.33614425270069 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 720  time= 50.8277428150177  loss= 19.528007950658942  val_loss= 97.20214055821792  f1= 0.02009009465819434  f2= 5.208014747299865  f3= 0.02562450906479321  f4= 14.274278599636084 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 730  time= 51.53063106536865  loss= 20.909447914861623  val_loss= 101.34551712286172  f1= 0.017036893727170477  f2= 5.444030253992646  f3= 0.023745213065314685  f4= 15.424635554076483 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 740  time= 52.31075978279114  loss= 19.46758254989832  val_loss= 102.9748973060731  f1= 0.01604272083344643  f2= 5.199211361281115  f3= 0.04907222386864817  f4= 14.20325624391511 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 750  time= 53.014995098114014  loss= 22.13229087792062  val_loss= 92.55046413132186  f1= 0.02094711280423156  f2= 5.361827742058006  f3= 0.002924222664112383  f4= 16.74659180039427 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 760  time= 53.795482873916626  loss= 29.447223546145587  val_loss= 97.60486035650327  f1= 0.011408079874354906  f2= 7.078353638012012  f3= 0.0030053812884317634  f4= 22.354456446970786 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 770  time= 54.46182155609131  loss= 29.808476321265594  val_loss= 101.28592846020615  f1= 0.023089916278348707  f2= 6.795008737344309  f3= 0.007937311838891823  f4= 22.98244035580404 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 780  time= 55.171826124191284  loss= 25.661919419456936  val_loss= 109.22693017488069  f1= 0.01466490467161288  f2= 5.363339730361096  f3= 0.009443591115690386  f4= 20.274471193308536 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 790  time= 55.8986451625824  loss= 20.220226401136838  val_loss= 94.90095263259639  f1= 0.026601953368845167  f2= 5.39877946288687  f3= 0.03148203632033933  f4= 14.763362948560781 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 800  time= 57.00407004356384  loss= 21.764449510624722  val_loss= 95.50110718457209  f1= 0.01621558568763032  f2= 5.312183852365682  f3= 0.01004162390992207  f4= 16.426008448661484 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 810  time= 58.098055362701416  loss= 22.50053163790824  val_loss= 91.62073041730228  f1= 0.014977008854730547  f2= 4.9690978049817565  f3= 0.04655504184365025  f4= 17.469901782228096 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 820  time= 58.787760496139526  loss= 21.178736016782562  val_loss= 91.33394615807983  f1= 0.010313363756518926  f2= 5.167903092791933  f3= 0.0029474391097340557  f4= 15.997572121124373 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 830  time= 59.458160400390625  loss= 22.386544179732457  val_loss= 93.38128961913077  f1= 0.01058551008607288  f2= 4.963832751602222  f3= 0.006958644076455819  f4= 17.40516727396771 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 840  time= 60.11951661109924  loss= 18.325587054092384  val_loss= 91.22079285322849  f1= 0.01504870108272979  f2= 5.056205233655647  f3= 0.05370399254742691  f4= 13.200629126806577 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 850  time= 60.85927629470825  loss= 25.59324370550678  val_loss= 93.81533394051932  f1= 0.011777779472933746  f2= 5.175512659946798  f3= 0.008667027044641387  f4= 20.397286239042405 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 860  time= 61.542741775512695  loss= 23.664434130766264  val_loss= 93.33939560320567  f1= 0.009499425824407649  f2= 5.0218726013854855  f3= 0.0035718570727698196  f4= 18.6294902464836 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 870  time= 62.26158118247986  loss= 23.424178247658784  val_loss= 101.37909750716857  f1= 0.011451009845931052  f2= 5.397449612448365  f3= 0.0006522710407286335  f4= 18.01462535432376 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 880  time= 62.99196195602417  loss= 19.52014979440641  val_loss= 108.68213818201228  f1= 0.013211231250971508  f2= 4.93548587276067  f3= 0.0008162156466024993  f4= 14.570636474748161 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 890  time= 63.7657368183136  loss= 27.283613074111503  val_loss= 99.48556454369765  f1= 0.013731815353469866  f2= 6.108364644248997  f3= 0.0051554068177129325  f4= 21.156361207691322 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 900  time= 64.49451470375061  loss= 20.193554274306653  val_loss= 103.30038853240467  f1= 0.010437755424314707  f2= 4.946297942542248  f3= 0.009905672340191206  f4= 15.226912903999896 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 910  time= 65.22834181785583  loss= 19.273515038771087  val_loss= 98.12282489929902  f1= 0.01182335469231287  f2= 4.624532125241642  f3= 0.040246589424660015  f4= 14.59691296941247 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 920  time= 65.90217733383179  loss= 19.277465065194498  val_loss= 89.06621906947991  f1= 0.019868041645041547  f2= 4.810471385912618  f3= 0.0032863415969451412  f4= 14.443839296039895 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 930  time= 66.59283185005188  loss= 28.490494966460826  val_loss= 105.44698010060809  f1= 0.019207540365149378  f2= 6.202077379326557  f3= 0.014749335581120678  f4= 22.254460711188 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 940  time= 67.30099892616272  loss= 20.921257066168998  val_loss= 93.11899434614641  f1= 0.023097286472688324  f2= 4.932415378236722  f3= 0.008817058201911697  f4= 15.956927343257671 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 950  time= 68.08253407478333  loss= 18.0092257665954  val_loss= 89.53621136926616  f1= 0.015182851125878805  f2= 4.7621339613819975  f3= 0.013289803764135192  f4= 13.218619150323386 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 960  time= 68.82302069664001  loss= 21.03881740540586  val_loss= 89.79846504121421  f1= 0.009649479843912578  f2= 4.889726878604577  f3= 0.0034865527984306375  f4= 16.135954494158945 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 970  time= 69.54875445365906  loss= 22.663687323234758  val_loss= 95.70027182387878  f1= 0.013834051158199683  f2= 4.815792402953797  f3= 0.023284400753317322  f4= 17.81077646836944 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 980  time= 70.25590205192566  loss= 18.195699909890486  val_loss= 100.64000124405537  f1= 0.015391718061306386  f2= 4.513048862855987  f3= 0.010096358982982997  f4= 13.657162969990207 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 990  time= 70.98380017280579  loss= 18.334617127365853  val_loss= 96.70392611545002  f1= 0.013941192704120788  f2= 4.723835235864209  f3= 0.0005278859374812514  f4= 13.596312812860042 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1000  time= 71.76506900787354  loss= 21.812701307004883  val_loss= 89.73273138434428  f1= 0.01065450532275526  f2= 4.859986163962984  f3= 0.008087993423257294  f4= 16.933972644295885 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1010  time= 72.50289845466614  loss= 18.387549285613865  val_loss= 86.72466095822408  f1= 0.019270307066353135  f2= 4.571471802797172  f3= 0.007814323334560915  f4= 13.78899285241578 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1020  time= 73.20656156539917  loss= 18.268953432818947  val_loss= 83.27885049764204  f1= 0.01590430357740157  f2= 4.571265438302769  f3= 0.019900871379181513  f4= 13.661882819559592 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1030  time= 73.89424419403076  loss= 18.073629965755426  val_loss= 95.09950330869556  f1= 0.019838106001650428  f2= 4.475042988842891  f3= 0.0023730475576537726  f4= 13.576375823353231 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1040  time= 74.61776685714722  loss= 21.098053035690903  val_loss= 84.21977115931948  f1= 0.011613072591442694  f2= 4.663886895474974  f3= 0.00033542943196483526  f4= 16.422217638192524 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1050  time= 75.35599660873413  loss= 18.00697791420377  val_loss= 89.35237880271508  f1= 0.008115359724440593  f2= 4.354735619940635  f3= 0.014988965127844263  f4= 13.629137969410852 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1060  time= 76.16283679008484  loss= 17.30986830023417  val_loss= 84.02950042836537  f1= 0.011781545130165987  f2= 4.087148748547613  f3= 0.012276358010558252  f4= 13.198661648545833 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1070  time= 76.93215227127075  loss= 18.047911184864574  val_loss= 95.33070324639556  f1= 0.016747575016052448  f2= 4.398150415352422  f3= 0.0469703958617212  f4= 13.586042798634379 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1080  time= 77.64311671257019  loss= 17.013695912930732  val_loss= 90.75907249414722  f1= 0.00968101750438091  f2= 3.907339955113782  f3= 0.0010433315066995384  f4= 13.095631608805869 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1090  time= 78.38742661476135  loss= 16.571125478653435  val_loss= 86.62166182766849  f1= 0.014726447604343253  f2= 3.973357410782477  f3= 0.02386134166920595  f4= 12.559180278597411 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1100  time= 79.14202356338501  loss= 18.908461307189267  val_loss= 89.16109429642847  f1= 0.014327156108458802  f2= 4.119100474479621  f3= 0.007186934574679939  f4= 14.767846742026506 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1110  time= 79.8746964931488  loss= 13.762507770744511  val_loss= 101.72680510817797  f1= 0.019953830229236354  f2= 3.5989465171945363  f3= 0.007866181608712945  f4= 10.135741241712024 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1120  time= 80.59016418457031  loss= 16.323410272772943  val_loss= 88.00264728957121  f1= 0.008496797613180576  f2= 3.9141626774825893  f3= 0.001151633638704258  f4= 12.399599164038468 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1130  time= 81.28689050674438  loss= 16.73673275881729  val_loss= 80.03058154134159  f1= 0.012650487798354391  f2= 3.9385239559335634  f3= 0.010105177104186918  f4= 12.775453137981186 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1140  time= 81.98338150978088  loss= 19.224069380560664  val_loss= nan  f1= 0.019417857063208685  f2= 4.003188372993266  f3= 0.01004520347328827  f4= 15.191417947030905 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1150  time= 82.69768857955933  loss= 15.919810170590761  val_loss= nan  f1= 0.015670333769929014  f2= 3.726903900176784  f3= 0.0185482706279956  f4= 12.158687666016053 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1160  time= 83.47060251235962  loss= 15.70410153076829  val_loss= nan  f1= 0.010728536473358958  f2= 3.637840405948371  f3= 0.0031222916082293737  f4= 12.052410296738328 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1170  time= 84.20740866661072  loss= 15.354295476150574  val_loss= nan  f1= 0.007717730157747572  f2= 3.3058009577051504  f3= 0.003895199350103383  f4= 12.036881588937568 num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1180  time= 84.95496106147766  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1190  time= 85.64378952980042  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1200  time= 86.28130722045898  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1210  time= 86.9802393913269  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1220  time= 87.64801478385925  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1230  time= 88.30440425872803  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1240  time= 89.00115919113159  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1250  time= 89.67996621131897  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1260  time= 90.36112451553345  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1270  time= 91.03897833824158  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1280  time= 91.73588466644287  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1290  time= 92.3951964378357  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1300  time= 93.0609073638916  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1310  time= 93.74755120277405  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1320  time= 94.40947246551514  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1330  time= 95.08303594589233  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1340  time= 95.81867122650146  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1350  time= 96.45470666885376  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1360  time= 97.1250319480896  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1370  time= 97.8231589794159  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1380  time= 98.53670191764832  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1390  time= 99.26219344139099  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1400  time= 99.95864248275757  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1410  time= 100.67222046852112  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1420  time= 101.3301031589508  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1430  time= 101.96397995948792  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1440  time= 102.63311910629272  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1450  time= 103.33940100669861  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1460  time= 104.03444790840149  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1470  time= 104.72134184837341  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1480  time= 105.35989165306091  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1490  time= 105.96473455429077  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1500  time= 106.58922982215881  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1510  time= 107.21842169761658  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1520  time= 107.86372089385986  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1530  time= 108.50610256195068  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1540  time= 109.14184236526489  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1550  time= 109.7677595615387  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1560  time= 110.3877067565918  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1570  time= 111.00861692428589  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1580  time= 111.63127589225769  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1590  time= 112.25244235992432  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1600  time= 112.87105774879456  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1610  time= 113.47179985046387  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1620  time= 114.08748316764832  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1630  time= 114.74969029426575  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1640  time= 115.38717579841614  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1650  time= 116.10873341560364  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1660  time= 116.80079650878906  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1670  time= 117.45506477355957  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1680  time= 118.12073993682861  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1690  time= 118.82307171821594  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1700  time= 119.48129034042358  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1710  time= 120.09385061264038  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1720  time= 120.71737575531006  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1730  time= 121.35179805755615  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1740  time= 121.96977090835571  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1750  time= 122.6149332523346  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1760  time= 123.29616189002991  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1770  time= 123.93623304367065  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1780  time= 124.53971076011658  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1790  time= 125.16345310211182  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1800  time= 125.77623248100281  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1810  time= 126.41867780685425  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1820  time= 127.11559510231018  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1830  time= 127.8057165145874  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1840  time= 128.51725792884827  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1850  time= 129.25758147239685  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1860  time= 129.9465470314026  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1870  time= 130.60792899131775  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1880  time= 131.3014271259308  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1890  time= 131.9375205039978  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1900  time= 132.57531023025513  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1910  time= 133.26326942443848  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1920  time= 133.96659779548645  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1930  time= 134.58650994300842  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1940  time= 135.261305809021  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1950  time= 135.95736527442932  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1960  time= 136.673668384552  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1970  time= 137.3140344619751  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1980  time= 137.98243737220764  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 1990  time= 138.67271494865417  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2000  time= 139.38085746765137  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2010  time= 140.10261917114258  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2020  time= 140.80912327766418  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2030  time= 141.5024378299713  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2040  time= 142.1620442867279  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2050  time= 142.7909336090088  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2060  time= 143.4296350479126  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2070  time= 144.081378698349  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2080  time= 144.71193885803223  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2090  time= 145.42163133621216  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2100  time= 146.12151503562927  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2110  time= 146.7730643749237  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2120  time= 147.48851656913757  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2130  time= 148.1662085056305  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2140  time= 148.84817790985107  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2150  time= 149.48672151565552  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2160  time= 150.08898830413818  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2170  time= 150.7374758720398  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2180  time= 151.39368271827698  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2190  time= 152.04749155044556  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2200  time= 152.75512790679932  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2210  time= 153.44161987304688  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2220  time= 154.1440281867981  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2230  time= 154.7788004875183  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2240  time= 155.46027779579163  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2250  time= 156.42999267578125  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2260  time= 157.4753658771515  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2270  time= 158.58826065063477  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2280  time= 159.60139727592468  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2290  time= 160.50361037254333  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2300  time= 161.1876196861267  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2310  time= 161.86277866363525  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2320  time= 162.50952410697937  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2330  time= 163.14956378936768  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2340  time= 163.81646180152893  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2350  time= 164.47817707061768  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2360  time= 165.16316056251526  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2370  time= 165.85556197166443  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2380  time= 166.52763867378235  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2390  time= 167.15340662002563  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2400  time= 167.77492547035217  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2410  time= 168.4046721458435  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2420  time= 169.01644849777222  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2430  time= 169.67244601249695  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2440  time= 170.31405782699585  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2450  time= 170.93770337104797  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2460  time= 171.59201765060425  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2470  time= 172.34679770469666  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2480  time= 173.01889562606812  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2490  time= 173.69982171058655  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2500  time= 174.31553173065186  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2510  time= 174.9292426109314  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2520  time= 175.55057430267334  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2530  time= 176.19068145751953  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2540  time= 176.83970856666565  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2550  time= 177.52014780044556  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2560  time= 178.21942567825317  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2570  time= 178.9142427444458  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2580  time= 179.62350130081177  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2590  time= 180.29134440422058  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2600  time= 180.98836708068848  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2610  time= 181.68716549873352  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2620  time= 182.43147110939026  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2630  time= 183.1544065475464  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2640  time= 183.87523698806763  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2650  time= 184.59825110435486  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2660  time= 185.24021315574646  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2670  time= 185.87021136283875  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2680  time= 186.57983493804932  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2690  time= 187.2319746017456  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2700  time= 187.95784974098206  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2710  time= 188.6392810344696  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2720  time= 189.3803129196167  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2730  time= 190.12604784965515  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2740  time= 190.8365249633789  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2750  time= 191.47127985954285  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2760  time= 192.10292434692383  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2770  time= 192.79647994041443  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2780  time= 193.45504808425903  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2790  time= 194.09905886650085  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2800  time= 194.78351497650146  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2810  time= 195.48200869560242  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2820  time= 196.1295781135559  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2830  time= 196.8107807636261  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2840  time= 197.46296381950378  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2850  time= 198.1150062084198  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2860  time= 198.80263113975525  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2870  time= 199.49975562095642  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2880  time= 200.16272473335266  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2890  time= 200.84643006324768  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2900  time= 201.48017692565918  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2910  time= 202.21364569664001  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2920  time= 202.83227729797363  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2930  time= 203.49926233291626  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2940  time= 204.22459626197815  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2950  time= 204.83329010009766  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2960  time= 205.48376178741455  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2970  time= 206.09085726737976  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2980  time= 206.73917031288147  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 2990  time= 207.3898730278015  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3000  time= 208.05612111091614  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3010  time= 208.69022011756897  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3020  time= 209.39158463478088  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3030  time= 210.05270051956177  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3040  time= 210.6811876296997  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3050  time= 211.31511330604553  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3060  time= 211.9593002796173  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3070  time= 212.58807396888733  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3080  time= 213.2725121974945  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3090  time= 213.9332935810089  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3100  time= 214.59122610092163  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3110  time= 215.20751333236694  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3120  time= 215.89753007888794  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3130  time= 216.6041030883789  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3140  time= 217.24722456932068  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3150  time= 217.9404740333557  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3160  time= 218.61422610282898  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3170  time= 219.30775237083435  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3180  time= 219.91794681549072  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3190  time= 220.5372302532196  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3200  time= 221.1954700946808  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3210  time= 221.88305068016052  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3220  time= 222.53477120399475  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3230  time= 223.1905436515808  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3240  time= 223.87697982788086  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3250  time= 224.5467348098755  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3260  time= 225.18800687789917  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3270  time= 225.87554240226746  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3280  time= 226.56849908828735  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3290  time= 227.21235299110413  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3300  time= 227.88580107688904  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3310  time= 228.57472944259644  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3320  time= 229.2557725906372  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3330  time= 229.93776321411133  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3340  time= 230.60363817214966  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3350  time= 231.25175142288208  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3360  time= 231.866352558136  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3370  time= 232.49163842201233  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3380  time= 233.11860132217407  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3390  time= 233.7637128829956  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3400  time= 234.38139009475708  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3410  time= 235.0017604827881  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3420  time= 235.71733498573303  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3430  time= 236.41885995864868  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3440  time= 237.1087622642517  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3450  time= 237.7754828929901  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3460  time= 238.48626971244812  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3470  time= 239.1518497467041  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3480  time= 239.87328457832336  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3490  time= 240.53080034255981  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3500  time= 241.1842086315155  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3510  time= 241.88977932929993  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3520  time= 242.57908654212952  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3530  time= 243.24072766304016  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3540  time= 243.9021053314209  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3550  time= 244.54064440727234  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3560  time= 245.1491458415985  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3570  time= 245.75291466712952  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3580  time= 246.4593436717987  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3590  time= 247.18958473205566  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3600  time= 247.86969089508057  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3610  time= 248.57808017730713  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3620  time= 249.27766728401184  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3630  time= 249.90669059753418  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3640  time= 250.5224950313568  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3650  time= 251.13382983207703  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3660  time= 251.79045414924622  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3670  time= 252.4807267189026  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3680  time= 253.10176420211792  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3690  time= 253.76577615737915  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3700  time= 254.44903254508972  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3710  time= 255.0915777683258  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3720  time= 255.7689507007599  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3730  time= 256.4049723148346  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3740  time= 257.0597822666168  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3750  time= 257.7399728298187  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3760  time= 258.3868761062622  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3770  time= 259.0564455986023  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3780  time= 259.72488260269165  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3790  time= 260.3382441997528  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3800  time= 260.9765160083771  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3810  time= 261.66815400123596  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3820  time= 262.30655217170715  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3830  time= 262.9459502696991  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3840  time= 263.5622353553772  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3850  time= 264.1689245700836  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3860  time= 264.7981626987457  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3870  time= 265.4695134162903  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3880  time= 266.1441752910614  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3890  time= 266.8304226398468  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3900  time= 267.5024621486664  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3910  time= 268.155394077301  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3920  time= 268.8339104652405  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3930  time= 269.54192543029785  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3940  time= 270.197625875473  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3950  time= 270.8229343891144  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3960  time= 271.45871114730835  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3970  time= 272.08942341804504  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3980  time= 272.7709469795227  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 3990  time= 273.4392821788788  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4000  time= 274.11974906921387  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4010  time= 274.83931970596313  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4020  time= 275.52126693725586  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4030  time= 276.184143781662  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4040  time= 276.81726717948914  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4050  time= 277.5602192878723  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4060  time= 278.3066952228546  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4070  time= 279.0389790534973  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4080  time= 279.6788499355316  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4090  time= 280.3500497341156  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4100  time= 281.05431938171387  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4110  time= 281.7769763469696  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4120  time= 282.49700593948364  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4130  time= 283.19119668006897  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4140  time= 283.93389654159546  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4150  time= 284.6412944793701  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4160  time= 285.32642102241516  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4170  time= 286.0526478290558  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4180  time= 286.74940061569214  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4190  time= 287.4182856082916  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4200  time= 288.08646273612976  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4210  time= 288.7337021827698  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4220  time= 289.38226103782654  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4230  time= 290.0801215171814  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4240  time= 290.7768621444702  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4250  time= 291.5032961368561  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4260  time= 292.20820713043213  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4270  time= 292.86023902893066  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4280  time= 293.52361130714417  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4290  time= 294.16436195373535  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4300  time= 294.8079869747162  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4310  time= 295.43092608451843  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4320  time= 296.04393196105957  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4330  time= 296.6711275577545  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4340  time= 297.32441759109497  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4350  time= 298.0098431110382  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4360  time= 298.69890427589417  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4370  time= 299.3385808467865  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4380  time= 300.02864718437195  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4390  time= 300.73756313323975  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4400  time= 301.44439792633057  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4410  time= 302.06772089004517  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4420  time= 302.73796248435974  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4430  time= 303.4240744113922  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4440  time= 304.10764169692993  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4450  time= 304.775190114975  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4460  time= 305.43950843811035  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4470  time= 306.1374759674072  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4480  time= 306.85603427886963  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4490  time= 307.5600917339325  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4500  time= 308.24366188049316  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4510  time= 308.96124386787415  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4520  time= 309.63800287246704  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4530  time= 310.2685523033142  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4540  time= 310.9059488773346  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4550  time= 311.58789014816284  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4560  time= 312.30911779403687  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4570  time= 312.9823236465454  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4580  time= 313.65307116508484  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4590  time= 314.3016149997711  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4600  time= 314.91085147857666  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4610  time= 315.5265121459961  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4620  time= 316.19570088386536  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4630  time= 316.89517188072205  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4640  time= 317.58773374557495  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4650  time= 318.27601504325867  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4660  time= 318.9585189819336  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4670  time= 319.654905796051  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4680  time= 320.3222465515137  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4690  time= 321.0072145462036  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4700  time= 321.7075333595276  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4710  time= 322.42727637290955  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4720  time= 323.10505509376526  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4730  time= 323.77841091156006  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4740  time= 324.4586968421936  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4750  time= 325.11285877227783  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4760  time= 325.8570177555084  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4770  time= 326.5584466457367  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4780  time= 327.24833488464355  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4790  time= 327.9474813938141  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4800  time= 328.61965131759644  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4810  time= 329.31995725631714  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4820  time= 330.04068422317505  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4830  time= 330.6763334274292  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4840  time= 331.35293412208557  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4850  time= 332.0732171535492  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4860  time= 332.7067189216614  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4870  time= 333.34039068222046  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4880  time= 333.97641038894653  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4890  time= 334.60992527008057  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4900  time= 335.2416501045227  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4910  time= 335.86142683029175  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4920  time= 336.50407791137695  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4930  time= 337.12005162239075  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4940  time= 337.80742025375366  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4950  time= 338.48056626319885  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4960  time= 339.1801452636719  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4970  time= 339.8251905441284  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4980  time= 340.4317719936371  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 4990  time= 341.1700367927551  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5000  time= 341.9063766002655  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5010  time= 342.585125207901  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5020  time= 343.2152726650238  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5030  time= 343.86961483955383  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5040  time= 344.4804940223694  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5050  time= 345.09486722946167  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5060  time= 345.73160552978516  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5070  time= 346.3460268974304  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5080  time= 346.9786379337311  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5090  time= 347.6622211933136  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5100  time= 348.269544839859  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5110  time= 348.89495182037354  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5120  time= 349.497451543808  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5130  time= 350.1373507976532  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5140  time= 350.78232765197754  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5150  time= 351.5299150943756  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5160  time= 352.2616112232208  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5170  time= 352.9815979003906  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5180  time= 353.60261940956116  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5190  time= 354.3020193576813  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5200  time= 354.9656455516815  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5210  time= 355.5740933418274  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5220  time= 356.2062201499939  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5230  time= 356.8397090435028  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5240  time= 357.47606563568115  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5250  time= 358.16449213027954  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5260  time= 358.8723316192627  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5270  time= 359.60110569000244  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5280  time= 360.2588858604431  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5290  time= 360.8701047897339  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5300  time= 361.59630846977234  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5310  time= 362.29219150543213  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5320  time= 362.9390380382538  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5330  time= 363.5763363838196  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5340  time= 364.2075593471527  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5350  time= 364.85131216049194  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5360  time= 365.4702367782593  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5370  time= 366.0939438343048  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5380  time= 366.7073667049408  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5390  time= 367.34337282180786  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5400  time= 368.01705408096313  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5410  time= 368.73131489753723  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5420  time= 369.49713683128357  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5430  time= 370.5542230606079  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5440  time= 371.48211884498596  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5450  time= 372.41779351234436  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5460  time= 373.3793218135834  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5470  time= 374.3877832889557  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5480  time= 375.05884170532227  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5490  time= 375.7121067047119  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5500  time= 376.3533673286438  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5510  time= 377.02234411239624  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5520  time= 377.67031598091125  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5530  time= 378.3084120750427  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5540  time= 378.96411323547363  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5550  time= 379.61053013801575  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5560  time= 380.22469687461853  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5570  time= 380.87463307380676  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5580  time= 381.5400698184967  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5590  time= 382.1750090122223  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5600  time= 382.80858755111694  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5610  time= 383.46002078056335  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5620  time= 384.1524803638458  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5630  time= 384.801677942276  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5640  time= 385.425906419754  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5650  time= 386.1042447090149  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5660  time= 386.7952227592468  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5670  time= 387.46711015701294  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5680  time= 388.1539797782898  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5690  time= 388.8473491668701  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5700  time= 389.47015500068665  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5710  time= 390.0830731391907  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5720  time= 390.68816685676575  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5730  time= 391.30509424209595  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5740  time= 391.97575092315674  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5750  time= 392.6753890514374  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5760  time= 393.3934977054596  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5770  time= 394.06689405441284  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5780  time= 394.6930134296417  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5790  time= 395.3259644508362  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5800  time= 395.98576045036316  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5810  time= 396.7094421386719  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5820  time= 397.4045059680939  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5830  time= 398.11384201049805  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5840  time= 398.7992706298828  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5850  time= 399.4612030982971  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5860  time= 400.15270805358887  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5870  time= 400.85186886787415  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5880  time= 401.4949941635132  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5890  time= 402.22016644477844  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5900  time= 402.93946504592896  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5910  time= 403.58866357803345  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5920  time= 404.253525018692  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5930  time= 404.93256068229675  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5940  time= 405.5650005340576  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5950  time= 406.173136472702  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5960  time= 406.80504155158997  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5970  time= 407.4295172691345  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5980  time= 408.0826201438904  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 5990  time= 408.7770428657532  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 6000  time= 409.4476680755615  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 6010  time= 410.0604386329651  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 6020  time= 410.69310665130615  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 6030  time= 411.41544914245605  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 6040  time= 412.14850997924805  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 6050  time= 412.77805185317993  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 6060  time= 413.43187046051025  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"," \n","epoch= 6070  time= 414.04021406173706  loss= nan  val_loss= nan  f1= nan  f2= nan  f3= nan  f4= nan num_batches= 13 percent lr= 0.01\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-104-b6b36971bbc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training sepNet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msepnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwholemat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_conditions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitialcon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwholemat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwholemat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevalmat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevalmat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mH0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;31m# results = PINN.train(sepnet,bs=10,num_epoch=5000,initial_conditions=initialcon,device=device, wholemat=wholemat,evalmat=evalmat,x0=x0,H0=H0,dim=dim,LR=LR,patience=100,c1=1,c2=1,c3=1,c4=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0msepnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-101-dc2941ebcd9e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, bs, num_epoch, initial_conditions, device, wholemat, evalmat, x0, H0, dim, LR, patience, c1, c2, c3, c4)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlossfuc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-101-dc2941ebcd9e>\u001b[0m in \u001b[0;36mlossfuc\u001b[0;34m(model, mat, x, y, device, x0, H0, dim, c1, c2, c3, c4, verbose)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlossfuc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwholemat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mf3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mH0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mdH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_unused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mdHdq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-101-dc2941ebcd9e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# print(\"hl2\", x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;31m# print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;31m# print(\"output\", x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-101-dc2941ebcd9e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# print(\"mul\", torch.einsum('ijk,ikl->ijl', x, self.weights))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;31m# print(\"add\", torch.add(torch.einsum('ijk,ikl->ijl', x, self.weights), self.bias))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ijk,ikl->ijl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;31m# return F.linear(x, self.weights, self.bias)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import matplotlib.pyplot as plt\n","from torch.autograd import Variable\n","from matplotlib import cm\n","from SeparableNNs.models import PINN\n","import torch\n","from SeparableNNs import metrics\n","\n","for i in range(1):\n","  seed = i\n","  np.random.seed(seed=seed)\n","  for ini in initialcon: \n","\n","    start, final = groundtruth_2dim.CreateTrainingDataTrajStormer(1,ini,spacedim,h,f1,f2,seed = seed,n_h = 1)\n","\n","    delta = (final-start)/h\n","\n","    print(start, delta, H(start))\n","    # delta2 = delta.copy()\n","    # delta2[0,:] = f1(start)\n","    # delta2[1,:] = f2(start)\n","    # dq_MSE = np.mean(np.square(delta2[0,:]-delta[0,:]))\n","    # dp_MSE = np.mean(np.square(delta2[1,:]-delta[1,:]))\n","    # data_MSE = np.mean(np.square(np.sqrt(delta[0,:]**2 + delta[1,:]**2)-np.sqrt(delta2[0,:]**2 + delta2[1,:]**2)))\n","\n","    \"\"\"# sepNN\"\"\"\n","\n","    \n","\n","    if torch.cuda.is_available():\n","      device=torch.device('cuda')\n","    else:\n","      device=torch.device('cpu')\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    wholemat, evalmat = PINN.data_preprocessing(start, delta,device)    \n","\n","    import torch.optim as optim\n","    import time \n","\n","\n","    sepnet = sepNet(2,32,32,1) #PINN.sepNet(2,11,11,1)\n","    starttime = time.time() \n","    print(\"training sepNet\")\n","\n","    results = train(sepnet,bs=min(int(len(wholemat)/5), 128),num_epoch=7000,initial_conditions=initialcon,device=device, wholemat=wholemat,evalmat=evalmat,x0=x0,H0=H0,dim=dim,LR=LR,patience=1000,c1=1,c2=1,c3=1,c4=1)\n","    # results = PINN.train(sepnet,bs=10,num_epoch=5000,initial_conditions=initialcon,device=device, wholemat=wholemat,evalmat=evalmat,x0=x0,H0=H0,dim=dim,LR=LR,patience=100,c1=1,c2=1,c3=1,c4=1)\n","    sepnet, epochs = results[0], results[1]\n","    septraintime = time.time()-starttime\n","    torch.save(sepnet.state_dict(), '/content/drive/MyDrive/CIKM2022/Kepler System/%s/%s_%s_%s_%s.pt' %(sys,sys,\"sepNN\",seed,ini))\n","\n","    # H_pred, dq_pred, dp_pred = np.zeros((H_true.shape)), np.zeros((H_true.shape)), np.zeros((H_true.shape))\n","    # for i in tqdm(range(len(sample_points))):\n","    #   out = get_H_grad(sepnet, sample_points[i], device)\n","    #   H_pred[i] = out[0]\n","    #   dq_pred[i] = out[1]\n","    #   dp_pred[i] = out[2]\n","    # print(H_pred.shape, dq_pred.shape, dp_pred.shape)\n","    # file_object = open('/content/drive/MyDrive/CIKM2022/SeparableExp/%s/%s_results.txt' %(sys,sys), 'a') \n","    # file_object.write('sepNN, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s \\n' \n","    #       %(ini, seed, dq_MSE, dp_MSE, data_MSE, septraintime, epochs, metrics.MSE(H_pred, H_true, diagdist), metrics.MSE(dq_pred, f1(np.squeeze(sample_points,2).transpose()), diagdist), metrics.MSE(dp_pred, f2(np.squeeze(sample_points,2).transpose()), diagdist),\n","    #       np.mean(np.square(H_pred- H_true)), np.mean(np.square(dq_pred- f1(np.squeeze(sample_points,2).transpose()))), np.mean(np.square(dp_pred- f2(np.squeeze(sample_points,2).transpose()))), \n","    #       np.mean(np.square(np.sqrt(dq_pred**2 + dp_pred**2)-np.sqrt(f1(np.squeeze(sample_points,2).transpose())**2 + f2(np.squeeze(sample_points,2).transpose())**2)))))\n","    # file_object.close()\n","    \n","    # H_pred, dq_pred, dp_pred = H_pred.reshape(xshort.shape), dq_pred.reshape(xshort.shape), dp_pred.reshape(xshort.shape)\n","\n","    # plt.figure(figsize = (10,10))\n","    # plt.quiver(xshort,yshort,dq_pred,dp_pred) #x,y,dH/dy,-dH/dx\n","    # plt.imshow(np.flip(H_pred.reshape(xshort.shape),0), cmap = cm.jet, extent = (spacedim[0][0], spacedim[0][1], spacedim[1][0], spacedim[1][1]))\n","    # plt.savefig('/content/drive/MyDrive/CIKM2022/SeparableExp/%s/%s_%s_%s_%s.png' %(sys,sys, ini, seed, \"sepNN\"))\n","\n","    net = Net(4,45,1)\n","    starttime = time.time() \n","    print(\"training PINN Net\")\n","    \n","    results = train(net,bs=min(int(len(wholemat)/5), 128),num_epoch=7000,initial_conditions=initialcon,device=device, wholemat=wholemat,evalmat=evalmat,x0=x0,H0=H0,dim=dim,LR=LR,patience=1000,c1=1,c2=1,c3=1,c4=1)\n","    net, epochs = results[0], results[1]\n","    PINNtraintime = time.time()-starttime\n","    torch.save(net.state_dict(), '/content/drive/MyDrive/CIKM2022/Kepler System/%s/%s_%s_%s_%s.pt' %(sys,sys,\"PINN\",seed,ini))\n","\n","    # H_pred, dq_pred, dp_pred = np.zeros((H_true.shape)), np.zeros((H_true.shape)), np.zeros((H_true.shape))\n","    # for i in tqdm(range(len(sample_points))):\n","    #   out = get_H_grad(net, sample_points[i], device)\n","    #   H_pred[i] = out[0]\n","    #   dq_pred[i] = out[1]\n","    #   dp_pred[i] = out[2]\n","    # print(H_pred.shape, dq_pred.shape, dp_pred.shape)\n","    # file_object = open('/content/drive/MyDrive/CIKM2022/SeparableExp/%s/%s_results.txt' %(sys,sys), 'a')\n","    # file_object.write('PINN, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s \\n' \n","    #       %(ini, seed, dq_MSE, dp_MSE, data_MSE, PINNtraintime, epochs, metrics.MSE(H_pred, H_true, diagdist), metrics.MSE(dq_pred, f1(np.squeeze(sample_points,2).transpose()), diagdist), metrics.MSE(dp_pred, f2(np.squeeze(sample_points,2).transpose()), diagdist),\n","    #       np.mean(np.square(H_pred- H_true)), np.mean(np.square(dq_pred- f1(np.squeeze(sample_points,2).transpose()))), np.mean(np.square(dp_pred- f2(np.squeeze(sample_points,2).transpose()))), \n","    #       np.mean(np.square(np.sqrt(dq_pred**2 + dp_pred**2)-np.sqrt(f1(np.squeeze(sample_points,2).transpose())**2 + f2(np.squeeze(sample_points,2).transpose())**2)))))\n","    # file_object.close()\n","    \n","    # H_pred, dq_pred, dp_pred = H_pred.reshape(xshort.shape), dq_pred.reshape(xshort.shape), dp_pred.reshape(xshort.shape)\n","    \n","    # plt.figure(figsize = (10,10))\n","    # plt.quiver(xshort,yshort,dq_pred,dp_pred) #x,y,dH/dy,-dH/dx\n","    # plt.imshow(np.flip(H_pred.reshape(xshort.shape),0), cmap = cm.jet, extent = (spacedim[0][0], spacedim[0][1], spacedim[1][0], spacedim[1][1]))\n","    # plt.savefig('/content/drive/MyDrive/CIKM2022/SeparableExp/%s/%s_%s_%s_%s.png' %(sys,sys, ini, seed, \"PINN\"))\n","\n","    # print(septraintime, PINNtraintime)"]},{"cell_type":"code","source":["net(torch.tensor([[x0]*4]).to('cpu'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3yauxiDQuflu","executionInfo":{"status":"ok","timestamp":1665989780356,"user_tz":-480,"elapsed":4,"user":{"displayName":"Khoo Zi-Yu","userId":"01763969606877785000"}},"outputId":"6e1e5b70-db43-489f-f9af-e108c2cda814"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[nan]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["seed, ini = 0, 2048\n","n_sample = 20\n","\n","sepnet_total, net_total = [],[]\n","for seed in range(1):\n","  for ini in initialcon:\n","    net = Net(4,45,1)\n","    net.load_state_dict(torch.load('/content/drive/MyDrive/CIKM2022/Kepler System/%s/%s_%s_%s_%s.pt' %(sys,sys,\"PINN\",seed,ini)))\n","\n","    sepnet = sepNet(2,32,32,1) #PINN.sepNet(2,11,11,1)\n","    sepnet.load_state_dict(torch.load('/content/drive/MyDrive/CIKM2022/Kepler System/%s/%s_%s_%s_%s.pt' %(sys,sys,\"sepNN\",seed,ini)))\n","\n","    def get_grad(model, z,device):\n","      inputs=Variable(z.clone().detach()).requires_grad_(True).to(device)\n","      out=model(inputs.float())\n","      dH=torch.autograd.grad(out, inputs, grad_outputs=out.data.new(out.shape).fill_(1),create_graph=True)[0]\n","      return np.asarray([dH.detach().cpu().numpy()[:,2], dH.detach().cpu().numpy()[:,3], -dH.detach().cpu().numpy()[:,0], -dH.detach().cpu().numpy()[:,1]]) # negative dH/dq is dp/dt\n","\n","\n","    f1 = lambda x: np.stack([x[:,2], x[:,3]])\n","    f2 = lambda x: np.stack([-x[:,0]*(x[:,0]**2+x[:,1]**2)**(-1.5), -x[:,1]*(x[:,0]**2+x[:,1]**2)**(-1.5)])\n","    fvec = lambda z: np.concatenate([f1(z), f2(z)])\n","    error = lambda x,y: np.sum(np.sqrt(np.sum((x-y)**2,0))/np.sqrt(np.sum(x**2,0))) # where x is the true vector and y is the approximated vector\n","\n","    n_sample = 20\n","    z = torch.tensor(np.array(np.meshgrid(np.linspace(spacedim[0][0], spacedim[0][1], n_sample),np.linspace(spacedim[1][0], spacedim[1][1],n_sample),\n","                                          np.linspace(spacedim[2][0], spacedim[2][1],n_sample),np.linspace(spacedim[3][0], spacedim[3][1],n_sample)))).reshape(4, n_sample**4).transpose(1,0)\n","    sepnet_error, net_error = 0, 0\n","    results = torch.zeros(z.shape)\n","    for i in range(int((n_sample**4)/1000)):\n","      inpz = z[1000*i:1000*(i+1), :]\n","      sepnet_error += error(fvec(inpz), get_grad(sepnet, inpz, 'cpu'))\n","      net_error += error(fvec(inpz), get_grad(net, inpz, 'cpu'))\n","    if sepnet_error > n_sample**4*0.1 or net_error > n_sample**4*0.1: print(\"WARNING\")\n","    sepnet_total.append(sepnet_error)\n","    net_total.append(net_error)\n","    print(ini, net_error, sepnet_error)"],"metadata":{"id":"q618gGeSl7b_","executionInfo":{"status":"aborted","timestamp":1665997702634,"user_tz":-480,"elapsed":7,"user":{"displayName":"Khoo Zi-Yu","userId":"01763969606877785000"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sepnet_total, net_total = [np.mean(sepnet_total[i::len(initialcon)]) for i in range(len(initialcon))], [np.mean(net_total[i::len(initialcon)]) for i in range(len(initialcon))]"],"metadata":{"id":"1GXN8EodcjqM","executionInfo":{"status":"ok","timestamp":1665995499324,"user_tz":-480,"elapsed":622,"user":{"displayName":"Khoo Zi-Yu","userId":"01763969606877785000"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.scatter(initialcon, sepnet_total, label = \"sepnet loss\")\n","plt.scatter(initialcon, net_total, label = \"net loss\")\n","# plt.yscale(\"log\")\n","plt.legend()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"xcUoerrt8fBb","executionInfo":{"status":"ok","timestamp":1665995499954,"user_tz":-480,"elapsed":8,"user":{"displayName":"Khoo Zi-Yu","userId":"01763969606877785000"}},"outputId":"6c944ed7-d910-47e9-8a2d-cdb37efca208"},"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7fda13ef6a10>"]},"metadata":{},"execution_count":96},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3gV9b3v8fe3EC66lSBGDxBa0pZyyiVyWSLnsVSUFoLSgj2VjVuPaaXSXS9oL1TcdiOPdbdaPGI5WvfjFipadgHZCvRoSzmotbYFDRe5UwNqSUSJXCJKqAG+54/5BRchE1hZyVpJ+LyeZz1r5ju/mfmuycr6rpnfrBlzd0REROryiWwnICIizZeKhIiIxFKREBGRWCoSIiISS0VCRERitc12Ao3t3HPP9Z49e2Y7DRGRFmX16tXvuXte7XirKxI9e/akpKQk22mIiLQoZvZWXXEdbhIRkVgqEiIiEktFQkREYrW6PgkRaVmqq6spKyvj0KFD2U7ltNChQwfy8/PJyck5pfYqEiKSVWVlZZx11ln07NkTM8t2Oq2au7Nnzx7KysooKCg4pXlOerjJzOaY2W4z21grfouZbTWzTWb2s6T4HWZWambbzGxUUrwoxErNbGpSvMDMVoX4AjNrF+Ltw3hpmN7zlF5RU1q/EGb2g+m50fP6hdnOSKTFO3ToEF26dFGByAAzo0uXLinttZ1Kn8TjQFGtFV0KjAUucPe+wP0h3geYAPQN8/zCzNqYWRvgYWA00Ae4OrQFuA+Y6e6fBfYBE0N8IrAvxGeGdtmzfiH8ZjJU7gQ8ev7NZBUKkUagApE5qW7rkxYJd38J2Fsr/B3gXnf/e2izO8THAvPd/e/u/gZQCgwJj1J33+HuHwHzgbEWZXsZsCjMPxcYl7SsuWF4ETDCsvlOWnE3VFcdH6uuiuIiIq1UQ89u+hwwLBwG+oOZXRji3YGdSe3KQiwu3gXY7+6Ha8WPW1aYXhnan8DMJplZiZmVVFRUNPAlnURlWWpxEZGT+MlPfhI7rWfPnrz33nsZzKZuDS0SbYFzgKHAFGBhNr/lu/uj7p5w90Re3gm/Km8cnfJTi4uInER9RaK5aGiRKAOe9sgrwFHgXKAc6JHULj/E4uJ7gFwza1srTvI8YXqn0D4rXv3MLVR5u+NiVd6OVz9zS5YyEjk9LV5bzsX3Pk/B1Ge5+N7nWby2/OQz1ePDDz/kiiuu4IILLqBfv34sWLAAgNWrV3PJJZcwePBgRo0axa5duwAYPnw4t956KwMGDKBfv3688sorAEyfPp3rr7+e4cOH8+lPf5pZs2YdW8evfvUrhgwZwoABA/j2t7/NkSNHmDp1KlVVVQwYMIBrrrmm3hwfeOAB+vXrR79+/XjwwQfrzXvq1Kn06dOHwsJCfvCDH6S1baDhp8AuBi4FXjCzzwHtgPeApcB/mtkDQDegF/AKYEAvMysg+vCfAPyTu7uZvQB8naifohhYEtaxNIz/JUx/3rN4r9XbNvdicPW3+GHbhXSzPbztXfjZ4fGs3tyLP301W1mJnF4Wry3njqc3UFV9BIDy/VXc8fQGAMYN7F7frLF+97vf0a1bN5599lkAKisrqa6u5pZbbmHJkiXk5eWxYMEC7rzzTubMmQPAwYMHWbduHS+99BLXX389GzdGJ39u3bqVF154gQMHDtC7d2++853vUFpayoIFC/jTn/5ETk4ON954I/PmzePee+/loYceYt26dfXmt3r1an75y1+yatUq3J2LLrqISy65hB07dpyQ9549e3jmmWfYunUrZsb+/fsbtE2SnbRImNmvgeHAuWZWBtwFzAHmhNNiPwKKwwf4JjNbCGwGDgM3ufuRsJybgWVAG2COu28Kq7gdmG9m9wBrgdkhPht40sxKiTrOJ6T9atPw9v4qyvkCSz/6wnFx218VM4eINLYZy7YdKxA1qqqPMGPZtgYXif79+/P973+f22+/nTFjxjBs2DA2btzIxo0b+fKXvwzAkSNH6Nq167F5rr76agC++MUv8v777x/7ML7iiito37497du357zzzuPdd99lxYoVrF69mgsvjLpuq6qqOO+88045v5dffpkrr7ySM888E4Cvfe1r/PGPf6SoqOiEvA8fPkyHDh2YOHEiY8aMYcyYMQ3aJslOWiTc/eqYSdfGtP834N/qiD8HPFdHfAfR2U+144eAq06WX6Z0y+1IeR0FoVtuxyxkI3J6ejvmS1lc/FR87nOfY82aNTz33HP86Ec/YsSIEVx55ZX07duXv/zlL3XOU7sLtma8ffv2x2Jt2rTh8OHDuDvFxcX89Kc/bXCOp5r3tGnTeOWVV1ixYgWLFi3ioYce4vnnn09rPbp20ymaMqo3HXPaHBfrmNOGKaN6ZykjkdNP3JeydL6svf3225xxxhlce+21TJkyhTVr1tC7d28qKiqOFYnq6mo2bdp0bJ6a4/8vv/wynTp1olOnTrHLHzFiBIsWLWL37uiXAnv37uWtt6Krcufk5FBdXV1vfsOGDWPx4sUcPHiQDz/8kGeeeYZhw4bVmfcHH3xAZWUll19+OTNnzuS1115r8HapoctynKKaXdkZy7bx9v4quuV2ZMqo3g3exRWR1E0Z1fu4PglI/8vahg0bmDJlCp/4xCfIycnhkUceoV27dixatIjJkydTWVnJ4cOHue222+jbty8QXf9o4MCBVFdXH+uniNOnTx/uueceRo4cydGjR8nJyeHhhx/mU5/6FJMmTaKwsJBBgwYxb968OucfNGgQ3/jGNxgyJDrg8q1vfYuBAweybNmyE/I+cOAAY8eO5dChQ7g7DzzwQIO3Sw3LYl9wk0gkEq6bDom0HFu2bOHzn//8KbdfvLY8q1/Whg8fzv33308ikcjYOhtbXdvczFa7+wkvSnsSItKijBvYXXvwGaQiISKSghdffDHbKWSUOq5FRCSWioSIiMRSkRARkVgqEiIiEktFQkQkRYsXL2bz5s11Tps+fTr3339/hjNqOioSIiIpqq9ItDYqEiLSsjTyvebffPNNPv/5z3PDDTfQt29fRo4cSVVVdC2o7du3U1RUxODBgxk2bBhbt27lz3/+M0uXLmXKlCkMGDCA7du3xy573bp1DB06lMLCQq688kr27dsHwKxZs45dznvChOjapX/4wx8YMGAAAwYMYODAgRw4cCCt19Vo3L1VPQYPHuwi0nJs3rz51Bu/tsD9nvPd7zr748c950fxBnrjjTe8TZs2vnbtWnd3v+qqq/zJJ590d/fLLrvM//rXv7q7+8qVK/3SSy91d/fi4mJ/6qmn6lzeXXfd5TNmzHB39/79+/uLL77o7u7/+q//6rfeequ7u3ft2tUPHTrk7u779u1zd/cxY8b4yy+/7O7uBw4c8Orq6ga/ppOpa5sDJV7HZ6r2JESk5Wiie80XFBQwYMAAAAYPHsybb77JBx98wJ///GeuuuqqYzcLqrnx0KmorKxk//79XHLJJQAUFxfz0ksvAVBYWMg111zDr371K9q2jX7TfPHFF/O9732PWbNmsX///mPxbFOREJGWo4nuNV/XJb6PHj1Kbm4u69atO/bYsmVLWuup8eyzz3LTTTexZs0aLrzwQg4fPszUqVN57LHHqKqq4uKLL2br1q2Nsq50qUiISMuRwXvNn3322RQUFPDUU08B0aH5mktvn3XWWSftM+jUqROdO3fmj3/8IwBPPvkkl1xyCUePHmXnzp1ceuml3HfffVRWVvLBBx+wfft2+vfvz+23386FF16oIiEikrIR0yCn1r0jcjpG8SYwb948Zs+ezQUXXEDfvn1ZsiS6u/KECROYMWMGAwcOrLfjeu7cuUyZMoXCwkLWrVvHtGnTOHLkCNdeey39+/dn4MCBTJ48mdzcXB588EH69etHYWEhOTk5jB49ukleU6pOeqlwM5sDjAF2u3u/WtO+D9wP5Ln7exbdnunnwOXAQeAb7r4mtC0GfhRmvcfd54b4YOBxoCPRnetudXc3s3OABUBP4E1gvLvvO9kL0qXCRVqWVC8VzvqFUR9EZVm0BzFiGhSOb7oEW6FULhV+KnsSjwNFtYNm1gMYCfwtKTwa6BUek4BHQttziO6NfRHRrUrvMrPOYZ5HgBuS5qtZ11Rghbv3AlaEcRE53RWOh+9uhOn7o2cViCZ10iLh7i8Be+uYNBP4IZC8KzIWeCKcUbUSyDWzrsAoYLm77w17A8uBojDtbHdfGU7BegIYl7SsuWF4blJcREQypEF9EmY2Fih399o3UO0O7EwaLwux+uJldcQBznf3mvPN3gHOryefSWZWYmYlFRUVqb4cEcmykx32lsaT6rZOuUiY2RnAvwBN01NUh7CXEfvK3P1Rd0+4eyIvLy9TaYlII+jQoQN79uxRocgAd2fPnj106NDhlOdpyK81PgMUAK9F/dTkA2vMbAhQDvRIapsfYuXA8FrxF0M8v472AO+aWVd33xUOS+1uQK4i0szl5+dTVlaGjgJkRocOHcjPP/VThlMuEu6+ATivZtzM3gQS4eympcDNZjafqJO6MnzILwN+ktRZPRK4w933mtn7ZjYUWAVcB/yf0GYpUAzcG56XpJqriDR/OTk5FBQUZDsNiXHSw01m9mvgL0BvMyszs4n1NH8O2AGUAv8B3Ajg7nuBHwOvhsfdIUZo81iYZzvw2xC/F/iymb0OfCmMi4hIBp30dxItjX4nISKSunR+JyEiIqcpFQkREYmlIiEiIrFUJEREJJaKhIiIxFKREBGRWCoSIiISS0VCRERiqUiIiEgsFQkREYmlIiEiIrFUJEREJJaKhIiIxFKREBGRWCoSIiISS0VCRERincqd6eaY2W4z25gUm2FmW81svZk9Y2a5SdPuMLNSM9tmZqOS4kUhVmpmU5PiBWa2KsQXmFm7EG8fxkvD9J6N9aJFROTUnMqexONAUa3YcqCfuxcCfwXuADCzPsAEoG+Y5xdm1sbM2gAPA6OBPsDVoS3AfcBMd/8ssA+ouT3qRGBfiM8M7UREJINOWiTc/SVgb63Y7939cBhdCeSH4bHAfHf/u7u/QXTf6iHhUeruO9z9I2A+MNbMDLgMWBTmnwuMS1rW3DC8CBgR2ouISIY0Rp/E9cBvw3B3YGfStLIQi4t3AfYnFZya+HHLCtMrQ3sREcmQtIqEmd0JHAbmNU46Dc5jkpmVmFlJRUVFNlMREWlVGlwkzOwbwBjgGnf3EC4HeiQ1yw+xuPgeINfM2taKH7esML1TaH8Cd3/U3RPunsjLy2voSxIRkVoaVCTMrAj4IfBVdz+YNGkpMCGcmVQA9AJeAV4FeoUzmdoRdW4vDcXlBeDrYf5iYEnSsorD8NeB55OKkYiIZEDbkzUws18Dw4FzzawMuIvobKb2wPLQl7zS3f/Z3TeZ2UJgM9FhqJvc/UhYzs3AMqANMMfdN4VV3A7MN7N7gLXA7BCfDTxpZqVEHecTGuH1iohICqy1fTlPJBJeUlKS7TRERFoUM1vt7onacf3iWkREYqlIiIhILBUJERGJpSIhIiKxVCRERCSWioSIiMRSkRARkVgqEiIiEktFQkREYqlIiIhILBUJERGJpSIhIiKxVCRERCSWioSIiMRSkRARkVgqEiIiEktFQkREYp20SJjZHDPbbWYbk2LnmNlyM3s9PHcOcTOzWWZWambrzWxQ0jzFof3rZlacFB9sZhvCPLMs3A81bh0iIpI5p7In8ThQVCs2FVjh7r2AFWEcYDTQKzwmAY9A9IFPdG/si4AhwF1JH/qPADckzVd0knWIiEiGnLRIuPtLwN5a4bHA3DA8FxiXFH/CIyuBXDPrCowClrv7XnffBywHisK0s919pUc3236i1rLqWoeIiGRIQ/skznf3XWH4HeD8MNwd2JnUrizE6ouX1RGvbx0nMLNJZlZiZiUVFRUNeDkiIlKXtDuuwx6AN0IuDV6Huz/q7gl3T+Tl5TVlKiIip5WGFol3w6EiwvPuEC8HeiS1yw+x+uL5dcTrW4eIiGRIQ4vEUqDmDKViYElS/LpwltNQoDIcMloGjDSzzqHDeiSwLEx738yGhrOarqu1rLrWISIiGdL2ZA3M7NfAcOBcMysjOkvpXmChmU0E3gLGh+bPAZcDpcBB4JsA7r7XzH4MvBra3e3uNZ3hNxKdQdUR+G14UM86REQkQyw63N96JBIJLykpyXYaIiItipmtdvdE7bh+cS0iIrFUJEREJJaKhIiIxFKREBGRWCoSIiISS0VCRERiqUiIiEgsFQkREYmlIiEiIrFUJEREJJaKhIiIxFKREBGRWCoSIiISS0VCRERiqUiIiEgsFQkREYmlIiEiIrHSKhJm9l0z22RmG83s12bWwcwKzGyVmZWa2QIzaxfatg/jpWF6z6Tl3BHi28xsVFK8KMRKzWxqOrmKiEjqGlwkzKw7MBlIuHs/oA0wAbgPmOnunwX2ARPDLBOBfSE+M7TDzPqE+foCRcAvzKyNmbUBHgZGA32Aq0NbERHJkHQPN7UFOppZW+AMYBdwGbAoTJ8LjAvDY8M4YfoIM7MQn+/uf3f3N4BSYEh4lLr7Dnf/CJgf2rYe6xfCzH4wPTd6Xr8w2xmJiBynwUXC3cuB+4G/ERWHSmA1sN/dD4dmZUD3MNwd2BnmPRzad0mO15onLn4CM5tkZiVmVlJRUdHQl5RZ6xfCbyZD5U7Ao+ffTFahEJFmJZ3DTZ2JvtkXAN2AM4kOF2Wcuz/q7gl3T+Tl5WUjhdStuBuqq46PVVdFcRGRZiKdw01fAt5w9wp3rwaeBi4GcsPhJ4B8oDwMlwM9AML0TsCe5HiteeLirUNlWWpxEZEsSKdI/A0YamZnhL6FEcBm4AXg66FNMbAkDC8N44Tpz7u7h/iEcPZTAdALeAV4FegVzpZqR9S5vTSNfJuXTvmpxUVEsiCdPolVRB3Qa4ANYVmPArcD3zOzUqI+h9lhltlAlxD/HjA1LGcTsJCowPwOuMndj4R+i5uBZcAWYGFo2zqMmAY5HY+P5XSM4iIizYRFX+Zbj0Qi4SUlJdlO49SsXxj1QVSWRXsQI6ZB4fhsZyUipyEzW+3uidrxtnU1lgwpHK+iICLNmi7LISIisVQkREQkloqEiIjEUpEQEZFYKhIiIhJLRUJERGKpSIiISCwVCRERiaUiISIisVQkREQkloqEiIjEUpEQEZFYKhIiIhJLRUJERGKpSIiISKy0ioSZ5ZrZIjPbamZbzOx/mNk5ZrbczF4Pz51DWzOzWWZWambrzWxQ0nKKQ/vXzaw4KT7YzDaEeWaF26SKiEiGpLsn8XPgd+7+34ELiG4zOhVY4e69gBVhHGA00f2rewGTgEcAzOwc4C7gImAIcFdNYQltbkiaryjNfEVEJAUNLhJm1gn4IuEe1u7+kbvvB8YCc0OzucC4MDwWeMIjK4FcM+sKjAKWu/ted98HLAeKwrSz3X2lR/dYfSJpWSIikgHp7EkUABXAL81srZk9ZmZnAue7+67Q5h3g/DDcHdiZNH9ZiNUXL6sjLiIiGZJOkWgLDAIecfeBwId8fGgJgLAH4Gms45SY2SQzKzGzkoqKiqZenYjIaSOdIlEGlLn7qjC+iKhovBsOFRGed4fp5UCPpPnzQ6y+eH4d8RO4+6PunnD3RF5eXhovSUREkjW4SLj7O8BOM+sdQiOAzcBSoOYMpWJgSRheClwXznIaClSGw1LLgJFm1jl0WI8EloVp75vZ0HBW03VJyxIRkQxom+b8twDzzKwdsAP4JlHhWWhmE4G3gPGh7XPA5UApcDC0xd33mtmPgVdDu7vdfW8YvhF4HOgI/DY8REQkQyzqNmg9EomEl5SUZDsNEZEWxcxWu3uidly/uBYRkVgqEiIiEktFQkREYqlIiIhILBUJERGJpSIhIiKxVCRERCSWioSIiMRSkRARkVgqEiIiEktFQkREYqlIiIhILBUJERGJpSIhIiKxVCRERCSWioSIiMRSkRARkVhpFwkza2Nma83s/4bxAjNbZWalZrYg3NoUM2sfxkvD9J5Jy7gjxLeZ2aikeFGIlZrZ1HRzFRGR1DTGnsStwJak8fuAme7+WWAfMDHEJwL7QnxmaIeZ9QEmAH2BIuAXofC0AR4GRgN9gKtDWxERyZC0ioSZ5QNXAI+FcQMuAxaFJnOBcWF4bBgnTB8R2o8F5rv73939DaAUGBIepe6+w90/AuaHtiIikiHp7kk8CPwQOBrGuwD73f1wGC8Duofh7sBOgDC9MrQ/Fq81T1z8BGY2ycxKzKykoqIizZckIiI1GlwkzGwMsNvdVzdiPg3i7o+6e8LdE3l5edlOR0Sk1WibxrwXA181s8uBDsDZwM+BXDNrG/YW8oHy0L4c6AGUmVlboBOwJyleI3meuLiIiGRAg/ck3P0Od893955EHc/Pu/s1wAvA10OzYmBJGF4axgnTn3d3D/EJ4eynAqAX8ArwKtArnC3VLqxjaUPzFRGR1KWzJxHndmC+md0DrAVmh/hs4EkzKwX2En3o4+6bzGwhsBk4DNzk7kcAzOxmYBnQBpjj7puaIF8REYlh0Zf51iORSHhJSUlqM61fCCvuhsoy6JQPI6ZB4fimSVBEpBkys9Xunqgdb4o9iZZl/UL4zWSororGK3dG46BCISKnPV2WY8XdHxeIGtVVUVxE5DSnIlFZllpcROQ0oiLRKT+1uIjIaURFYsQ0yOl4fCynYxQXETnNqUgUjoevzIJOPQCLnr8yS53WIiLo7KZI4XgVBRGROmhPQkREYqlIiIhILBUJERGJpSIhIiKxVCRERCSWioSIiMRSkRARkVj6nYRII1q8tpwZy7bx9v4quuV2ZMqo3owbWOet2UVaBBUJkUayeG05dzy9garqIwCU76/ijqc3AKhQSIulw00ijWTGsm3HCkSNquojzFi2LUsZiaSvwUXCzHqY2QtmttnMNpnZrSF+jpktN7PXw3PnEDczm2VmpWa23swGJS2rOLR/3cyKk+KDzWxDmGeWmVk6L1akKb29vyqluEhLkM6exGHg++7eBxgK3GRmfYCpwAp37wWsCOMAo4Fe4TEJeASiogLcBVwEDAHuqiksoc0NSfMVpZGvSJPqltsxpbhIS9DgIuHuu9x9TRg+AGwBugNjgbmh2VxgXBgeCzzhkZVArpl1BUYBy919r7vvA5YDRWHa2e6+0qMbcT+RtCyRZmfKqN50zGlzXKxjThumjOqdpYxE0tcoHddm1hMYCKwCznf3XWHSO8D5Ybg7sDNptrIQqy9eVke8rvVPIto74ZOf/GTK+euMFGkMNe8ZvZekNUm7SJjZPwD/Bdzm7u8ndxu4u5uZp7uOk3H3R4FHARKJRErr0xkp0pjGDeyu941kXFN+0U3r7CYzyyEqEPPc/ekQfjccKiI87w7xcqBH0uz5IVZfPL+OeKPSGSki0pLVfNEt31+F8/EX3cVrG+fjMp2zmwyYDWxx9weSJi0Fas5QKgaWJMWvC2c5DQUqw2GpZcBIM+scOqxHAsvCtPfNbGhY13VJy2o0OiNFRFqypv6im87hpouB/wVsMLN1IfYvwL3AQjObCLwF1Nzy7TngcqAUOAh8E8Dd95rZj4FXQ7u73X1vGL4ReBzoCPw2PBpVt9yOlNdREHRGioi0BE39RbfBRcLdXwbifrcwoo72DtwUs6w5wJw64iVAv4bmeCqmjOp9XJ8E6IwUEWk5mvqL7mn/i+txA7vz06/1p3tuRwzontuRn36tvzofpWHWL4SZ/WB6bvS8fmG2M5JWrqlPvda1m9AZKdJI1i+E30yG6vCtrnJnNA5QOD5+PpE0NPWp1xYdBWo9EomEl5SUZDuNlm/9QlhxN1SWQad8GDFNH3QnM7NfVBhq69QDvrsx8/mIpMDMVrt7onZcexJyIn0jbpjKstTiIi3Aad8nIXVYcffHBaJGdVUUl3id8lOLizSWJuwLU5GQE+kbccOMmAY5tc4oyekYxUWaSs2ef+VOwD/e82+kQqEiISc42PG/pRSXoHA8fGVW1AeBRc9fmaVDdNK0mnjPX30ScoKfVf8jP/RfcIZ9dCx20Nvxs+p/ZHr20moZCserKEhmNfGev/Yk5ARzPxjC1OpvUXb0XI66UXb0XKZWf4u5HwzJdmoiUlsT94VpTyKLmuslyrvldmTp/i+w9KMvHBfvrkuViDQ/I6YdfzYiNGpfmPYksqSpr9yYDt08R6QFaeK+MO1JZEl9V27M9t6Ebp4j0sI0YV+YikSWNPdLlOtSJSICOtyUNXFXaNQlykWkOVGRyBId9xeRlkCHm7JEx/1FpCVo9kXCzIqAnwNtgMfc/d4sp9RodNxfRJq7Zn24yczaAA8Do4E+wNVm1ie7WYmInD6adZEAhgCl7r7D3T8C5gNjs5yTiMhpo7kXie5A8l1cykJMREQyoLkXiVNiZpPMrMTMSioqKrKdjohIq9Hci0Q50CNpPD/EjuPuj7p7wt0TeXl5GUtORKS1a9b3uDaztsBfgRFExeFV4J/cfVM981QAb9UKnwu811R5pqm55qa8Utdcc1NeqWuuuTVlXp9y9xO+ZTfrU2Dd/bCZ3QwsIzoFdk59BSLMc8KLNLOSum7w3Rw019yUV+qaa27KK3XNNbds5NWsiwSAuz8HPJftPERETkfNvU9CRESy6HQpEo9mO4F6NNfclFfqmmtuyit1zTW3jOfVrDuuRUQku06XPQkREWkAFQkREYnV6ouEmRWZ2TYzKzWzqRledw8ze8HMNpvZJjO7NcSnm1m5ma0Lj8uT5rkj5LrNzEY1YW5vmtmGsP6SEDvHzJab2evhuXOIm5nNCnmtN7NBTZhX76Ttss7M3jez27KxzcxsjpntNrONSbGUt5GZFYf2r5tZcRPmNsPMtob1P2NmuSHe08yqkrbdvyfNMzi8D0pD/tYEeaX8t2vs/9uYvBYk5fSmma0L8Uxur7jPiGbxPgPA3Vvtg+i3FduBTwPtgNeAPhlcf1dgUBg+i+iHgX2A6cAP6mjfJ+TYHigIubdpotzeBM6tFfsZMDUMTwXuC8OXA78FDBgKrMrg3+8d4FPZ2GbAF4FBwMaGbiPgHGBHeO4chjs3UW4jgbZh+L6k3Homt6u1nFdCvhbyH90EeaX0t7S7dTcAAAOTSURBVGuK/9u68qo1/X8D07KwveI+I5rF+8zdW/2eRFavIuvuu9x9TRg+AGyh/gsUjgXmu/vf3f0NoJToNWTKWGBuGJ4LjEuKP+GRlUCumXXNQD4jgO3uXvsX9MmabJu5+0vA3jrWl8o2GgUsd/e97r4PWA4UNUVu7v57dz8cRlcSXcYmVsjvbHdf6dEnzRNJr6fR8qpH3N+u0f9v68sr7A2MB35d3zKaaHvFfUY0i/cZtP7DTc3mKrJm1hMYCKwKoZvD7uKcml1JMpuvA783s9VmNinEznf3XWH4HeD8LOSVbALH/+Nme5tB6tsoW9vueqJvnDUKzGytmf3BzIaFWPeQTyZyS+Vvl+ltNgx4191fT4plfHvV+oxoNu+z1l4kmgUz+wfgv4Db3P194BHgM8AAYBfRrm6mfcHdBxHd0OkmM/ti8sTwTSlr50ebWTvgq8BTIdQcttlxsr2N4pjZncBhYF4I7QI+6e4Dge8B/2lmZ2cwpWb3t6vlao7/MpLx7VXHZ8Qx2X6ftfYicUpXkW1KZpZD9Mef5+5PA7j7u+5+xN2PAv/Bx4dHMpavu5eH593AMyGHd2sOI4Xn3ZnOK8loYI27vxvyzPo2C1LdRhnNz8y+AYwBrgkfLoTDOXvC8Gqi4/2fC3kkH5Jqktwa8LfL2Daz6CKiXwMWJOWb0e1V12cEzeh91tqLxKtALzMrCN9MJwBLM7XycKxzNrDF3R9Iiicfz78SqDnjYikwwczam1kB0Iuoo6yx8zrTzM6qGSbq8NwY1l9zVkQxsCQpr+vCmRVDgcqkXeGmcty3u2xvsySpbqNlwEgz6xwOs4wMsUZn0f3gfwh81d0PJsXzLLoVMGb2aaJttCPk976ZDQ3v1euSXk9j5pXq3y6T/7dfAra6+7HDSJncXnGfETSn91lj9H435wfR2QB/Jfo2cGeG1/0Fot3E9cC68LgceBLYEOJLga5J89wZct1GmmdO1JPXp4nOGHkN2FSzXYAuwArgdeD/AeeEuBHda3x7yDvRxNvtTGAP0CkplvFtRlSkdgHVRMd4JzZkGxH1D5SGxzebMLdSouPSNe+1fw9t/2f4O68D1gBfSVpOguhDezvwEOEqDI2cV8p/u8b+v60rrxB/HPjnWm0zub3iPiOaxfvM3XVZDhERidfaDzeJiEgaVCRERCSWioSIiMRSkRARkVgqEiIiEktFQkREYqlIiIhIrP8PWT5h1hY1nSkAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# import matplotlib.pyplot as plt\n","# from torch.autograd import Variable\n","# from matplotlib import cm\n","# from understandingPINNs.models import PINN\n","# import torch\n","# from understandingPINNs import metrics\n","\n","# for i in range(20):\n","#   seed = i\n","#   np.random.seed(seed=seed)\n","#   for ini in initialcon: \n","\n","#     start, final = groundtruth_2dim.CreateTrainingDataTrajStormer(1,ini,spacedim,h,f1,f2,seed = seed,n_h = 1)\n","\n","#     delta = (final-start)/0.1\n","#     delta2 = delta.copy()\n","#     delta2[0,:] = f1(start)\n","#     delta2[1,:] = f2(start)\n","#     dq_MSE = np.mean(np.square(delta2[0,:]-delta[0,:]))\n","#     dp_MSE = np.mean(np.square(delta2[1,:]-delta[1,:]))\n","#     data_MSE = np.mean(np.square(np.sqrt(delta[0,:]**2 + delta[1,:]**2)-np.sqrt(delta2[0,:]**2 + delta2[1,:]**2)))\n","\n","#     \"\"\"# sumNN\"\"\"\n","\n","    \n","\n","#     if torch.cuda.is_available():\n","#       device=torch.device('cuda')\n","#     else:\n","#       device=torch.device('cpu')\n","#     print(\"device used\", device)\n","#     torch.manual_seed(seed)\n","#     torch.cuda.manual_seed_all(seed)\n","\n","#     wholemat, evalmat = PINN.data_preprocessing(start, delta,device)    \n","\n","#     import torch.optim as optim\n","#     import time \n","\n","\n","#     sumnet = PINN.SumNet(2,16,2)\n","#     starttime = time.time() \n","\n","#     results = PINN.train(sumnet,bs=10,num_epoch=5000,initial_conditions=initialcon,device=device, wholemat=wholemat,evalmat=evalmat,x0=x0,H0=H0,dim=dim,LR=LR,patience=100,c1=1,c2=1,c3=1,c4=1)\n","#     sumnet, epochs = results[0], results[1]\n","#     septraintime = time.time()-starttime\n","#     torch.save(sumnet.state_dict(), '/content/drive/MyDrive/CIKM2022/SeparableExp/%s/%s_%s_%s_%s.pt' %(sys,sys,\"sumNN\",seed,ini))\n","\n","#     H_pred, dq_pred, dp_pred = np.zeros((H_true.shape)), np.zeros((H_true.shape)), np.zeros((H_true.shape))\n","#     for i in tqdm(range(len(sample_points))):\n","#       out = get_H_grad(sumnet, sample_points[i], device)\n","#       H_pred[i] = out[0]\n","#       dq_pred[i] = out[1]\n","#       dp_pred[i] = out[2]\n","#     print(H_pred.shape, dq_pred.shape, dp_pred.shape)\n","#     file_object = open('/content/drive/MyDrive/CIKM2022/SeparableExp/%s/%s_results.txt' %(sys,sys), 'a') \n","#     file_object.write('sumNN, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s \\n' \n","#           %(ini, seed, dq_MSE, dp_MSE, data_MSE, septraintime, epochs, metrics.MSE(H_pred, H_true, diagdist), metrics.MSE(dq_pred, f1(np.squeeze(sample_points,2).transpose()), diagdist), metrics.MSE(dp_pred, f2(np.squeeze(sample_points,2).transpose()), diagdist),\n","#           np.mean(np.square(H_pred- H_true)), np.mean(np.square(dq_pred- f1(np.squeeze(sample_points,2).transpose()))), np.mean(np.square(dp_pred- f2(np.squeeze(sample_points,2).transpose()))), \n","#           np.mean(np.square(np.sqrt(dq_pred**2 + dp_pred**2)-np.sqrt(f1(np.squeeze(sample_points,2).transpose())**2 + f2(np.squeeze(sample_points,2).transpose())**2)))))\n","#     file_object.close()\n","    \n","#     H_pred, dq_pred, dp_pred = H_pred.reshape(xshort.shape), dq_pred.reshape(xshort.shape), dp_pred.reshape(xshort.shape)\n","\n","#     plt.figure(figsize = (10,10))\n","#     plt.quiver(xshort,yshort,dq_pred,dp_pred) #x,y,dH/dy,-dH/dx\n","#     plt.imshow(np.flip(H_pred.reshape(xshort.shape),0), cmap = cm.jet, extent = (spacedim[0][0], spacedim[0][1], spacedim[1][0], spacedim[1][1]))\n","#     plt.savefig('/content/drive/MyDrive/CIKM2022/SeparableExp/%s/%s_%s_%s_%s.png' %(sys,sys, ini, seed, \"sumNN\"))"],"metadata":{"id":"6YbVlfboYN5e"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNRuOhHqVaXW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1iYzld4PVaZu"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from torch.autograd import Variable\n","from matplotlib import cm\n","from understandingPINNs.models import PINN\n","import torch\n","from understandingPINNs import metrics\n","\n","for i in [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]:\n","  seed = i\n","  np.random.seed(seed=seed)\n","  for ini in initialcon: \n","\n","    start, final = groundtruth_2dim.CreateTrainingDataTrajSympEuler(1,ini,spacedim,h,f1,f2,seed = seed,n_h = 1)\n","\n","    delta = (final-start)/0.1\n","    delta2 = delta.copy()\n","    delta2[0,:] = f1(start)\n","    delta2[1,:] = f2(start)\n","    dq_MSE = np.mean(np.square(delta2[0,:]-delta[0,:]))\n","    dp_MSE = np.mean(np.square(delta2[1,:]-delta[1,:]))\n","    data_MSE = np.mean(np.square(np.sqrt(delta[0,:]**2 + delta[1,:]**2)-np.sqrt(delta2[0,:]**2 + delta2[1,:]**2)))\n","\n","    \"\"\"# sepNN\"\"\"\n","\n","    \n","\n","    if torch.cuda.is_available():\n","      device=torch.device('cuda')\n","    else:\n","      device=torch.device('cpu')\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    wholemat, evalmat = PINN.data_preprocessing(start, delta,device)    \n","\n","    import torch.optim as optim\n","    import time \n","\n","\n","    sepnet = PINN.sepNet(1,11,11,1)\n","    starttime = time.time() \n","\n","    results = PINN.train(sepnet,bs=10,num_epoch=5000,initial_conditions=initialcon,device=device, wholemat=wholemat,evalmat=evalmat,x0=x0,H0=H0,dim=dim,LR=LR,patience=100,c1=1,c2=1,c3=1,c4=1)\n","    sepnet, epochs = results[0], results[1]\n","    septraintime = time.time()-starttime\n","    torch.save(sepnet.state_dict(), '/content/drive/MyDrive/CIKM2022/SeparableExp/%s (euler)/%s_%s_%s_%s.pt' %(sys,sys,\"sepNN\",seed,ini))\n","\n","    H_pred, dq_pred, dp_pred = np.zeros((H_true.shape)), np.zeros((H_true.shape)), np.zeros((H_true.shape))\n","    for i in tqdm(range(len(sample_points))):\n","      out = get_H_grad(sepnet, sample_points[i], device)\n","      H_pred[i] = out[0]\n","      dq_pred[i] = out[1]\n","      dp_pred[i] = out[2]\n","    print(H_pred.shape, dq_pred.shape, dp_pred.shape)\n","    file_object = open('/content/drive/MyDrive/CIKM2022/SeparableExp/%s (euler)/%s_results.txt' %(sys,sys), 'a') \n","    file_object.write('sepNN, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s \\n' \n","          %(ini, seed, dq_MSE, dp_MSE, data_MSE, septraintime, epochs, metrics.MSE(H_pred, H_true, diagdist), metrics.MSE(dq_pred, f1(np.squeeze(sample_points,2).transpose()), diagdist), metrics.MSE(dp_pred, f2(np.squeeze(sample_points,2).transpose()), diagdist),\n","          np.mean(np.square(H_pred- H_true)), np.mean(np.square(dq_pred- f1(np.squeeze(sample_points,2).transpose()))), np.mean(np.square(dp_pred- f2(np.squeeze(sample_points,2).transpose()))), \n","          np.mean(np.square(np.sqrt(dq_pred**2 + dp_pred**2)-np.sqrt(f1(np.squeeze(sample_points,2).transpose())**2 + f2(np.squeeze(sample_points,2).transpose())**2)))))\n","    file_object.close()\n","    \n","    H_pred, dq_pred, dp_pred = H_pred.reshape(xshort.shape), dq_pred.reshape(xshort.shape), dp_pred.reshape(xshort.shape)\n","\n","    plt.figure(figsize = (10,10))\n","    plt.quiver(xshort,yshort,dq_pred,dp_pred) #x,y,dH/dy,-dH/dx\n","    plt.imshow(np.flip(H_pred.reshape(xshort.shape),0), cmap = cm.jet, extent = (spacedim[0][0], spacedim[0][1], spacedim[1][0], spacedim[1][1]))\n","    plt.savefig('/content/drive/MyDrive/CIKM2022/SeparableExp/%s (euler)/%s_%s_%s_%s.png' %(sys,sys, ini, seed, \"sepNN\"))\n","\n","    net = PINN.Net(2,16,1)\n","    starttime = time.time() \n","    \n","    results = PINN.train(net,bs=10,num_epoch=5000,initial_conditions=initialcon,device=device, wholemat=wholemat,evalmat=evalmat,x0=x0,H0=H0,dim=dim,LR=LR,patience=100,c1=1,c2=1,c3=1,c4=1)\n","    net, epochs = results[0], results[1]\n","    PINNtraintime = time.time()-starttime\n","    torch.save(net.state_dict(), '/content/drive/MyDrive/CIKM2022/SeparableExp/%s (euler)/%s_%s_%s_%s.pt' %(sys,sys,\"PINN\",seed,ini))\n","\n","    H_pred, dq_pred, dp_pred = np.zeros((H_true.shape)), np.zeros((H_true.shape)), np.zeros((H_true.shape))\n","    for i in tqdm(range(len(sample_points))):\n","      out = get_H_grad(net, sample_points[i], device)\n","      H_pred[i] = out[0]\n","      dq_pred[i] = out[1]\n","      dp_pred[i] = out[2]\n","    print(H_pred.shape, dq_pred.shape, dp_pred.shape)\n","    file_object = open('/content/drive/MyDrive/CIKM2022/SeparableExp/%s (euler)/%s_results.txt' %(sys,sys), 'a')\n","    file_object.write('PINN, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s \\n' \n","          %(ini, seed, dq_MSE, dp_MSE, data_MSE, PINNtraintime, epochs, metrics.MSE(H_pred, H_true, diagdist), metrics.MSE(dq_pred, f1(np.squeeze(sample_points,2).transpose()), diagdist), metrics.MSE(dp_pred, f2(np.squeeze(sample_points,2).transpose()), diagdist),\n","          np.mean(np.square(H_pred- H_true)), np.mean(np.square(dq_pred- f1(np.squeeze(sample_points,2).transpose()))), np.mean(np.square(dp_pred- f2(np.squeeze(sample_points,2).transpose()))), \n","          np.mean(np.square(np.sqrt(dq_pred**2 + dp_pred**2)-np.sqrt(f1(np.squeeze(sample_points,2).transpose())**2 + f2(np.squeeze(sample_points,2).transpose())**2)))))\n","    file_object.close()\n","    \n","    H_pred, dq_pred, dp_pred = H_pred.reshape(xshort.shape), dq_pred.reshape(xshort.shape), dp_pred.reshape(xshort.shape)\n","    \n","    plt.figure(figsize = (10,10))\n","    plt.quiver(xshort,yshort,dq_pred,dp_pred) #x,y,dH/dy,-dH/dx\n","    plt.imshow(np.flip(H_pred.reshape(xshort.shape),0), cmap = cm.jet, extent = (spacedim[0][0], spacedim[0][1], spacedim[1][0], spacedim[1][1]))\n","    plt.savefig('/content/drive/MyDrive/CIKM2022/SeparableExp/%s (euler)/%s_%s_%s_%s.png' %(sys,sys, ini, seed, \"PINN\"))\n","\n","    print(septraintime, PINNtraintime)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1pRcY_3Vab-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkw2Qi1LVaeW"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"authorship_tag":"ABX9TyOwWprayTKpMkvjnTtJA9i/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}